{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r754oy-rU1Zo"
   },
   "source": [
    "# Fine-tuning BERT\n",
    "\n",
    "In this notebook, you will download a pre-trained BERT network, which was already fine-tuned on a task of predicting whether two sequences are semantically similar or not (a two-class classification problem). As Colab GPUs are not always available, you will test this on the CPU - which is of course slower than a GPU.\n",
    "\n",
    "Note that the Huggingface transformer interface (that you used in the tokenization notebook) makes these tasks far easier, by providing functions that download trained models; we here spell out the code, to give you an impression of what happens behind the scenes.\n",
    "\n",
    "First, we install the D2L module - **if you get an error, do not restart the session!** Just continue, things should work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNuM7bjdPXK2"
   },
   "outputs": [],
   "source": [
    "!pip install d2l==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Itajs2ruXPMl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from d2l import torch as d2l\n",
    "import json\n",
    "import multiprocessing\n",
    "import tensorflow_datasets as tfds   # if Colab complains, first install (similar to installation of d2l above)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2LDRZdNXik9"
   },
   "source": [
    "Here, we make use of a dataset distributed with tensorflow. It's part of GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/), a collection of resources for training, evaluating, and analyzing natural language understanding systems. In particular, we focus on the Microsoft Research Paraphrase Corpus (MRPC), a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-Eaw4-vJpOm"
   },
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "glue, info = tfds.load('glue/mrpc', with_info=True,\n",
    "                       # It's small, load the whole dataset\n",
    "                       batch_size=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCLommOIaSLH"
   },
   "source": [
    "The MRPC dataset contains training data, test data and validation data. Items in the datasets contain pairs of sentences (sentence1 and sentence2) with the associated label indicating that the two sentences are equivalent (1) or not (0). In the test data, the label is set to -1 (unlabelled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLZWuCyqJvfz"
   },
   "outputs": [],
   "source": [
    "def extract_text(s):\n",
    "    # Remove information that will not be used by us\n",
    "    s = re.sub('\\\\(', '', s)\n",
    "    s = re.sub('\\\\)', '', s)\n",
    "    # Substitute two or more consecutive whitespace with space\n",
    "    s = re.sub('\\\\s{2,}', ' ', s)\n",
    "    return s.strip()\n",
    "\n",
    "def read_preprocess(data):\n",
    "    \"\"\"Read the dataset into sentence1, sentence2, and labels.\"\"\"\n",
    "    label_set = {'not_equivalent': 0, 'equivalent': 1}\n",
    "    sentences1 = [extract_text(s.numpy().decode()) for s in data['sentence1']]\n",
    "    sentences2 = [extract_text(s.numpy().decode()) for s in data['sentence2']]\n",
    "    labels = [s.numpy() for s in data['label']]\n",
    "    return sentences1, sentences2, labels\n",
    "\n",
    "print('Labels: ', info.features['label'].names)\n",
    "train_data      = read_preprocess(glue['train'])\n",
    "test_data       = read_preprocess(glue['test'])\n",
    "validation_data = read_preprocess(glue['validation'])\n",
    "\n",
    "for data in [train_data, test_data, validation_data]:\n",
    "    print([[row for row in data[2]].count(i) for i in [-1,0,1]])\n",
    "    # For train, test, validation: print number of cases with label -1,0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SuZzwzDcJJW"
   },
   "source": [
    "Some examples of sentence pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEmjCuhecHEo"
   },
   "outputs": [],
   "source": [
    "sents1, sents2, labels = train_data\n",
    "print(sents1[0], '<->', sents2[0], ': ', labels[0])\n",
    "sents1, sents2, labels = validation_data\n",
    "print(sents1[0], '<->', sents2[0], ': ', labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40PrlOlnc922"
   },
   "source": [
    "We now will load a pre-trained BERT, specifically a 12-layer, 768-hidden units, 12-head, 110M parameter base model (there are also small and large versions). We also load the vocabulary which was obtained when pre-training the model.\n",
    "\n",
    "First, we will load the data - this may take a few minutes depending on your Internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9nUja429qi-"
   },
   "outputs": [],
   "source": [
    "!git clone https://git.wur.nl/bioinformatics/grs34806-deep-learning-course-data.git data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCgz4jOvYoqK"
   },
   "outputs": [],
   "source": [
    "# Change to GPU for speedup\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Define an empty vocabulary to load the predefined vocabulary\n",
    "vocab = d2l.Vocab()\n",
    "vocab.idx_to_token = json.load(open(os.path.join('data/vocab.json')))\n",
    "vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
    "\n",
    "# Instantiate the network architecture\n",
    "bert = d2l.BERTModel(len(vocab), num_hiddens=768, ffn_num_hiddens=3072,\n",
    "                     num_heads=12, num_blks=12, dropout=0.1, max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKn1uxlWYaf-"
   },
   "source": [
    "Let's have a quick look at what `bert` looks like; we will use most (but not all) of this network below for our dedicated classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H5OY7wKKYVEy"
   },
   "outputs": [],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGaDs_AkeNqh"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "In the `bert` architecture above, check that you recognize the encoder blocks (how many?), each with four submodules (which ones?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWi41PrIZn9S"
   },
   "outputs": [],
   "source": [
    "class GLUEBERTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, max_len, vocab=None):\n",
    "        # Unpack all tokens\n",
    "        all_tokens = [[s1_tokens, s2_tokens]\n",
    "            for s1_tokens, s2_tokens in zip(*[\n",
    "                d2l.tokenize([s.lower() for s in sentences])\n",
    "                    for sentences in dataset[:2]])]\n",
    "\n",
    "        self.labels = torch.tensor(dataset[2])\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        (self.all_token_ids, self.all_segments,\n",
    "         self.valid_lens) = self._preprocess(all_tokens)\n",
    "        print('Read ' + str(len(self.all_token_ids)) + ' examples')\n",
    "\n",
    "    def _preprocess(self, all_tokens):\n",
    "        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n",
    "        out = pool.map(self._mp_worker, all_tokens)\n",
    "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
    "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
    "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
    "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
    "                torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens))\n",
    "\n",
    "    def _mp_worker(self, all_tokens):\n",
    "        s1_tokens, s2_tokens = all_tokens\n",
    "        self._truncate_pair_of_tokens(s1_tokens, s2_tokens)\n",
    "        tokens, segments = d2l.get_tokens_and_segments(s1_tokens, s2_tokens)\n",
    "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
    "                             * (self.max_len - len(tokens))\n",
    "        segments = segments + [0] * (self.max_len - len(segments))\n",
    "        valid_len = len(tokens)\n",
    "        return token_ids, segments, valid_len\n",
    "\n",
    "    def _truncate_pair_of_tokens(self, s1_tokens, s2_tokens):\n",
    "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT input\n",
    "        while len(s1_tokens) + len(s2_tokens) > self.max_len - 3:\n",
    "            if len(s1_tokens) > len(s2_tokens):\n",
    "                s1_tokens.pop()\n",
    "            else:\n",
    "                s2_tokens.pop()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.all_token_ids[idx].to(device),\n",
    "                self.all_segments[idx].to(device),\n",
    "                self.valid_lens[idx].to(device)), self.labels[idx].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XT_9kUN-UO9"
   },
   "source": [
    "## The dataset for fine-tuning BERT\n",
    "\n",
    "For the task on the GLUE dataset, we define a customized dataset class `GLUEBERTDataset`.\n",
    "In each example, the two sentences form a pair of text sequences packed into one BERT input sequence. Segment IDs are used to distinguish the two text sequences.\n",
    "With the predefined maximum length of a BERT input sequence (`max_len`, here 128), the last token of the longer of the input text pair keeps getting removed until the maximum length is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4b5PSRxZwIt"
   },
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "batch_size = 8 # We use a small batch size here for demonstration purposes\n",
    "\n",
    "train_set      = GLUEBERTDataset(train_data, max_len, vocab)\n",
    "validation_set = GLUEBERTDataset(validation_data, max_len, vocab)\n",
    "test_set       = GLUEBERTDataset(test_data, max_len, vocab)\n",
    "\n",
    "# We use glue_validate for testing (test set is unlabelled)\n",
    "train_iter      = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
    "validation_iter = torch.utils.data.DataLoader(validation_set, batch_size, shuffle=False)\n",
    "test_iter       = torch.utils.data.DataLoader(test_set, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYeVoUFWPXLH"
   },
   "source": [
    "Next, we create a network out of parts of BERT model - its encoder and hidden layer - followed by a simple linear output layer with 2 units coding for our two classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TWiA2O3gasMW"
   },
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        # Note how we exctract here various pieces of the BERT model defined above\n",
    "        self.encoder = bert.encoder\n",
    "        self.hidden = bert.hidden\n",
    "        # 768 is the dimension of the hidden state of bert.hidden\n",
    "        self.output = nn.Sequential(nn.Linear(768, 2))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        tokens_X, segments_X, valid_lens_x = inputs\n",
    "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
    "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
    "\n",
    "net = BERTClassifier(bert)\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z1W_XWnKSSph"
   },
   "source": [
    "Let's now load the parameters of the model, obtained by pre-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDeu5BZYPXLI"
   },
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(\"data/GLUEBERT.net\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKi0O3OKc8aP"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "You can use the code below to investigate the context dependency of the BERT embeddings of the same word - in this case \"bank\", which gets a different 768D embedding depending on the sentence in which it is used. Do the similarities make sense? See if you can come up with some different sentences where the same words are used in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd0Codqcc7TE"
   },
   "outputs": [],
   "source": [
    "def get_bert_encoding(net, tokens):\n",
    "    toks, segments = d2l.get_tokens_and_segments(tokens)\n",
    "    token_ids = torch.tensor(vocab[toks], device=device).unsqueeze(0)\n",
    "    segments = torch.tensor(segments, device=device).unsqueeze(0)\n",
    "    valid_len = torch.tensor(len(toks), device=device).unsqueeze(0)\n",
    "    encoded_X = net(token_ids, segments, valid_len)\n",
    "    return encoded_X\n",
    "\n",
    "bert.to(device)\n",
    "bert.eval()\n",
    "\n",
    "tokens_a = 'i walked along the road to get cash from my bank'.split()\n",
    "tokens_b = 'we managed to open a savings account at the local bank'.split()\n",
    "tokens_c = 'i swam across the river to get to its other bank'.split()\n",
    "\n",
    "# First token is <cls>, so 'bank' is the 11th token.\n",
    "enc_a = get_bert_encoding(bert.encoder, tokens_a)[:,11,:]\n",
    "enc_b = get_bert_encoding(bert.encoder, tokens_b)[:,11,:]\n",
    "enc_c = get_bert_encoding(bert.encoder, tokens_c)[:,11,:]\n",
    "\n",
    "print(F.cosine_similarity(enc_a,enc_b))\n",
    "print(F.cosine_similarity(enc_a,enc_c))\n",
    "print(F.cosine_similarity(enc_b,enc_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9eDhD5yAETU"
   },
   "source": [
    "Now we can try out the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJ7Zu8eAe1EI"
   },
   "outputs": [],
   "source": [
    "# Test the network on a batch of sentence pairs\n",
    "net.to(device)\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  X,y = next(iter(train_iter))\n",
    "  yhat = np.argmax(net(X).detach().cpu().numpy(),axis=1)\n",
    "\n",
    "for i in range(len(yhat)):\n",
    "  print('Input: ', ' '.join([vocab.idx_to_token[j] for j in X[0][i]]))\n",
    "  print('Prediction: ', yhat[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ9EyX1yPXLJ"
   },
   "outputs": [],
   "source": [
    "# To test our own sentences, first create a GLUEBERTDataset with them\n",
    "s1 = \"We use this as a test sentence to see whether the network works.\"\n",
    "s2 = \"To test if the network works we use this sentence.\"\n",
    "\n",
    "#s1 = \"Google is a very large company indeed.\"\n",
    "#s2 = \"And now for something completely different.\"\n",
    "\n",
    "data = ([extract_text(s1)], [extract_text(s2)], [0])\n",
    "print(data)\n",
    "\n",
    "my_set = GLUEBERTDataset(data, max_len, vocab)\n",
    "my_iter = torch.utils.data.DataLoader(my_set, 1)\n",
    "\n",
    "with torch.no_grad():\n",
    "  X, y = next(iter(my_iter))\n",
    "  yhat = np.argmax(net(X).detach().cpu().numpy(),axis=1)\n",
    "print('Prediction: ', yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY3IMuvWAS7B"
   },
   "source": [
    "Instead of only looking at the actual prediction of the model (as above) we can also look at the probabilities for the two possible labels (\"similar or not similar\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrzbcei9_F8F"
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(net(X).detach(),dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yquEAb6L62CN"
   },
   "source": [
    "## Exercise 3\n",
    "Try changing specific key words in the sentence pairs above, to get some idea on how much understanding the model has about language. For example, replace \"network\" by \"model\" or by \"car\" in one of the two sentences of the first pair. Look at the resulting prediction (equivalent or not) and also at the underlying probabilities which the model gives as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkkmUK8PAIeV"
   },
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh4t7VaHAK8b"
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "You can clearly identify 12 encoder blocks, as follows:\n",
    "\n",
    "```\n",
    "TransformerEncoderBlock(\n",
    "        (attention): MultiHeadAttention(\n",
    "          (attention): DotProductAttention(\n",
    "            (dropout): Dropout(p=0.1, inplace=False)\n",
    "          )\n",
    "          (W_q): Linear(in_features=0, out_features=768, bias=True)\n",
    "          (W_k): Linear(in_features=0, out_features=768, bias=True)\n",
    "          (W_v): Linear(in_features=0, out_features=768, bias=True)\n",
    "          (W_o): Linear(in_features=0, out_features=768, bias=True)\n",
    "        )\n",
    "        (addnorm1): AddNorm(\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "        (ffn): PositionWiseFFN(\n",
    "          (dense1): Linear(in_features=0, out_features=3072, bias=True)\n",
    "          (relu): ReLU()\n",
    "          (dense2): Linear(in_features=0, out_features=768, bias=True)\n",
    "        )\n",
    "        (addnorm2): AddNorm(\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        )\n",
    "      )\n",
    "```\n",
    "\n",
    "At the end, you will see an MLM layer (maxed language modelling) and an NSP layer (next sentence prediction); these were both used in pretraining. When fine-tuning BERT, we take the output of the encoder block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3Phn3DKANsj"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "If all goes well, you should get a similarity of ~0.96 for the two sentences that use \"bank\" to mean \"financial institution\", and lower (but still reasonably high) for the pairs with one sentence using \"bank\" as \"riverside\".\n",
    "\n",
    "There are many more homonyms that you can play with, see e.g. https://www.yourdictionary.com/articles/examples-homonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vi4P4syTAhEd"
   },
   "source": [
    "## Exercise 3\n",
    "\n",
    "Changing \"network\" in the first sentence to \"model\" or \"car\" indeed changes the classifcation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
