{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d5e2098-5c13-4649-a523-a62363ee3598",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holmrenser/deep_learning/blob/main/word_embeddings.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Word embeddings\n",
    "In this notebook we work with pretrained word embedding scores from the [GloVe project](https://nlp.stanford.edu/projects/glove/). We use the smallest version, which maps 400,000 words into 50D embedding space, and was trained on 6billion words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01db6910-745b-4fdb-8dd1-9b12f3247564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-03-12 15:23:53--  https://github.com/holmrenser/deep_learning/raw/main/data/glove.6B.50d.txt.gz\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/holmrenser/deep_learning/main/data/glove.6B.50d.txt.gz [following]\n",
      "--2024-03-12 15:23:53--  https://raw.githubusercontent.com/holmrenser/deep_learning/main/data/glove.6B.50d.txt.gz\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69239637 (66M) [application/octet-stream]\n",
      "Saving to: ‘glove.6B.50d.txt.gz’\n",
      "\n",
      "glove.6B.50d.txt.gz 100%[===================>]  66,03M  25,4MB/s    in 2,6s    \n",
      "\n",
      "2024-03-12 15:24:03 (25,4 MB/s) - ‘glove.6B.50d.txt.gz’ saved [69239637/69239637]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/holmrenser/deep_learning/raw/main/data/glove.6B.50d.txt.gz\n",
    "!gunzip glove.6B.50d.txt.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "aeb7dc75-8854-4a31-9ccb-ccd10126d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from warnings import warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5226c299-865a-4425-9645-9cc32e24ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rome'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    def __init__(self, vocab: dict[str, int], embeddings: torch.tensor):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, filename: str) -> 'WordEmbedding':\n",
    "        vocab = {'<pad>':0, '<unk>':1}\n",
    "        embeddings = []\n",
    "        \n",
    "        with open(filename,'r') as fh:\n",
    "            for i,line in enumerate(fh):\n",
    "                parts = line.split()\n",
    "                \n",
    "                token = parts[0]\n",
    "                vocab[token] = i + 2 # add two to account for predefined tokens\n",
    "                \n",
    "                embedding = list(map(float, parts[1:]))\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "        unk_emb = torch.zeros((1, embeddings.shape[1]))\n",
    "        pad_emb = embeddings.mean(dim=0)\n",
    "        embeddings = torch.vstack([pad_emb, unk_emb, embeddings])\n",
    "        \n",
    "        return cls(vocab, embeddings)\n",
    "\n",
    "    def forward(self, word: str) -> torch.tensor:\n",
    "        i = self.vocab.get(word, 0)\n",
    "        if i == 0:\n",
    "            warn(f'{word} is not in the vocabulary, returning average embedding')\n",
    "        return self.embeddings(torch.tensor([i]))\n",
    "\n",
    "    def find_closest(self, vec: torch.tensor, top_k: int=1) -> str:\n",
    "        cos_sim = F.cosine_similarity(emb.embeddings.weight, vec)\n",
    "        closest_idx = {*torch.argsort(cos_sim)[:top_k]}\n",
    "        words = [word for word,idx in self.vocab.items() if idx in closest_idx]\n",
    "        return word[0]\n",
    "\n",
    "emb = WordEmbedding.from_pretrained('glove.6B.50d.txt')\n",
    "emb.find_closest(emb('paris') - emb('france') + emb('italy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c9dcca9a-1185-45b5-8d7c-4d5c144dd0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aisenbergs'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.find_closest(emb('mud') - emb('water'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "6ab17af3-a34d-4c08-8e49-1d3c3f05e7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/ymk92z8x4cb3wb1qbzb0k5v00000gn/T/ipykernel_13780/347095768.py:32: UserWarning: hot1 is not in the vocabulary, returning average embedding\n",
      "  warn(f'{word} is not in the vocabulary, returning average embedding')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1292, -0.2887, -0.0122, -0.0568, -0.2021, -0.0839,  0.3336,  0.1605,\n",
       "          0.0387,  0.1783,  0.0470, -0.0029,  0.2910,  0.0461, -0.2092, -0.0661,\n",
       "         -0.0682,  0.0767,  0.3134,  0.1785, -0.1226, -0.0992, -0.0750,  0.0641,\n",
       "          0.1444,  0.6089,  0.1746,  0.0534, -0.0127,  0.0347, -0.8124, -0.0469,\n",
       "          0.2019,  0.2031, -0.0394,  0.0697, -0.0155, -0.0341, -0.0653,  0.1225,\n",
       "          0.1399, -0.1745, -0.0801,  0.0850, -0.0104, -0.1370,  0.2013,  0.1007,\n",
       "          0.0065,  0.0169]])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb('hot1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
