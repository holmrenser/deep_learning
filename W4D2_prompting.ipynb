{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "In this notebook we continue with the GPT2 architecture we developed earlier, but this time we focus on strategies from generating samples from a trained model (i.e. prompting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We use the tiny shakespeare dataset because it is small and has interesting structure (i.e. lots of newlines, character names in all capital letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we specify a TextDataset class. However, instead of using a character-level tokenizer (like before), we train a bytepair encoder from the huggingface tokenizer library. \n",
    "\n",
    "We use our trained tokenizer to tokenize the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    def __init__(self, tokens: list[int], tokenizer, context_size: int=256):\n",
    "        self.tokens = tokens\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_chars = len(self.data)\n",
    "        vocab_size = self.vocab_size\n",
    "        context_size = self.context_size\n",
    "        return f'CharacterDataset({n_chars=}, {vocab_size=}, {context_size=})'\n",
    "\n",
    "    @classmethod\n",
    "    def from_textfile(cls, filename: str, tokenizer, context_size: int=256) -> 'TextDataset':\n",
    "        \"\"\"Load a textfile and automatically 'train' a character level tokenizer\"\"\"\n",
    "        with open(filename, 'r') as fh:\n",
    "            data = fh.read()\n",
    "            tokens = tokenizer.encode(data).ids\n",
    "            return cls(tokens, tokenizer, context_size=context_size)\n",
    "\n",
    "    def train_test_split(self, train_percentage: float=0.9) -> tuple['TextDataset','TextDataset']:\n",
    "        n_train_chars = int(train_percentage * len(self.tokens))\n",
    "\n",
    "        train_tokens = self.tokens[:n_train_chars]\n",
    "        train_dataset = TextDataset(train_tokens, self.tokenizer, self.context_size)\n",
    "\n",
    "        test_tokens = self.tokens[n_train_chars:]\n",
    "        test_dataset = TextDataset(test_tokens, self.tokenizer, self.context_size)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens) - self.context_size\n",
    "\n",
    "    def __getitem__(self, pos: int) -> torch.tensor:\n",
    "        \"\"\"Return tokens starting at pos up to pos + context_size, targets are shifted by one position\"\"\"\n",
    "        # grab a chunk of context_size tokens from the data\n",
    "        tokens = self.tokens[pos:pos + self.context_size + 1]\n",
    "        # convert to tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        # targets are shifted one position from input\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "# Specify a BPE tokenizer using Huggingface's tokenizers\n",
    "tokenizer = Tokenizer(models.BPE(byte_fallback=True))\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Train on shakespeare\n",
    "trainer = trainers.BpeTrainer(vocab_size=512)\n",
    "tokenizer.train([\"input.txt\"], trainer=trainer)\n",
    "\n",
    "dataset = TextDataset.from_textfile('./input.txt', tokenizer=tokenizer)\n",
    "train_dataset, test_dataset = dataset.train_test_split()\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Below is a full implementation of a GPT2-style transformer, except we use a fast cuda implementation to calculate the dot-product attention. \n",
    "\n",
    "The biggest difference compared to our earlier model is in the `generate` method of the `GPT` class, which now includes several options to configure the way we sample tokens:\n",
    "- Greedy generation: at each sampling iteration picks the token with the highest probability\n",
    "- Stochastic generation: at each iteration sample from a distribution with probabilities determined by our model (this is what we have done so far)\n",
    "- Top-K sampling: sample tokens from a distribution, but only the k most likely tokens get a non-zero probability\n",
    "- Temperature scaling: scale logits by a 'temperature' value, lower temperatures lead to sampling from a more 'spiked' probability distribution.\n",
    "\n",
    "Executing the code block below generates a sample from an untrained model and prints the individual tokens to highlight that we are not working at the character level anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple multi-layer perceptron with two linear layers and a relu non-linearity in between\"\"\"\n",
    "    def __init__(self, embedding_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=4*embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4*embedding_dim, out_features=embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "class MultiHeadDotProductAttention(nn.Module):\n",
    "    \"\"\"Multi Head Dot Product attention\"\"\"\n",
    "    def __init__(self, embedding_dim: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        if embedding_dim % n_heads != 0:\n",
    "            raise Exception('n_heads must be dividable by n_embed')\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # attention input projections\n",
    "        self.w_q = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_k = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_v = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        # output projection\n",
    "        self.out_project = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        #dropouts\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.projection_dropout = nn.Dropout(dropout)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        # B, L, N\n",
    "        # N = n_heads x head_size\n",
    "        batch_dim, input_length, embedding_dim = x.size()\n",
    "\n",
    "        # calculate input projections and divide over heads\n",
    "        # 'view' and 'transpose' reorder in subtly different ways and we need both\n",
    "        # (B, L, n_heads, head_dim) -> (B, n_heads, L, head_dim)\n",
    "        q = self.w_q(x).view(batch_dim, input_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "        k = self.w_k(x).view(batch_dim, input_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "        v = self.w_v(x).view(batch_dim, input_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "\n",
    "        # use fast cuda kernel for attention\n",
    "        pred = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "\n",
    "        # reshape multiple heads back into contiguous representation\n",
    "        pred = pred.transpose(1, 2).contiguous().view(batch_dim, input_length, embedding_dim)\n",
    "\n",
    "        # return linear projection\n",
    "        return self.projection_dropout(self.out_project(pred))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block that combines attention and MLP, both with pre-layernorm and residual connections\"\"\"\n",
    "    def __init__(self, embedding_dim: int, n_heads:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            MultiHeadDotProductAttention(\n",
    "                embedding_dim=embedding_dim,\n",
    "                n_heads=n_heads,\n",
    "                dropout=dropout,\n",
    "            ),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            MLP(embedding_dim=embedding_dim, dropout=dropout),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate attention and communication between channels, both with residual connections\"\"\"\n",
    "        # Communicate between positions (i.e. attention)\n",
    "        attn = self.attention(x) + x\n",
    "        # Communicate between embedding dimensions\n",
    "        res = self.mlp(attn) + attn\n",
    "        return res\n",
    "\n",
    "class AdditivePositionalEmbedding(nn.Module):\n",
    "    \"\"\"Wrapper class to add positional encoding to already embedded tokens\"\"\"\n",
    "    def __init__(self, context_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=context_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Add positional embeddings based on input dimensions, use residual connection\"\"\"\n",
    "        pos = torch.arange(0, x.size(1), dtype=torch.long, device=x.device)\n",
    "        return self.embedding(pos) + x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_size: int=None,\n",
    "        vocab_size: int=None,\n",
    "        n_layers: int=6,\n",
    "        n_heads: int=8,\n",
    "        embedding_dim: int=384,\n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim),\n",
    "            AdditivePositionalEmbedding(context_size, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[TransformerBlock(embedding_dim=embedding_dim, n_heads=n_heads, dropout=dropout) for _ in range(n_layers)]),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, out_features=vocab_size)\n",
    "        )\n",
    "\n",
    "        # weight tying of input embedding and output projection (https://paperswithcode.com/method/weight-tying)\n",
    "        self.transformer[0].weight = self.transformer[-1].weight\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None) -> torch.tensor:\n",
    "        logits = self.transformer(idx)\n",
    "        loss = None if targets is None else F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        accuracy = None if targets is None else (logits.argmax(dim=-1) == targets).sum() / targets.numel()\n",
    "        return logits,loss,accuracy\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt:str='\\n',\n",
    "        sample_length: int=256,\n",
    "        greedy: bool=True,\n",
    "        top_k: int=None,\n",
    "        temperature: float=1.0\n",
    "    ) -> list[int]:\n",
    "        \"\"\"Generate sample tokens using a variety of strategies: greedy, stochastic, top-k. Temperature scales model logits\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        prompt_tokens = dataset.tokenizer.encode(prompt).ids\n",
    "        idx = torch.tensor(prompt_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "        for _ in trange(sample_length, desc='Generating sample'):\n",
    "            logits,_,_ = self(idx[-self.context_size:][None])\n",
    "            logits = logits[0,-1,:]\n",
    "\n",
    "            if greedy:\n",
    "                idx_next = logits.argmax()[None]\n",
    "            else:\n",
    "                if top_k:\n",
    "                    logits[logits.argsort()[:-top_k]] = -torch.inf\n",
    "                if temperature:\n",
    "                    logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=0)\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        return idx.tolist()\n",
    "\n",
    "\n",
    "model = GPT(context_size=dataset.context_size, vocab_size=dataset.vocab_size)\n",
    "sample = model.generate('I am', sample_length=16)\n",
    "print([dataset.tokenizer.decode([s]) for s in sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "We specify a GPT transformer with a context size of 256 tokens, 8 `TransformerBlock` layers, 128 embedding dimensions divided over 16 attention heads. We train for 1000 steps using a batch size of 16, which should take ±3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TextDataset.from_textfile('./input.txt', tokenizer=tokenizer, context_size=256)\n",
    "\n",
    "model = GPT(\n",
    "    context_size=dataset.context_size,\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_dim=128,\n",
    "    n_layers=8,\n",
    "    n_heads=16,\n",
    "    dropout=0.2\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "train_steps = 2_000\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset, test_dataset = dataset.train_test_split()\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset, num_samples=train_steps * batch_size),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=RandomSampler(test_dataset, replacement=True),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dataloader = iter(test_dataloader)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)\n",
    "model.train()\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for i, (train_x, train_y) in enumerate(tqdm(train_dataloader)):\n",
    "    # forward the model\n",
    "    _,train_loss,train_accuracy = model(train_x.to(DEVICE), train_y.to(DEVICE))\n",
    "\n",
    "    # save losses on train and test every 20 iterations\n",
    "    if i % 20 == 0:\n",
    "        train_losses.append(train_loss.item())\n",
    "        train_accuracies.append(train_accuracy.item())\n",
    "        test_x, test_y = next(test_dataloader)\n",
    "        _,test_loss,test_accuracy = model(test_x.to(DEVICE), test_y.to(DEVICE))\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accuracies.append(test_accuracy.item())\n",
    "\n",
    "    # backprop and update the parameters\n",
    "    model.zero_grad(set_to_none=True)\n",
    "    train_loss.backward()\n",
    "\n",
    "    # prevent gradients from exploding\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "\n",
    "fig,[ax1, ax2] = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ax1.plot(train_losses, label='train loss')\n",
    "ax1.plot(test_losses, label='test loss')\n",
    "ax1.legend()\n",
    "ax2.plot(train_accuracies, label='train accuracy')\n",
    "ax2.plot(test_accuracies, label='test accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Train a GPT on the tinyshakespeare dataset using the configuration provided in the notebook. Use the codeblock below to perform several prompting and sampling experiments.\n",
    "- Generate a sample using greedy decoding, and a few using stochastic decoding (i.e. `greedy=False`). What difference do you find?\n",
    "- Generate a sample using stochastic decoding, but specify `top_k=1`. What do you notice?\n",
    "- Generate two samples using stochastic decoding, one with `temperature=0.01` and one with `temperature=10.0`. What do you notice?\n",
    "- Experiment with specifying a different prompt, use stochastic decoding with `top_k=5` and `temperature=1.0`. Is there variation in the quality of the results when you specify different prompts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sample = model.generate(prompt='Aluminium', greedy=False, top_k=5)\n",
    "print(dataset.tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "### Exercise 1\n",
    "- Greedy sample is not very good, and always the same. Stochastic samples often (but not always) capture more of the structure found in the tinyshakespeare dataset (like double whitespaces and character names in all capital letters). Stochastic sample feels more 'real'\n",
    "- `top_k=1` is the same as greedy decoding!\n",
    "- Low temperature sample is very similar to greedy decoding, high temperature looks a lot like untrained model\n",
    "- Our model is still not producing very readable text so it can be difficult to judge, but for example prompting with 'DUKE ' often leads to samples that contain the word 'lord' a lot. Prompting with 'Be gone ' is almost always followed by the words 'of' or 'to the'. Prompting 'KING RICHARD ' sometimes results in varieties of the name that are in the training set such as 'KING RICHARD II' and 'KING RICHARD III' but also in reasonable sounding varieties that are not the data such as 'KING RICHARD OF YORK'. Prompting 'Aluminium' never gets recognized as a word that should be followed by a space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
