{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "In this notebook we continue with the GPT2 architecture we developed earlier, but this time we focus on strategies from generating samples from a trained model (i.e. prompting). We download some pretrained weights, similar to yesterday's bonus exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download pretrained weights based on three different datasets\n",
    "!wget -nc https://github.com/holmrenser/deep_learning/raw/refs/heads/main/shakespeare.model.pkl\n",
    "!wget -nc https://github.com/holmrenser/deep_learning/raw/refs/heads/main/war_peace_plain.model.pkl\n",
    "!wget -nc https://github.com/holmrenser/deep_learning/raw/refs/heads/main/openwebtext.model.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "from tqdm.auto import tqdm, trange\n",
    "import pickle\n",
    "import math \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Below is a full implementation of a GPT2-style transformer, identical to what we created yesterday.\n",
    "\n",
    "The biggest difference compared to our earlier model is in the `generate` method of the `GPT` class, which now includes several options to configure the way we sample tokens:\n",
    "- Greedy generation: at each sampling iteration picks the token with the highest probability\n",
    "- Stochastic generation: at each iteration sample from a distribution with probabilities determined by our model (this is what we have done so far)\n",
    "- Top-K sampling: sample tokens from a distribution, but only the k most likely tokens get a non-zero probability\n",
    "- Temperature scaling: scale logits by a 'temperature' value, lower temperatures lead to sampling from a more 'spiked' probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Character level tokenizer that enumerates the first 256 unicode characters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 256\n",
    "        self.encoding_dict = {chr(token_i): token_i for token_i in range(256)}\n",
    "        self.decoding_dict = {token_i: chr(token_i) for token_i in range(256)}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'CharacterTokenizer(vocab_size={self.vocab_size})'\n",
    "\n",
    "    def get_vocab(self) -> dict[str, int]:\n",
    "        return self.encoding_dict\n",
    "\n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        return [self.encoding_dict.get(char, -1) for char in data]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        return ''.join(self.decoding_dict.get(token, '<unk>') for token in tokens)\n",
    "\n",
    "class MultiheadDotProductAttention(nn.Module):\n",
    "    \"\"\"Multihead dot product softmax attention\"\"\"\n",
    "    def __init__(self, embedding_dim: int, n_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        if embedding_dim % n_heads != 0:\n",
    "            raise Exception('n_heads must be dividable by n_embed')\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # attention input projections\n",
    "        self.w_q = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_k = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_v = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        # output projection\n",
    "        self.out_project = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "        #dropouts\n",
    "        self.attention_dropout = nn.Dropout(dropout)\n",
    "        self.projection_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate multihead attention, expects input of shape (batch, context_length, embedding_dim)\"\"\"\n",
    "        batch_dim, context_length, embedding_dim = x.size()\n",
    "\n",
    "        # calculate input projections and divide over heads\n",
    "        # 'view' and 'transpose' reorder in subtly different ways and we need both\n",
    "        # (B, L, n_heads, head_dim) -> (B, n_heads, L, head_dim)\n",
    "        q = self.w_q(x).view(batch_dim, context_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "        k = self.w_k(x).view(batch_dim, context_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "        v = self.w_v(x).view(batch_dim, context_length, self.n_heads, embedding_dim // self.n_heads).transpose(1,2)\n",
    "\n",
    "        # calculate attention\n",
    "        # (B, n_heads, L, head_size) x (B, n_heads, head_size, L) -> (B, n_heads, L, L)\n",
    "        attention = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(embedding_dim))\n",
    "        # Apply causal attention mask\n",
    "        mask = torch.triu(torch.ones(context_length, context_length, dtype=torch.bool, device=q.device), diagonal=1)\n",
    "        attention = attention.masked_fill(mask, -torch.inf)\n",
    "\n",
    "        # Calculate row-wise logits\n",
    "        attention = F.softmax(attention, dim=-1)\n",
    "\n",
    "        # Random dropout of the attention matrix\n",
    "        attention = self.attention_dropout(attention)\n",
    "\n",
    "        # weight outputs with calculated attention\n",
    "        # (B, n_heads, L, L) x (B, n_heads, L, head_dim) -> (B, n_heads, L, head_dim)\n",
    "        pred = attention @ v\n",
    "\n",
    "        # reshape multiple heads back into contiguous representation\n",
    "        pred = pred.transpose(1, 2).contiguous().view(batch_dim, context_length, embedding_dim)\n",
    "\n",
    "        # return linear projection\n",
    "        return self.projection_dropout(self.out_project(pred))\n",
    "\n",
    "class PositionWiseMLP(nn.Module):\n",
    "    \"\"\"Position-wise feedforward MLP: simple multi-layer perceptron for position-wise exchange of information between channels\"\"\"\n",
    "    def __init__(self, embedding_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=4*embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4*embedding_dim, out_features=embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block that combines attention and FeedforwardMLP,\n",
    "    both with layer normalization and residual connections\"\"\"\n",
    "    def __init__(self, embedding_dim: int, n_heads:int, dropout:float):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            MultiheadDotProductAttention(\n",
    "                embedding_dim=embedding_dim,\n",
    "                n_heads=n_heads,\n",
    "                dropout=dropout\n",
    "            )\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            PositionWiseMLP(embedding_dim=embedding_dim, dropout=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate attention and communication between channels, both with residual connections\"\"\"\n",
    "        # Communicate between positions (i.e. attention)\n",
    "        attn = self.attention(x) + x\n",
    "        # Communicate between embedding dimensions (i.e. channels)\n",
    "        res = self.mlp(attn) + attn\n",
    "        return res\n",
    "\n",
    "class AdditivePositionalEmbedding(nn.Module):\n",
    "    \"\"\"Wrapper class to add positional encoding to already embedded tokens\"\"\"\n",
    "    def __init__(self, context_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=context_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Add positional embeddings based on input dimensions, use residual connection\"\"\"\n",
    "        pos = torch.arange(0, x.size(1), dtype=torch.long, device=x.device)\n",
    "        return self.embedding(pos) + x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        context_size: int,\n",
    "        tokenizer: CharacterTokenizer,\n",
    "        n_layers: int=6,\n",
    "        n_heads: int=8,\n",
    "        embedding_dim: int=32,\n",
    "        dropout: float=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_layers = n_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # transformer architecture (ref. our naive transformer, only difference is in the transformer block)\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=embedding_dim),\n",
    "            AdditivePositionalEmbedding(context_size, embedding_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Sequential(*[\n",
    "                TransformerBlock(embedding_dim=embedding_dim, n_heads=n_heads, dropout=dropout)\n",
    "                for _ in range(n_layers)\n",
    "            ]),\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, out_features=self.vocab_size)\n",
    "        )\n",
    "\n",
    "        # weight tying of input embedding and output projection (https://paperswithcode.com/method/weight-tying)\n",
    "        self.transformer[0].weight = self.transformer[-1].weight\n",
    "\n",
    "        # initialize all weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        context_size = self.context_size\n",
    "        vocab_size = self.vocab_size\n",
    "        n_attention_layers = self.n_layers\n",
    "        n_heads = self.n_heads\n",
    "        embedding_dim = self.embedding_dim\n",
    "        dropout = self.dropout\n",
    "        num_params = sum(p.numel() for p in self.parameters())\n",
    "        return f'GPT({num_params=}, {context_size=}, {vocab_size=}, {n_attention_layers=}, {n_heads=}, {embedding_dim=}, {dropout=})'\n",
    "\n",
    "    def save(self, filename: str = 'model.pkl') -> None:\n",
    "      \"\"\"Saves all relevant model parameters and weights to a file to reload later\"\"\"\n",
    "      # Identify how the model was initialized\n",
    "      init_params = {k:v for k,v in self.__dict__.items() if k[0] != '_' and k not in ['training','tokenizer','vocab_size']}\n",
    "      # Get model weights\n",
    "      state_dict = {k: v.to('cpu') for k, v in self.state_dict().items()}\n",
    "      # Combine initialization params and weights into a single dict\n",
    "      param_dict = dict(init_params=init_params, state_dict=state_dict)\n",
    "      # save to file\n",
    "      with open(filename,'wb') as fh:\n",
    "        pickle.dump(param_dict, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load_pretrained(cls, filename: str = 'model.pkl') -> 'GPT':\n",
    "      \"\"\"Loads a pretrained model\"\"\"\n",
    "      # Load params and weights from file\n",
    "      with open(filename,'rb') as fh:\n",
    "        param_dict = pickle.load(fh)\n",
    "      # Initialize model with previous init params\n",
    "      model = cls(**param_dict['init_params'], tokenizer=CharacterTokenizer())\n",
    "      # Apply pretrained model weights\n",
    "      model.load_state_dict({k:v.to(DEVICE) for k,v in param_dict['state_dict'].items()})\n",
    "      return model\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor=None) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        logits = self.transformer(tokens)\n",
    "        loss = None if targets is None else F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        accuracy = None if targets is None else (logits.argmax(dim=-1) == targets).sum() / targets.numel()\n",
    "        return logits, loss, accuracy\n",
    "\n",
    "    def _init_weights(self, module: nn.Module) -> None:\n",
    "        \"\"\"Empirically this seems to be a good way to initialize\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def generate(self, prompt: str = None, sample_length: int=256, greedy:bool=False, top_k:int=None, temperature: float=None) -> str:\n",
    "        \"\"\"Generate text from the model, with optional prompt to prime the generation and various decoding strategies\"\"\"\n",
    "        self.eval()\n",
    "        device = next(self.parameters()).device\n",
    "        if prompt is None:\n",
    "            tokens = torch.zeros(1, dtype=torch.long, device=device)\n",
    "        else:\n",
    "            tokens = torch.tensor(self.tokenizer.encode(prompt), dtype=torch.long, device=device)\n",
    "\n",
    "        for _ in trange(sample_length, desc='Generating sample'):\n",
    "            logits,_,_ = self(tokens[-self.context_size:][None])\n",
    "            logits = logits[0,-1,:]\n",
    "\n",
    "            if greedy:\n",
    "                next_token = logits.argmax()[None]\n",
    "            else:\n",
    "                if top_k:\n",
    "                    logits[logits.argsort()[:-top_k]] = -torch.inf\n",
    "                if temperature:\n",
    "                    logits = logits / temperature\n",
    "                probs = F.softmax(logits, dim=0)\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "            tokens = torch.cat([tokens, next_token])\n",
    "        tokens = tokens.tolist() # move from tensor on gpu to list on cpu\n",
    "        return self.tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Load one of the pretrained models using the codeblock below and perform several prompting and sampling experiments.\n",
    "- Generate a sample using greedy decoding, and a few using stochastic decoding (i.e. `greedy=False`). What difference do you find?\n",
    "- Generate a sample using stochastic decoding, but specify `top_k=1`. What do you notice?\n",
    "- Generate two samples using stochastic decoding, one with `temperature=0.01` and one with `temperature=10.0`. What do you notice?\n",
    "- Experiment with specifying a different prompt, use stochastic decoding with `top_k=5` and `temperature=1.0`. Is there variation in the quality of the results when you specify different prompts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = GPT.load_pretrained('shakespeare.model.pkl')\n",
    "print(pretrained_model)\n",
    "sample = pretrained_model.generate('KING')\n",
    "print(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
