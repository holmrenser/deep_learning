{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers\n",
    "### Exercise 1\n",
    "- Greedy sample is not very good, and always the same. Stochastic samples often (but not always) capture more of the structure found in the tinyshakespeare dataset (like double whitespaces and character names in all capital letters). Stochastic sample feels more 'real'\n",
    "- `top_k=1` is the same as greedy decoding!\n",
    "- Low temperature sample is very similar to greedy decoding, high temperature looks a lot like untrained model\n",
    "- Our model is still not producing very readable text so it can be difficult to judge, but for example prompting with 'DUKE ' often leads to samples that contain the word 'lord' a lot. Prompting with 'Be gone ' is almost always followed by the words 'of' or 'to the'. Prompting 'KING RICHARD ' sometimes results in varieties of the name that are in the training set such as 'KING RICHARD II' and 'KING RICHARD III' but also in reasonable sounding varieties that are not the data such as 'KING RICHARD OF YORK'. Prompting 'Aluminium' never gets recognized as a word that should be followed by a space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
