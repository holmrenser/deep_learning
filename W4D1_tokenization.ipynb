{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a67439a-7953-41aa-b8ac-d010d1c4867a",
   "metadata": {
    "id": "0a67439a-7953-41aa-b8ac-d010d1c4867a"
   },
   "source": [
    "# Tokenization\n",
    "Machine learning approaches for natural language processing face the problem of representing long stretches of natural language (i.e. words, sentences, paragraphs, chapters, etc.) in a meaningful and computationally efficient way. A trivial approach is to split text on interpunction and whitespace, effectively selecting individual words. The downside of this approach is that semantically similar words are encoded differently. For example, 'small', 'smaller', and 'smallest', would all be encoded as different entities, forcing a model to learn any semantic similarity from data alone. An alternative approach would encode 'small' as one entity, and 'er', and 'est' as separate entities. The benefit of this 'subword' approach is that it is more straightforward to model semantic similarity, the downside is that it is not straightforward to identify an optimal subword selection scheme.\n",
    "\n",
    "In this notebook we will explore the [byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) algorithm for creating subword representations, a.k.a. tokens. Put simply, byte pair encoding iteratively merges the most frequent token pair into a new token, starting from the most simple tokens (e.g. letters), and continuing until the desired number of tokens is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2713507d-28cf-49a9-9847-7cd1f5407a38",
   "metadata": {
    "id": "2713507d-28cf-49a9-9847-7cd1f5407a38"
   },
   "outputs": [],
   "source": [
    "# All dependencies for the whole notebook\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm.auto import trange\n",
    "import json\n",
    "from typing import Generator\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcca5c50-e037-40d5-9ac7-f8903395c9db",
   "metadata": {
    "id": "dcca5c50-e037-40d5-9ac7-f8903395c9db"
   },
   "source": [
    "## Data\n",
    "We use the 'tiny shakespeare' dataset, which is a single textfile of ~1MB that contains all work of shakespeare. It has some interesting characteristics for which it is easy to evaluate if a model captures them well, such repeated newlines characters, all capital names, and specific interpunction used when describing theatrical plays/conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "079b9986-aa40-4a63-a6f3-e9c5efea262b",
   "metadata": {
    "id": "079b9986-aa40-4a63-a6f3-e9c5efea262b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8796b7-393e-4c41-9af8-a9468a689f8d",
   "metadata": {
    "id": "5a8796b7-393e-4c41-9af8-a9468a689f8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tiny shakespeare data and show the first 100 characters\n",
    "with open('input.txt', 'r') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74ffeb-4785-44e8-99d2-62eb7f2eb5cf",
   "metadata": {
    "id": "cb74ffeb-4785-44e8-99d2-62eb7f2eb5cf"
   },
   "source": [
    "## Naive 'tokens'\n",
    "Probably the simplest tokenization strategy is to assign integers to individual characters in a given dataset. We'll implement this strategy below with a few lines of code. We create two dictionaries that function as lookup table: one encoding characters to integers, and one decoding integers back to characters. To apply our lookup tables we use a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9439091-ae2b-467b-b807-5c64cd0ade56",
   "metadata": {
    "id": "e9439091-ae2b-467b-b807-5c64cd0ade56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_text = 'First Citizen:\\nBefore we proce'\n",
      "tokens = [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43]\n",
      "chars = ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "# Calling 'set' on our data returns all individual characters, which are then lexicographically sorted\n",
    "chars = sorted(set(data))\n",
    "# Calling 'enumerate' returns the original iterator and increasing integers, which we'll use as tokens\n",
    "char_to_token = {char:token for token,char in enumerate(chars)}\n",
    "# Reverse the mapping to be able to decode\n",
    "token_to_char = {token:char for char,token in char_to_token.items()}\n",
    "\n",
    "some_text = data[:30]\n",
    "print(f'{some_text = }')\n",
    "\n",
    "# Encode the first 30 characters of the tiny shakespeare dataset\n",
    "tokens = [char_to_token[c] for c in some_text]\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Check that we retrieve our original text when decoding\n",
    "chars = [token_to_char[t] for t in tokens]\n",
    "print(f'{chars = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0830d5d-9c19-452a-81a6-4f98568bbd29",
   "metadata": {
    "id": "f0830d5d-9c19-452a-81a6-4f98568bbd29"
   },
   "source": [
    "Below we add a bit more functionality and structure to the idea presented above. This allows us to more efficiently use our mappings as tokenizer, pass the tokenizer around more easily, and aligns with code conventions used in many of-the-shelf tokenizer libraries. The main elements are the same as above, e.g. the 'encoding_dict' is named 'char_to_token' in the example above, etc. Training (i.e. enumerating characters in a dataset) can be done with the `train` method, going from a string to a list of tokens is done with the `encode` method, going from a list of tokens back to a string is done with the `decode` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73292fcd-e9af-47a2-91d8-09b69748f246",
   "metadata": {
    "id": "73292fcd-e9af-47a2-91d8-09b69748f246"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained tokenizer: CharacterTokenizer(vocab_size=0)\n",
      "Trained tokenizer: CharacterTokenizer(vocab_size=65)\n",
      "tokens = [20, 47, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59]\n",
      "Hi how are you\n"
     ]
    }
   ],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Character level tokenizer that enumerates unique characters in a training text\"\"\"\n",
    "    def __init__(self, encoding_dict: dict[str, int]=None):\n",
    "        if encoding_dict is None:\n",
    "            self.encoding_dict = dict()\n",
    "        else:\n",
    "            self.encoding_dict = encoding_dict\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'CharacterTokenizer(vocab_size={self.vocab_size})'\n",
    "\n",
    "    @property\n",
    "    def decoding_dict(self) -> dict[int, str]:\n",
    "        \"\"\"Decoding dict is implemented as property to automatically sync with changed encoding dict\"\"\"\n",
    "        return {token:char for char,token in self.encoding_dict.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.encoding_dict)\n",
    "\n",
    "    def get_vocab(self) -> dict[str, int]:\n",
    "        return self.encoding_dict\n",
    "\n",
    "    def train(self, data: str) -> None:\n",
    "        \"\"\"Train on a piece of text by enumerating unique characters\"\"\"\n",
    "        chars = sorted(set(data))\n",
    "        self.encoding_dict = {char:token for token,char in enumerate(chars)}\n",
    "\n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        return [self.encoding_dict.get(char, -1) for char in data]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        return ''.join(self.decoding_dict.get(token, '<unk>') for token in tokens)\n",
    "\n",
    "# Initialize an empty NaiveTokenizer\n",
    "tokenizer = CharacterTokenizer()\n",
    "print(f'Untrained tokenizer: {tokenizer}')\n",
    "\n",
    "# 'Train' on tiny shakespeare\n",
    "tokenizer.train(data)\n",
    "print(f'Trained tokenizer: {tokenizer}')\n",
    "\n",
    "# Encode a string that is not in the training data\n",
    "tokens = tokenizer.encode('Hi how are you')\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Decode the encoding\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wD_pIdx8ZsD5",
   "metadata": {
    "id": "wD_pIdx8ZsD5"
   },
   "source": [
    "### Exercise 1\n",
    "Investigate the vocabulary of the trained tokenizer by printing `tokenizer.get_vocab()`. Can you come up with a string that cannot be effectively encoded by our naive tokenizer? What happens to this string? How would you circumvent this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172bf54-0ea4-4820-88f1-ca27550abaee",
   "metadata": {
    "id": "e172bf54-0ea4-4820-88f1-ca27550abaee"
   },
   "source": [
    "\n",
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "#### Converting to and from bytes\n",
    "Apart from not being able to process unseen characters, tokenizing by enumerating characters in a training text has another issue: different training datasets can assign different tokens to the same character. The most common solution to these problems is to encode characters using [Unicode](https://en.wikipedia.org/wiki/Unicode) codepoints, specifically [UTF-8](https://en.wikipedia.org/wiki/UTF-8). Without going into too much detail, UTF-8 uses up to 4 bytes to encode individual characters, where every codepoint (i.e. byte or byte sequence) can be interpreted as an integer. The byte pair encoding algorithm iteratively merges these unicode codepoints.\n",
    "\n",
    "In python, converting individual characters to unicode codepoints and back can be done with the built in `ord` and `chr` functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531b0b2b-8ad1-459f-9438-530d62e0310c",
   "metadata": {
    "id": "531b0b2b-8ad1-459f-9438-530d62e0310c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single character to unicode\n",
    "ord('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "012763b9-3f7c-4ace-9fb9-7f3889375f0c",
   "metadata": {
    "id": "012763b9-3f7c-4ace-9fb9-7f3889375f0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single unicode codepoint to character\n",
    "chr(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cb18ada-42e4-4124-bfd0-af20914cf1c0",
   "metadata": {
    "id": "0cb18ada-42e4-4124-bfd0-af20914cf1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unicode_codepoints = [68, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 105, 115, 32, 97, 119, 101, 115, 111, 109, 101]\n",
      "characters = ['D', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e']\n"
     ]
    }
   ],
   "source": [
    "# Converting a string to unicode and back\n",
    "some_text = 'Deep learning is awesome'\n",
    "\n",
    "unicode_codepoints = [ord(letter) for letter in some_text]\n",
    "print(f'{unicode_codepoints = }')\n",
    "\n",
    "characters = [chr(codepoint) for codepoint in unicode_codepoints]\n",
    "print(f'{characters = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69399373-d6ef-40ae-8103-ab02b8d53f93",
   "metadata": {
    "id": "69399373-d6ef-40ae-8103-ab02b8d53f93"
   },
   "source": [
    "The same principles outlined above can be applied to multi-character strings with a slightly different syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47498365-3d17-4771-abd1-3141bc0b8a16",
   "metadata": {
    "id": "47498365-3d17-4771-abd1-3141bc0b8a16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_bytes = b'Deep learning is awesome'\n",
      "tokens = [68, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 105, 115, 32, 97, 119, 101, 115, 111, 109, 101]\n",
      "reconstructed_text = 'Deep learning is awesome'\n"
     ]
    }
   ],
   "source": [
    "some_text = 'Deep learning is awesome'\n",
    "\n",
    "# Using the 'encode' method on a string converts to bytes, note the leading 'b' when printing\n",
    "text_bytes = some_text.encode('utf-8')\n",
    "print(f'{text_bytes = }')\n",
    "\n",
    "# The list constructor iterates over the bytes, automatically converting to integers\n",
    "tokens = list(text_bytes)\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Turning a list of integers into bytes and subsequently 'decoding' into text\n",
    "reconstructed_text = bytes(tokens).decode('utf-8')\n",
    "print(f'{reconstructed_text = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fef04-93e7-4d7d-987f-8e82f365f85b",
   "metadata": {
    "id": "e55fef04-93e7-4d7d-987f-8e82f365f85b"
   },
   "source": [
    "#### Counting pairs\n",
    "Byte pair encoding iteratively merges the most frequent pairs of bytes. We use some built in python functionality to count pairs in an iterator (the example uses characters, BPE  uses bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f758176e-ec02-4e3d-a50a-ad3d8f42f76c",
   "metadata": {
    "id": "f758176e-ec02-4e3d-a50a-ad3d8f42f76c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_pair = [(('r', 'e'), 5)]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = 'A text with some repetition somesome reprepreprepetition'\n",
    "\n",
    "# zip together original and shifted sequence\n",
    "pairs = zip(example_sequence[:-1], example_sequence[1:])\n",
    "\n",
    "# count using inbuilt counter from collections module (part of python standard lib)\n",
    "pair_counts = Counter(pairs)\n",
    "\n",
    "# select most common pair\n",
    "most_common_pair = pair_counts.most_common(1)\n",
    "print(f'{most_common_pair = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdb4e7-fa75-4326-b3a4-7074018f041a",
   "metadata": {
    "id": "f2cdb4e7-fa75-4326-b3a4-7074018f041a"
   },
   "source": [
    "#### BPE implementation\n",
    "Below we implement a function to merge token pairs and some functionality to train the tokenizer, encode strings, decode tokens, and save and load trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63191281-6333-4986-97dc-19ed433da6d4",
   "metadata": {
    "id": "63191281-6333-4986-97dc-19ed433da6d4",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " '\\x05': 5,\n",
       " '\\x06': 6,\n",
       " '\\x07': 7,\n",
       " '\\x08': 8,\n",
       " '\\t': 9,\n",
       " '\\n': 10,\n",
       " '\\x0b': 11,\n",
       " '\\x0c': 12,\n",
       " '\\r': 13,\n",
       " '\\x0e': 14,\n",
       " '\\x0f': 15,\n",
       " '\\x10': 16,\n",
       " '\\x11': 17,\n",
       " '\\x12': 18,\n",
       " '\\x13': 19,\n",
       " '\\x14': 20,\n",
       " '\\x15': 21,\n",
       " '\\x16': 22,\n",
       " '\\x17': 23,\n",
       " '\\x18': 24,\n",
       " '\\x19': 25,\n",
       " '\\x1a': 26,\n",
       " '\\x1b': 27,\n",
       " '\\x1c': 28,\n",
       " '\\x1d': 29,\n",
       " '\\x1e': 30,\n",
       " '\\x1f': 31,\n",
       " ' ': 32,\n",
       " '!': 33,\n",
       " '\"': 34,\n",
       " '#': 35,\n",
       " '$': 36,\n",
       " '%': 37,\n",
       " '&': 38,\n",
       " \"'\": 39,\n",
       " '(': 40,\n",
       " ')': 41,\n",
       " '*': 42,\n",
       " '+': 43,\n",
       " ',': 44,\n",
       " '-': 45,\n",
       " '.': 46,\n",
       " '/': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " '2': 50,\n",
       " '3': 51,\n",
       " '4': 52,\n",
       " '5': 53,\n",
       " '6': 54,\n",
       " '7': 55,\n",
       " '8': 56,\n",
       " '9': 57,\n",
       " ':': 58,\n",
       " ';': 59,\n",
       " '<': 60,\n",
       " '=': 61,\n",
       " '>': 62,\n",
       " '?': 63,\n",
       " '@': 64,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90,\n",
       " '[': 91,\n",
       " '\\\\': 92,\n",
       " ']': 93,\n",
       " '^': 94,\n",
       " '_': 95,\n",
       " '`': 96,\n",
       " 'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " '{': 123,\n",
       " '|': 124,\n",
       " '}': 125,\n",
       " '~': 126,\n",
       " '\\x7f': 127,\n",
       " '\\x80': 128,\n",
       " '\\x81': 129,\n",
       " '\\x82': 130,\n",
       " '\\x83': 131,\n",
       " '\\x84': 132,\n",
       " '\\x85': 133,\n",
       " '\\x86': 134,\n",
       " '\\x87': 135,\n",
       " '\\x88': 136,\n",
       " '\\x89': 137,\n",
       " '\\x8a': 138,\n",
       " '\\x8b': 139,\n",
       " '\\x8c': 140,\n",
       " '\\x8d': 141,\n",
       " '\\x8e': 142,\n",
       " '\\x8f': 143,\n",
       " '\\x90': 144,\n",
       " '\\x91': 145,\n",
       " '\\x92': 146,\n",
       " '\\x93': 147,\n",
       " '\\x94': 148,\n",
       " '\\x95': 149,\n",
       " '\\x96': 150,\n",
       " '\\x97': 151,\n",
       " '\\x98': 152,\n",
       " '\\x99': 153,\n",
       " '\\x9a': 154,\n",
       " '\\x9b': 155,\n",
       " '\\x9c': 156,\n",
       " '\\x9d': 157,\n",
       " '\\x9e': 158,\n",
       " '\\x9f': 159,\n",
       " '\\xa0': 160,\n",
       " '¡': 161,\n",
       " '¢': 162,\n",
       " '£': 163,\n",
       " '¤': 164,\n",
       " '¥': 165,\n",
       " '¦': 166,\n",
       " '§': 167,\n",
       " '¨': 168,\n",
       " '©': 169,\n",
       " 'ª': 170,\n",
       " '«': 171,\n",
       " '¬': 172,\n",
       " '\\xad': 173,\n",
       " '®': 174,\n",
       " '¯': 175,\n",
       " '°': 176,\n",
       " '±': 177,\n",
       " '²': 178,\n",
       " '³': 179,\n",
       " '´': 180,\n",
       " 'µ': 181,\n",
       " '¶': 182,\n",
       " '·': 183,\n",
       " '¸': 184,\n",
       " '¹': 185,\n",
       " 'º': 186,\n",
       " '»': 187,\n",
       " '¼': 188,\n",
       " '½': 189,\n",
       " '¾': 190,\n",
       " '¿': 191,\n",
       " 'À': 192,\n",
       " 'Á': 193,\n",
       " 'Â': 194,\n",
       " 'Ã': 195,\n",
       " 'Ä': 196,\n",
       " 'Å': 197,\n",
       " 'Æ': 198,\n",
       " 'Ç': 199,\n",
       " 'È': 200,\n",
       " 'É': 201,\n",
       " 'Ê': 202,\n",
       " 'Ë': 203,\n",
       " 'Ì': 204,\n",
       " 'Í': 205,\n",
       " 'Î': 206,\n",
       " 'Ï': 207,\n",
       " 'Ð': 208,\n",
       " 'Ñ': 209,\n",
       " 'Ò': 210,\n",
       " 'Ó': 211,\n",
       " 'Ô': 212,\n",
       " 'Õ': 213,\n",
       " 'Ö': 214,\n",
       " '×': 215,\n",
       " 'Ø': 216,\n",
       " 'Ù': 217,\n",
       " 'Ú': 218,\n",
       " 'Û': 219,\n",
       " 'Ü': 220,\n",
       " 'Ý': 221,\n",
       " 'Þ': 222,\n",
       " 'ß': 223,\n",
       " 'à': 224,\n",
       " 'á': 225,\n",
       " 'â': 226,\n",
       " 'ã': 227,\n",
       " 'ä': 228,\n",
       " 'å': 229,\n",
       " 'æ': 230,\n",
       " 'ç': 231,\n",
       " 'è': 232,\n",
       " 'é': 233,\n",
       " 'ê': 234,\n",
       " 'ë': 235,\n",
       " 'ì': 236,\n",
       " 'í': 237,\n",
       " 'î': 238,\n",
       " 'ï': 239,\n",
       " 'ð': 240,\n",
       " 'ñ': 241,\n",
       " 'ò': 242,\n",
       " 'ó': 243,\n",
       " 'ô': 244,\n",
       " 'õ': 245,\n",
       " 'ö': 246,\n",
       " '÷': 247,\n",
       " 'ø': 248,\n",
       " 'ù': 249,\n",
       " 'ú': 250,\n",
       " 'û': 251,\n",
       " 'ü': 252,\n",
       " 'ý': 253,\n",
       " 'þ': 254,\n",
       " 'ÿ': 255}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tokens(tokens: list[int], token_pair: tuple[int,int], new_token: int) -> list[int]:\n",
    "    \"\"\"Takes a list of tokens and replaces every occurence of token_pair with new_token\"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    # Iterate in a while loop because we want to jump ahead two steps sometimes\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        # Edge case: final individual token\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(token)\n",
    "            break\n",
    "        # Look ahead one token to find a token pair\n",
    "        next_token = tokens[i+1]\n",
    "        # On match we should jump ahead two tokens to skip the original pair\n",
    "        if token_pair == (token, next_token):\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "class BytePairEncoder:\n",
    "    base_vocab_size = 256\n",
    "    \"\"\"Bytepair encoder with a base vocabulary of the first 256 utf-8 codepoints (this captures all 'normal' alphanumeric characters)\"\"\"\n",
    "    def __init__(self, merges: dict[int, tuple[int, int]]=None):\n",
    "        if merges is None:\n",
    "            self.merges = dict()\n",
    "        else:\n",
    "            self.merges = merges\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        vocab_size = self.vocab_size\n",
    "        n_merges = len(self.merges)\n",
    "        return f'BytePairEncoder({vocab_size=} {n_merges=})'\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.get_vocab())\n",
    "\n",
    "    def get_vocab(self) -> dict[int, str]:\n",
    "        # Base vocabulary of first 256 utf-8 characters\n",
    "        base_vocab = {chr(token): token for token in range(self.base_vocab_size)}\n",
    "        # Additional vocabulary is determined by trained merges\n",
    "        merge_vocab = {self.decode([token]): token for token in self.merges}\n",
    "        # Total vocabulary is the union of base and merge vocabs\n",
    "        vocab = base_vocab | merge_vocab\n",
    "        return vocab\n",
    "\n",
    "    def _get_parent_tokens(self, token: int) -> Generator[int, None, None]:\n",
    "        \"\"\"Recursively identify whether a token is made up of parent tokens\"\"\"\n",
    "        if token not in self.merges:\n",
    "            yield token\n",
    "            return\n",
    "        for pair_token in self.merges[token]:\n",
    "            yield from self._get_parent_tokens(pair_token)\n",
    "\n",
    "    def train(self, input: str, vocab_size: int = 512) -> None:\n",
    "        \"\"\"Training proceeds by iteratively merging the most frequent token pair until the desired number of tokens is reached\"\"\"\n",
    "        assert vocab_size > self.base_vocab_size, f'Invalid vocab_size: {vocab_size}, must be larger than 256'\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        num_merges = vocab_size - self.base_vocab_size\n",
    "        for i in trange(num_merges):\n",
    "            pair_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "            merge_pair = pair_counts.most_common(1)[0][0]\n",
    "            new_token = self.base_vocab_size + i\n",
    "            self.merges[new_token] = merge_pair\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        for new_token, merge_pair in self.merges.items():\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        decoded_tokens = chain.from_iterable(map(self._get_parent_tokens, tokens))\n",
    "        return bytes(decoded_tokens).decode('utf-8', errors='replace')\n",
    "\n",
    "    def save(self, prefix: str) -> None:\n",
    "        \"\"\"Save a trained model\"\"\"\n",
    "        with open(f'{prefix}.vocab', 'w') as fh:\n",
    "            json.dump(self.get_vocab(), fh)\n",
    "        with open(f'{prefix}.model', 'w') as fh:\n",
    "            json.dump(self.merges, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_filename: str) -> 'BytePairEncoder':\n",
    "        \"\"\"Load a pretrained model from a .model file\"\"\"\n",
    "        assert model_filename.endswith('.model'), f'{model_filename} is not a valid model file, must end with .model'\n",
    "        with open(model_filename, 'r') as fh:\n",
    "            merges = json.load(fh)\n",
    "        # The json fileformat does not accept integers as dict keys, and does not have tuples\n",
    "        sanitized_merges = {int(k):tuple(v) for k,v in merges.items()}\n",
    "        return cls(sanitized_merges)\n",
    "\n",
    "# Show base vocabulary without training\n",
    "BytePairEncoder().get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da9972-aac0-4d61-9368-c7a920efa1e6",
   "metadata": {
    "id": "9263114c-4829-4725-ad0e-d7d5fe6529a6"
   },
   "source": [
    "### Exercise 2\n",
    "Train a byte pair encoder on the tiny shakespeare dataset with a vocab_size of 512 and inspect the vocabulary. Can you identify tokens that encode some semantically meaningful identity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ccb87-882e-4d66-a871-fff8582ceab4",
   "metadata": {
    "id": "6e6ccb87-882e-4d66-a871-fff8582ceab4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b7ede7a51f4b1382896bd9004d924f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train and save a BPE tokenizer\n",
    "bpe = BytePairEncoder()\n",
    "bpe.train(data)\n",
    "bpe.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16decaf-7d53-4c7e-ad23-bb8438bcc217",
   "metadata": {
    "id": "9263114c-4829-4725-ad0e-d7d5fe6529a6"
   },
   "source": [
    "## Optimized BPE using the Huggingface transformers library\n",
    "As you can imagine, our python implementation is not optimized to be fast. Several optimized tokenizers are commonly used, most of which have python bindings for ease of use. Below we will reproduce the configuration of our python BPE tokenizer using the [tokenizers section of the Huggingface transformers library](https://huggingface.co/docs/transformers/en/fast_tokenizers). This allows us to train larger vocabularies in a shorter amount of time.\n",
    "\n",
    "> _Note:_ [Huggingface](https://huggingface.co/) is a large online community for sharing machine learning datasets, models, and applications. Here we use the BPE tokenizer, later we will also use pretrained models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e4b620-79aa-43fb-9821-8abed25a1ee6",
   "metadata": {
    "id": "d1e4b620-79aa-43fb-9821-8abed25a1ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify a BPE tokenizer using Huggingface's tokenizers\n",
    "tokenizer = Tokenizer(models.BPE(byte_fallback=True))\n",
    "trainer = trainers.BpeTrainer(\n",
    "    initial_alphabet=[chr(i) for i in range(256)], # same base vocabulary as our python implementation\n",
    "    vocab_size=512\n",
    ")\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Train on shakespeare\n",
    "tokenizer.train([\"input.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142d6be6-150b-4398-8285-67cd04940fd3",
   "metadata": {
    "id": "142d6be6-150b-4398-8285-67cd04940fd3"
   },
   "source": [
    "Below we do some quick validation that the Huggingface tokenizer and our python tokenizer do the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e271670-16e0-4e88-b5bd-733ce7cbb1be",
   "metadata": {
    "id": "1e271670-16e0-4e88-b5bd-733ce7cbb1be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_tokens = ['H', 'i', ' h', 'ow', ' ', 'are ', 'you', ' ', '1', '2', '3', '4']\n",
      "our_tokens         = ['H', 'i', ' h', 'ow', ' ', 'are ', 'you ', '1', '2', '3', '4']\n"
     ]
    }
   ],
   "source": [
    "# Check whether our test string is turned into the same tokens\n",
    "test_string = 'Hi how are you 1234'\n",
    "\n",
    "huggingface_tokens = tokenizer.encode(test_string).tokens\n",
    "print(f'{huggingface_tokens = }')\n",
    "\n",
    "our_tokens = [bpe.decode([i]) for i in bpe.encode(test_string)]\n",
    "print(f'{our_tokens         = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199343f-8a2f-45b0-b48d-67d0faafff81",
   "metadata": {
    "id": "b199343f-8a2f-45b0-b48d-67d0faafff81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_token_ids = [72, 105, 289, 284, 32, 420, 280, 32, 49, 50, 51, 52]\n",
      "our_token_ids         = [72, 105, 162, 157, 32, 292, 406, 49, 50, 51, 52]\n"
     ]
    }
   ],
   "source": [
    "# Check whether the same tokens use the same ids\n",
    "huggingface_token_ids = tokenizer.encode(test_string).ids\n",
    "print(f'{huggingface_token_ids = }')\n",
    "\n",
    "our_token_ids = bpe.encode(test_string)\n",
    "print(f'{our_token_ids         = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800d3c8-ba7e-4785-a3d4-82de267d90a6",
   "metadata": {
    "id": "a800d3c8-ba7e-4785-a3d4-82de267d90a6",
    "scrolled": true
   },
   "source": [
    "Note that whereas the Huggingface tokenizer identifies almost the same character combinations, it assigns different identifiers for several tokens! After some debugging, it seems that the Huggingface tokenizer is inconsistent in how it handles double newline characters (i.e. '\\n\\n'), and there has been some discussion on this feature for [similar tokenizers](https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289). For all our practical purposes this currently does not matter too much.\n",
    "\n",
    "### Exercise 3\n",
    "Train a BPE encoder using the Huggingface tokenizer with a vocab_size of 1024 and inspect the vocabulary. Similar as in exercise 2: do you see tokens that encode meaningful semantics (just like in our python implementation, you can use `tokenizer.get_vocab()` to inspect the learned vocabulary)? What happens if you increase the vocab_size even further? Is there a limit to the vocab_size?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9155fe05-e4d6-4387-8e44-c44c38582c6f",
   "metadata": {
    "id": "a2bba09f-05b9-4c33-bba6-0ec7517ce047"
   },
   "source": [
    "## Answers\n",
    "\n",
    "### Exercise 1\n",
    "The only number in the vocabulary is '3', so any other number would work, as would signs like for example a #. These characters get a token -1, '< unk >' (for \"unknown\"). A solution is to use a base vocabulary of all known or relevant single characters.\n",
    "\n",
    "### Exercise 2\n",
    "Code below trains, saves, and loads a BPE tokenizer and prints the learned vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30da4a-2ae6-4e4e-8f31-d0df4be3b083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save a BPE tokenizer\n",
    "bpe = BytePairEncoder()\n",
    "bpe.train(data, vocab_size=512)\n",
    "bpe.save(prefix='shakespeare_512')\n",
    "bpe = BytePairEncoder.load(model_filename='./shakespeare_512.model')\n",
    "bpe.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f218f0c-d2c9-4630-ae34-fac44b500c00",
   "metadata": {
    "id": "a2bba09f-05b9-4c33-bba6-0ec7517ce047"
   },
   "source": [
    "There are several words and word fragments, such as 'the', 'and', 'or', 'with', 'have', and 'your'. Interestingly, 'thou' is also in the vocab, indicating the typical language of Shakespeare's texts.\n",
    "\n",
    "### Exercise 3\n",
    "The larger the vocabulary, the more word-like tokens become, up to a point where multiple words get merged into one token (e.g. a vocab_size of 20000 gives 'You shall not ' as one of the tokens). The upper limit is data dependent and occurs when no further merges can happen; for a large dataset this limit will be high."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
