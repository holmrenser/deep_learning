{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e0459c-212d-4ad2-8ba3-81238bb53a2c",
   "metadata": {},
   "source": [
    "# Naive transformer\n",
    "In this notebook we start implementing a 'naive' version of a GPT-style transformer architecture for educational purposes. The main idea is to focus on the most simple form of the attention mechanism (i.e. dot product softmax attention, without batching), and plug that into a transformer setup. We don't expect good performance on text generation yet, and training will be relatively slow since we don't implement a batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cca113c-86f8-464e-905c-051ecff81327",
   "metadata": {
    "id": "9cca113c-86f8-464e-905c-051ecff81327"
   },
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "import math\n",
    "from tqdm.auto import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b20e6-f32e-48b3-bca8-bcd012724f44",
   "metadata": {
    "id": "a51b20e6-f32e-48b3-bca8-bcd012724f44"
   },
   "source": [
    "## Data\n",
    "We once again turn to the tiny shakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d06545-378a-4e58-abaa-0026d32ef96c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06d06545-378a-4e58-abaa-0026d32ef96c",
    "outputId": "58b70696-b534-4233-a765-d44307768c0b"
   },
   "outputs": [],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e950e9-a15e-4360-9699-603db6592e1f",
   "metadata": {
    "id": "f5e950e9-a15e-4360-9699-603db6592e1f"
   },
   "source": [
    "We use a character level tokenizer and a dataset class to select a context of a given size and the next tokens to predict. Very similar to what we used for n-gram markov prediction and 1D CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8bf499-15f4-4904-bac5-78cedc6cf278",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2d8bf499-15f4-4904-bac5-78cedc6cf278",
    "outputId": "9ebb153d-3328-4f58-9094-c619ab34fc19"
   },
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Character level tokenizer that enumerates unique characters in a training text\"\"\"\n",
    "    def __init__(self, encoding_dict: dict[str, int]=None):\n",
    "        if encoding_dict is None:\n",
    "            self.encoding_dict = dict()\n",
    "        else:\n",
    "            self.encoding_dict = encoding_dict\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'CharacterTokenizer(vocab_size={self.vocab_size})'\n",
    "\n",
    "    @property\n",
    "    def decoding_dict(self) -> dict[int, str]:\n",
    "        \"\"\"Decoding dict is implemented as property to automatically sync with changed encoding dict\"\"\"\n",
    "        return {token:char for char,token in self.encoding_dict.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.encoding_dict)\n",
    "\n",
    "    def get_vocab(self) -> dict[str, int]:\n",
    "        return self.encoding_dict\n",
    "\n",
    "    def train(self, data: str) -> None:\n",
    "        \"\"\"Train on a piece of text by enumerating unique characters\"\"\"\n",
    "        chars = sorted(set(data))\n",
    "        self.encoding_dict = {char:token for token,char in enumerate(chars)}\n",
    "\n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        return [self.encoding_dict.get(char, -1) for char in data]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        return ''.join(self.decoding_dict.get(token, '<unk>') for token in tokens)\n",
    "\n",
    "class CharacterDataset:\n",
    "    def __init__(self, data: str, tokenizer: CharacterTokenizer, context_size: int=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_chars = len(self.data)\n",
    "        vocab_size = self.vocab_size\n",
    "        context_size = self.context_size\n",
    "        return f'CharacterDataset({n_chars=}, {vocab_size=}, {block_size=})'\n",
    "\n",
    "    @classmethod\n",
    "    def from_textfile(cls, filename: str, context_size: int=256) -> 'CharacterDataset':\n",
    "        \"\"\"Load a textfile and automatically 'train' a character level tokenizer\"\"\"\n",
    "        tokenizer = CharacterTokenizer()\n",
    "        with open(filename, 'r') as fh:\n",
    "            data = fh.read()\n",
    "            tokenizer.train(data)\n",
    "            return cls(data, tokenizer, context_size=context_size)\n",
    "\n",
    "    def train_test_split(self, train_percentage: float=0.8) -> Tuple['CharacterDataset','CharacterDataset']:\n",
    "        n_train_chars = int(train_percentage * len(self.data))\n",
    "\n",
    "        train_data = self.data[:n_train_chars]\n",
    "        train_dataset = CharacterDataset(train_data, self.tokenizer, self.context_size)\n",
    "\n",
    "        test_data = self.data[n_train_chars:]\n",
    "        test_dataset = CharacterDataset(test_data, self.tokenizer, self.context_size)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.context_size\n",
    "\n",
    "    def __getitem__(self, pos: int) -> torch.tensor:\n",
    "        \"\"\"Return tokens starting at pos up to pos + context_size, targets are shifted by one position\"\"\"\n",
    "        # grab a chunk of block_size characters from the data\n",
    "        chunk = self.data[pos:pos + self.context_size + 1]\n",
    "        # encode every character to an integer\n",
    "        tokens = self.tokenizer.encode(chunk)\n",
    "        # convert to tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        # targets are shifted one position from input\n",
    "        return tokens[:-1], tokens[1:]\n",
    "\n",
    "dataset = CharacterDataset.from_textfile('./input.txt')\n",
    "train_dataset,test_dataset = dataset.train_test_split()\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0922a49-4c0b-4d15-a9aa-bf1c2b50f752",
   "metadata": {
    "id": "e0922a49-4c0b-4d15-a9aa-bf1c2b50f752"
   },
   "source": [
    "## Model\n",
    "Below is the entire implementation of a transformer architecture (minus the actual attention mechanism, see exercise 1) in one codeblock. We keep everything in one block so our model is always in sync with any code changes we implement. \n",
    "\n",
    "The main components are implemented in seperate classes:\n",
    "\n",
    "- `DotProductSoftmaxAttention`: Our non-optimized simplest form of the attention mechanism.\n",
    "- `TransformerBlock`: Executes the `DotProductSoftmaxAttention`, performs layer normalization to keep our gradients well-behaved, and adds a residual connection to let the original signal flow through (which includes for example positional information).\n",
    "- `AdditivePositionalEmbedding`: A wrapper class to add positional embeddings to previously determined token embeddings.\n",
    "- `NaiveTransformer`: Combines all of the above functionality and adds dropout and layer normalization in a few places to help with generalization and prevent overfitting. In addition this implements the `generate` method so we can easily sample novel token sequences.\n",
    "\n",
    "### Exercise 1:\n",
    "Implement dot product softmax attention in the currently empty `DotProductSoftmaxAttention` class. Remember to define three linear input transformations for queries, keys, and values, and to scale the attention matrix by the embedding dimensionality. If you have properly implemented the attention mechanism, the codeblock below will generate a token sample from an untrained model. Once your model works, continue training it on tinyshakespeare with the codeblock under the 'Training' section, and generate a few samples with the code from the 'Evaluation' section. How does your naive transformer model perform (i.e. compared to yesterdays 1D convolutional neural network)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b36e3e-5dcf-446c-b3ee-05743d8c082a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "d7d69e643eb542388f01d9d234fc5985",
      "6ba3e071464743d89d078732dd846e21",
      "41a267f862df482fba7d9766115e2399",
      "653b7bfe250c4a49b183db6ec813bc2f",
      "22edeaf304b3413e8894bc8fcdb1e446",
      "519e59c3ef3d4700b90bb70759a7314f",
      "48c7b8dead864ef68b05fa23bd49e946",
      "3f98f282030a4108ac84fe22ac0fe14d",
      "ce7a69d263984ce5922f017546534110",
      "4cdd68dac2ee40f29e94ed39f70d49fa",
      "d598e24f7a4b44c2b44485d8aadaaf54"
     ]
    },
    "id": "86b36e3e-5dcf-446c-b3ee-05743d8c082a",
    "outputId": "008a0d4e-787a-4285-802a-4d8b471bd13c"
   },
   "outputs": [],
   "source": [
    "class DotProductSoftmaxAttention(nn.Module):\n",
    "    \"\"\"Simplest dot product attention block, no batches\"\"\"\n",
    "    def __init__(self, embedding_dim: int, causal: bool=True):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        # IMPLEMENT ME\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate dot product attention, input x is not batched!\"\"\"\n",
    "        # IMPLEMENT ME\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block that combines attention with layer normalization and residual connections\"\"\"\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            DotProductSoftmaxAttention(embedding_dim=embedding_dim),\n",
    "            nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate attention with residual connections\"\"\"\n",
    "        attn = self.attention(x) + x\n",
    "        return attn \n",
    "\n",
    "class AdditivePositionalEmbedding(nn.Module):\n",
    "    \"\"\"Wrapper class to add positional encoding to already embedded tokens\"\"\"\n",
    "    def __init__(self, context_size: int, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=context_size, embedding_dim=embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Add positional embeddings based on input dimensions, use residual connection\"\"\"\n",
    "        pos = torch.arange(0, x.size(0), dtype=torch.long, device=x.device)\n",
    "        return self.embedding(pos) + x\n",
    "\n",
    "class NaiveTransformer(nn.Module):\n",
    "    \"\"\"Full transformer architecture, but without optimizations such as batching or dropout\"\"\"\n",
    "    def __init__(self, context_size: int, vocab_size: int, n_layers: int=4, embedding_dim: int=256, dropout: float=0.2,):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.transformer = nn.Sequential(\n",
    "            nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim),                     # token embedding\n",
    "            AdditivePositionalEmbedding(context_size, embedding_dim),                                 # add position embedding\n",
    "            nn.Dropout(dropout),                                                                      # random dropout to decrease overfitting\n",
    "            nn.Sequential(*[TransformerBlock(embedding_dim=embedding_dim) for _ in range(n_layers)]), # attention layers\n",
    "            nn.LayerNorm(embedding_dim),                                                              # layer normalization to keep our gradients nice\n",
    "            nn.Linear(in_features=embedding_dim, out_features=vocab_size),                            # position-wise output projection\n",
    "            nn.Dropout(dropout)                                                                       # random dropout to decrease overfitting\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor, targets: torch.tensor=None) -> tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        \"\"\"Predict class logits and calculate loss if targets are specified\"\"\"\n",
    "        logits = self.transformer(x)\n",
    "        loss = None if targets is None else F.cross_entropy(logits, targets)\n",
    "        accuracy = None if targets is None else (logits.argmax(dim=1) == targets).sum() / targets.size(0)\n",
    "        return logits, loss, accuracy\n",
    "\n",
    "    def generate(self, sample_length: int=256) -> list[int]:\n",
    "        \"\"\"Generate sample tokens\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        idx = torch.tensor([0], dtype=torch.long, device=device)\n",
    "\n",
    "        for _ in trange(sample_length):\n",
    "            logits,*_ = self(idx[-self.context_size:])\n",
    "            logits = logits[-1,:]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat([idx, idx_next])\n",
    "\n",
    "        return idx[1:].tolist()\n",
    "\n",
    "model = NaiveTransformer(context_size=dataset.context_size, vocab_size=dataset.vocab_size)\n",
    "sample = model.generate()\n",
    "print(dataset.tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786d3928-c78e-4389-8c45-6c9c0e816cf6",
   "metadata": {},
   "source": [
    "## Training \n",
    "The code below configures and trains our 'naive' transformer model on tiny shakespeare. Loss and accuracy for both train and test sets are calculated and saved.\n",
    "\n",
    "> _Note:_ Training of batched data is implemented in a slightly unusual fashion because our simple `DotProductSoftmaxAttention` is not designed to handle batched input. Instead we loop over every sample in a batch and backpropagate our loss, but we only zero the gradients after we have looked at the entire batch. This technique is called 'gradient accumulation' and is also used to train very large models where only one sample can be loaded into memory simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05bdb7-74dd-465b-b29e-7d5a3a527214",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480,
     "referenced_widgets": [
      "5a5c552237be4d7a8eb9bcef44f38e67",
      "9a918b74972a46bebf79765408155e6d",
      "3f9d0c791aac48629f42624530ff80a7",
      "9e0853672fd14c7990d468e2cd53e5e0",
      "4d21ba607a384b049c8c3fd5a5584d56",
      "2f6100c31d3041b79689bcce60e9efc3",
      "908e1dbc6918477f95e3c82106cc73ba",
      "b16835ec39434a70add09f6ac53a8696",
      "24081137ffef4c55a861141761087d61",
      "08d84a6dffd940aea1963c346c019484",
      "0b2dbb0aec2d49bc8f3b3d1786e5011d"
     ]
    },
    "id": "fd05bdb7-74dd-465b-b29e-7d5a3a527214",
    "outputId": "904b613d-9880-46c8-f706-95d25c23cb58"
   },
   "outputs": [],
   "source": [
    "dataset = CharacterDataset.from_textfile('./input.txt', context_size=256)\n",
    "train_dataset, test_dataset = dataset.train_test_split()\n",
    "\n",
    "model = NaiveTransformer(\n",
    "    context_size=dataset.context_size,\n",
    "    vocab_size=dataset.vocab_size,\n",
    "    embedding_dim=256,\n",
    "    n_layers=6,\n",
    "    dropout=0.1,\n",
    ")\n",
    "model = model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "batch_size = 16\n",
    "n_train_steps = 500\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset, num_samples=batch_size * n_train_steps),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=RandomSampler(test_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = iter(test_dataloader)\n",
    "\n",
    "train_loss = []\n",
    "train_accuracy = []\n",
    "test_loss = []\n",
    "test_accuracy = []\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for i, train_batch in enumerate(tqdm(train_dataloader)):\n",
    "    model.zero_grad()\n",
    "    for x,y in zip(*train_batch):\n",
    "        _,loss,accuracy = model(x.to(DEVICE), y.to(DEVICE))\n",
    "        # backprop and update the parameters\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(accuracy.item())\n",
    "        test_x, test_y = map(lambda t: t.to(DEVICE), next(test_dataloader))\n",
    "        test_losses,test_accuracies = list(zip(*[model(x,y)[1:] for x,y in zip(test_x, test_y)]))\n",
    "        test_loss.append(torch.tensor(test_losses).mean().item())\n",
    "        test_accuracy.append(torch.tensor(test_accuracies).mean().item())\n",
    "        \n",
    "fig,[ax1,ax2] = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ax1.plot(train_loss, label='train loss')\n",
    "ax1.plot(test_loss, label='test loss')\n",
    "ax1.legend()\n",
    "ax2.plot(train_accuracy, label='train accuracy')\n",
    "ax2.plot(test_accuracy, label='test accuracy')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e744c6-8485-478c-ad0b-ade3295124ba",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Generate samples from a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7ceae-e14a-4ecd-a9af-78a4be91036d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "ff287936a74f4a98ac596203492f207c",
      "b5b3d8fd65004eefbb12bc600509cbf5",
      "7b6d70f3305a4537ad783e6718671f0e",
      "085bce58071a48c2a0f1e3f44cf06994",
      "cb389d33fb594ebf9ed9e2c80244579a",
      "b3cbda420d3641379cf272a7a7625125",
      "1a3b2d47e4c4474cb2d82ed908e1c7c7",
      "35870b71832a43328c1e164b7b8ef364",
      "e2cc9e3d47aa40c6b24ebceb3b84d36a",
      "d9022957bbad4c8ea64331050888282c",
      "f1cd06e9cbfc44daada8fdca30fda287"
     ]
    },
    "id": "81b7ceae-e14a-4ecd-a9af-78a4be91036d",
    "outputId": "5dba92fa-438f-40fc-f243-e801174e378c"
   },
   "outputs": [],
   "source": [
    "model.eval() # Going in eval mode disables the random dropout\n",
    "sample = model.generate(sample_length=256)\n",
    "print(dataset.tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c82869-8525-4158-b6ea-741a3868f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductSoftmaxAttention(nn.Module):\n",
    "    \"\"\"Simplest dot product attention block, no batches\"\"\"\n",
    "    def __init__(self, embedding_dim: int):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.w_k = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_q = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "        self.w_v = nn.Linear(in_features=embedding_dim, out_features=embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        \"\"\"Calculate dot product attention, input x is not batched!\"\"\"\n",
    "        k = self.w_k(x)\n",
    "        q = self.w_q(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        attention = (k @ q.T) / math.sqrt(self.embedding_dim)\n",
    "\n",
    "        # uncomment the lines below for causal/masked self-attention\n",
    "        # mask = torch.tril(attention) == 0\n",
    "        # attention[mask] = -torch.inf\n",
    "\n",
    "        return F.softmax(attention, dim=-1) @ v"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "085bce58071a48c2a0f1e3f44cf06994": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9022957bbad4c8ea64331050888282c",
      "placeholder": "​",
      "style": "IPY_MODEL_f1cd06e9cbfc44daada8fdca30fda287",
      "value": " 256/256 [00:01&lt;00:00, 142.38it/s]"
     }
    },
    "08d84a6dffd940aea1963c346c019484": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b2dbb0aec2d49bc8f3b3d1786e5011d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a3b2d47e4c4474cb2d82ed908e1c7c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "22edeaf304b3413e8894bc8fcdb1e446": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24081137ffef4c55a861141761087d61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f6100c31d3041b79689bcce60e9efc3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35870b71832a43328c1e164b7b8ef364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f98f282030a4108ac84fe22ac0fe14d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f9d0c791aac48629f42624530ff80a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b16835ec39434a70add09f6ac53a8696",
      "max": 2500,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24081137ffef4c55a861141761087d61",
      "value": 2500
     }
    },
    "41a267f862df482fba7d9766115e2399": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f98f282030a4108ac84fe22ac0fe14d",
      "max": 256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ce7a69d263984ce5922f017546534110",
      "value": 256
     }
    },
    "48c7b8dead864ef68b05fa23bd49e946": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4cdd68dac2ee40f29e94ed39f70d49fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d21ba607a384b049c8c3fd5a5584d56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "519e59c3ef3d4700b90bb70759a7314f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5c552237be4d7a8eb9bcef44f38e67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a918b74972a46bebf79765408155e6d",
       "IPY_MODEL_3f9d0c791aac48629f42624530ff80a7",
       "IPY_MODEL_9e0853672fd14c7990d468e2cd53e5e0"
      ],
      "layout": "IPY_MODEL_4d21ba607a384b049c8c3fd5a5584d56"
     }
    },
    "653b7bfe250c4a49b183db6ec813bc2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cdd68dac2ee40f29e94ed39f70d49fa",
      "placeholder": "​",
      "style": "IPY_MODEL_d598e24f7a4b44c2b44485d8aadaaf54",
      "value": " 256/256 [00:03&lt;00:00, 70.51it/s]"
     }
    },
    "6ba3e071464743d89d078732dd846e21": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_519e59c3ef3d4700b90bb70759a7314f",
      "placeholder": "​",
      "style": "IPY_MODEL_48c7b8dead864ef68b05fa23bd49e946",
      "value": "100%"
     }
    },
    "7b6d70f3305a4537ad783e6718671f0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35870b71832a43328c1e164b7b8ef364",
      "max": 256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e2cc9e3d47aa40c6b24ebceb3b84d36a",
      "value": 256
     }
    },
    "908e1dbc6918477f95e3c82106cc73ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9a918b74972a46bebf79765408155e6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f6100c31d3041b79689bcce60e9efc3",
      "placeholder": "​",
      "style": "IPY_MODEL_908e1dbc6918477f95e3c82106cc73ba",
      "value": "100%"
     }
    },
    "9e0853672fd14c7990d468e2cd53e5e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08d84a6dffd940aea1963c346c019484",
      "placeholder": "​",
      "style": "IPY_MODEL_0b2dbb0aec2d49bc8f3b3d1786e5011d",
      "value": " 2500/2500 [02:35&lt;00:00, 17.04it/s]"
     }
    },
    "b16835ec39434a70add09f6ac53a8696": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3cbda420d3641379cf272a7a7625125": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5b3d8fd65004eefbb12bc600509cbf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3cbda420d3641379cf272a7a7625125",
      "placeholder": "​",
      "style": "IPY_MODEL_1a3b2d47e4c4474cb2d82ed908e1c7c7",
      "value": "100%"
     }
    },
    "cb389d33fb594ebf9ed9e2c80244579a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce7a69d263984ce5922f017546534110": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d598e24f7a4b44c2b44485d8aadaaf54": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7d69e643eb542388f01d9d234fc5985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ba3e071464743d89d078732dd846e21",
       "IPY_MODEL_41a267f862df482fba7d9766115e2399",
       "IPY_MODEL_653b7bfe250c4a49b183db6ec813bc2f"
      ],
      "layout": "IPY_MODEL_22edeaf304b3413e8894bc8fcdb1e446"
     }
    },
    "d9022957bbad4c8ea64331050888282c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2cc9e3d47aa40c6b24ebceb3b84d36a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f1cd06e9cbfc44daada8fdca30fda287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff287936a74f4a98ac596203492f207c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5b3d8fd65004eefbb12bc600509cbf5",
       "IPY_MODEL_7b6d70f3305a4537ad783e6718671f0e",
       "IPY_MODEL_085bce58071a48c2a0f1e3f44cf06994"
      ],
      "layout": "IPY_MODEL_cb389d33fb594ebf9ed9e2c80244579a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
