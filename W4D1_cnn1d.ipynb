{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e85360-2cdd-4a4b-88dd-99cc7c1c2be4",
   "metadata": {},
   "source": [
    "# 1D Convolutional Neural Network \n",
    "In this notebook we implement a first attempt at next token prediction using a neural network. To align with previous weeks in this course we choose a 'simple' 1D convolutional neural network. The motivation behind this is that by composing convolutions we can create a receptive field on our sequence context that is more efficient in its parameter use than our previous n-gram markov model, thereby allowing longer sequence context to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e85bdc-3ea1-439b-a5c1-83152660c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm, trange\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "DEVICE = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec8d97a-7e7a-473c-bb06-4fdff75c6ec7",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e824fd16-4608-4c9f-a2ac-713150076ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tinyshakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1f2bf-569b-4a29-8fc1-3ccbfe0e1044",
   "metadata": {},
   "source": [
    "We use a character level tokenizer and a dataset class to select (and one-hot encode) a context of a given size and the next token to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c338b-2f68-4454-9436-215b214fc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CharacterTokenizer:\n",
    "    decode_dict: dict[int, str] = field(default_factory=dict)\n",
    "    encode_dict: dict[str, int] = field(default_factory=dict)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Character to int mapping\"\"\"\n",
    "        return self.encode_dict\n",
    "\n",
    "    def train(self, input_str: str) -> None:\n",
    "        \"\"\"Determine what character will be mapped to which int using lexicograpical order\"\"\"\n",
    "        chars = sorted(set(input_str))\n",
    "        self.decode_dict: dict[int, str] = dict(enumerate(chars))\n",
    "        self.encode_dict: dict[str, int] = {v:k for k,v in self.decode_dict.items()}\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        \"\"\"Turn a string into a list of ints using a pretrained lookup table\"\"\"\n",
    "        return [self.encode_dict[char] for char in input]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Turn a list of ints into a string using a reverse lookup table\"\"\"\n",
    "        return ''.join(self.decode_dict[token] for token in tokens)\n",
    "\n",
    "class CharacterDataset:\n",
    "    def __init__(self, data: str, tokenizer: CharacterTokenizer, context_size: int=256):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_chars = len(self.data)\n",
    "        vocab_size = self.vocab_size\n",
    "        block_size = self.context_size\n",
    "        return f'CharacterDataset({n_chars=}, {context_size=}, {block_size=})'\n",
    "\n",
    "    @classmethod\n",
    "    def from_textfile(cls, filename: str, context_size: int=256) -> 'CharacterDataset':\n",
    "        tokenizer = CharacterTokenizer()\n",
    "        with open(filename, 'r') as fh:\n",
    "            data = fh.read()\n",
    "            tokenizer.train(data)\n",
    "            return cls(data, tokenizer, context_size=context_size)\n",
    "\n",
    "    def train_test_split(self, train_percentage: float=0.8) -> tuple['CharacterDataset','CharacterDataset']:\n",
    "        n_train_chars = int(train_percentage * len(self.data))\n",
    "\n",
    "        train_data = self.data[:n_train_chars]\n",
    "        train_dataset = CharacterDataset(train_data, self.tokenizer, self.context_size)\n",
    "\n",
    "        test_data = self.data[n_train_chars:]\n",
    "        test_dataset = CharacterDataset(test_data, self.tokenizer, self.context_size)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.context_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.tensor:\n",
    "        # grab a chunk of context_size + 1 characters from the data\n",
    "        chunk = self.data[idx:idx + self.context_size + 1]\n",
    "        # encode every character to an integer\n",
    "        tokens = self.tokenizer.encode(chunk)\n",
    "        # convert to tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        # Onehot encode, transpose because Conv1D takes (batch, channels, length) as input dims: https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "        x = F.one_hot(tokens[:-1], num_classes=self.vocab_size).type(torch.float32).T\n",
    "        # Use last character as target\n",
    "        y = tokens[-1:]\n",
    "        return x,y\n",
    "\n",
    "dataset = CharacterDataset.from_textfile('./input.txt', context_size=32)\n",
    "x,y = dataset[0]\n",
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a22527-6f67-4f94-add4-08488de30196",
   "metadata": {},
   "source": [
    "## Model\n",
    "Below we implement our 1D convolutional neural network for next character prediction. \n",
    "\n",
    "## Exercise 1\n",
    "Add the 1D CNN to the provided implementation below. Create two convolution blocks of the specified number of channels that use [Conv1d](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html), [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html), and [MaxPool1d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html), followed by a [linear projection](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) to the output dimensionality (vocab_size). Use a convolution kernel size of 3, with padding to keep the output the same size. Use max-pooling with a kernel size of 2 and a stride to keep the same output size. Train your model using the training codeblock. What training and test loss does your 'simple' 1D convolution net achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7122e-490a-492e-b479-b90f4c36b0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    \"\"\"1D Convolutional Neural Network for next token prediction\"\"\"\n",
    "    def __init__(self, vocab_size: int, context_size: int, conv_channels: int=128, use_bias: bool=False):\n",
    "        super().__init__()\n",
    "        assert context_size % 2 == 0, f'Invalid block_size, {block_size} is not an even number'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.context_size = context_size\n",
    "        self.cnn = # IMPLEMENT ME!\n",
    "    \n",
    "    def forward(self, X: torch.tensor, targets: torch.tensor=None) -> tuple[torch.tensor, torch.tensor]:\n",
    "        \"\"\"Predict logits of next character conditioned on context_size previous characters\"\"\"\n",
    "        logits = self.cnn(X)\n",
    "        loss = None if targets is None else F.cross_entropy(logits, targets.view(-1), ignore_index=-1)\n",
    "        return logits,loss\n",
    "\n",
    "    def generate(self, sample_length: int=256) -> list[int]:\n",
    "        \"\"\"Generate sample of tokens\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        # Start generating with n=context_size newline tokens, these will later be omitted\n",
    "        idx = torch.zeros((1, self.context_size), dtype=torch.long, device=device)\n",
    "        \n",
    "        for _ in trange(sample_length, desc='Sampling'):\n",
    "            # onehot encode the last context_size tokens\n",
    "            context_tokens = idx[:, -self.context_size:]\n",
    "            input = F.one_hot(context_tokens, self.vocab_size).to(torch.float).transpose(1,2) # transpose because of Conv1D shape requirements\n",
    "            # forward model and calculate token probabilities\n",
    "            logits, _ = model(input)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample next token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append generated token to current sample\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        # Omit first context_size tokens (these were all newlines to get the sampling started)\n",
    "        return idx[0, self.context_size:].tolist()\n",
    "\n",
    "# Create a sample with an untrained model for comparison/testing\n",
    "model = CNN1D(dataset.vocab_size, context_size=dataset.context_size)\n",
    "sample = model.generate()\n",
    "print(dataset.tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97252b41-ee1a-4ca2-bbbd-efc177dcdd36",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe40c8-92d9-4ac5-9600-1a09ccd5c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharacterDataset.from_textfile('./input.txt', context_size=64)\n",
    "train_dataset,test_dataset = dataset.train_test_split()\n",
    "\n",
    "model = CNN1D(vocab_size=dataset.vocab_size, context_size=dataset.context_size)\n",
    "model.to(DEVICE)\n",
    "\n",
    "train_steps = 2000\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset, num_samples=train_steps * batch_size),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=RandomSampler(test_dataset),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dataloader = iter(test_dataloader)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for i,(x,y) in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "    x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "    # forward model and calculate loss\n",
    "    _,loss = model(x,y)\n",
    "    # save train and test loss every 20 steps\n",
    "    if i % 20 == 0:\n",
    "        train_losses.append(loss.item())\n",
    "        test_x, test_y = next(test_dataloader)\n",
    "        _,test_loss = model(test_x.to(DEVICE), test_y.to(DEVICE))\n",
    "        test_losses.append(test_loss.item())\n",
    "    # backprop and update the parameters\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3725da-dbb0-41bb-8b79-7c77009120f8",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172ca20-e529-4146-9037-748e8b0b0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = model.generate(sample_length=256)\n",
    "print(dataset.tokenizer.decode(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787fd10-1379-44d6-b61c-60a2d5a3e139",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "### Exercise 1\n",
    "Example 1D CNN implementation below, after training for 2000 iterations using batch_size=64 and context_size=64 this achieves a training and test loss of 2.0 - 2.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2250bae8-dfc5-402e-a11a-41c07243f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.cnn = nn.Sequential(\n",
    "    # conv block 1\n",
    "    nn.LazyConv1d(out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "    # conv block 2\n",
    "    nn.LazyConv1d(out_channels=conv_channels, kernel_size=3, padding='same', bias=use_bias),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "    # output projection\n",
    "    nn.Flatten(1, -1),\n",
    "    nn.LazyLinear(self.vocab_size, bias=use_bias)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
