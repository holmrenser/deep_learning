{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a67439a-7953-41aa-b8ac-d010d1c4867a",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holmrenser/deep_learning/blob/main/tokenization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Tokenization\n",
    "Machine learning approaches for natural language processing face the problem of representing long stretches of natural language (i.e. words, sentences, paragraphs, chapters, etc.) in a meaningful and computationally efficient way. A trivial approach is to split text on interpunction and whitespace, effectively selecting individual words. The downside of this approach is that semantically similar words are encoded differently. For example, 'small', 'smaller', and 'smallest', would all be encoded as different entities, forcing a model to learn any semantic similarity from data alone. An alternative approach would encode 'small' as one entity, and 'er', and 'est' as separate entities. The benefit of this 'subword' approach is that it is more straightforward to model semantic similarity, the downside is that it is not straightforward to identify an optimal subword selection scheme. \n",
    "\n",
    "In this notebook we will explore the [byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) algorithm for creating subword representations, a.k.a. tokens. Put simply, byte pair encoding iteratively merges the most frequent token pair into a new token, starting from the most simple tokens (e.g. letters), and continuing until the desired number of tokens is reached.\n",
    "\n",
    "For computational reasons, letters are often represented as integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2713507d-28cf-49a9-9847-7cd1f5407a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the whole notebook\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm.auto import trange\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Generator\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "079b9986-aa40-4a63-a6f3-e9c5efea262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘input.txt’ already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a8796b7-393e-4c41-9af8-a9468a689f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proce'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tiny shakespeare data and show the first 100 characters\n",
    "with open('input.txt', 'r') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "data[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74ffeb-4785-44e8-99d2-62eb7f2eb5cf",
   "metadata": {},
   "source": [
    "## Naive 'tokens'\n",
    "Probably the simplest tokenization strategy is to assign integers to individual characters in a given dataset. We'll implement this strategy below with a few lines of code. We create two dictionaries that function as lookup table: one encoding characters to integers, and one decoding integers back to characters. To apply our lookup tables we use a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9439091-ae2b-467b-b807-5c64cd0ade56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43]\n",
      "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "# Calling 'set' on our data returns all individual characters, which are then lexicographically sorted\n",
    "chars = sorted(set(data))\n",
    "# Calling 'enumerate' returns the original iterator and increasing integers, which we'll use as tokens\n",
    "char_to_token = {char:token for token,char in enumerate(chars)}\n",
    "# Reverse the mapping to be able to decode\n",
    "token_to_char = {token:char for char,token in char_to_token.items()}\n",
    "\n",
    "# Encode the first 30 characters of the tiny shakespeare dataset\n",
    "tokens = [char_to_token[c] for c in data[:30]]\n",
    "print(tokens)\n",
    "\n",
    "# Check that we retrieve our original text when decoding\n",
    "chars = [token_to_char[t] for t in tokens]\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0830d5d-9c19-452a-81a6-4f98568bbd29",
   "metadata": {},
   "source": [
    "Below we add a bit more functionality and structure to the idea presented above. This allows us to more efficiently use our mappings as tokenizer, pass the tokenizer around more easily, and aligns with code conventions used in many of-the-shelf tokenizer libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73292fcd-e9af-47a2-91d8-09b69748f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained tokenizer: NaiveTokenizer(vocab_size=0)\n",
      "Trained tokenizer: NaiveTokenizer(vocab_size=65)\n",
      "[20, 47, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59]\n",
      "Hi how are you\n"
     ]
    }
   ],
   "source": [
    "class NaiveTokenizer:\n",
    "    def __init__(self, encoding_dict: dict[str, int]=None):\n",
    "        if encoding_dict is None:\n",
    "            self.encoding_dict = dict()\n",
    "        else:\n",
    "            self.encoding_dict = encoding_dict\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NaiveTokenizer(vocab_size={self.vocab_size})'\n",
    "\n",
    "    @property\n",
    "    def decoding_dict(self) -> dict[int, str]:\n",
    "        return {token:char for char,token in self.encoding_dict.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.encoding_dict)\n",
    "\n",
    "    def train(self, data: str) -> None:\n",
    "        chars = sorted(set(data))\n",
    "        self.encoding_dict = {char:token for token,char in enumerate(chars)}\n",
    "\n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        return [self.encoding_dict.get(char, -1) for char in data]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return ''.join(self.decoding_dict.get(token, '<unk>') for token in tokens)\n",
    "\n",
    "# Initialize an empty NaiveTokenizer\n",
    "tokenizer = NaiveTokenizer()\n",
    "print(f'Untrained tokenizer: {tokenizer}')\n",
    "\n",
    "# 'Train' on tiny shakespeare\n",
    "tokenizer.train(data)\n",
    "print(f'Trained tokenizer: {tokenizer}')\n",
    "\n",
    "# Encode a string (that is not in the training data)\n",
    "tokens = tokenizer.encode('Hi how are you')\n",
    "print(tokens)\n",
    "\n",
    "# Decode the encoding\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172bf54-0ea4-4820-88f1-ca27550abaee",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Can you come up with a string that cannot be effectively encoded by our naive tokenizer? What happens to this string? How would you circumvent this issue?\n",
    "\n",
    "## Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47498365-3d17-4771-abd1-3141bc0b8a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " 105,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 32,\n",
       " 67,\n",
       " 105,\n",
       " 116,\n",
       " 105,\n",
       " 122,\n",
       " 101,\n",
       " 110,\n",
       " 58,\n",
       " 10,\n",
       " 66,\n",
       " 101,\n",
       " 102,\n",
       " 111,\n",
       " 114]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bytes = data.encode('utf-8')\n",
    "tokens = list(text_bytes)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b010850-85c3-4c05-a376-4c5afacfac1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70,\n",
       " 105,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 32,\n",
       " 67,\n",
       " 105,\n",
       " 116,\n",
       " 105,\n",
       " 122,\n",
       " 101,\n",
       " 110,\n",
       " 58,\n",
       " 10,\n",
       " 66,\n",
       " 101,\n",
       " 102,\n",
       " 111,\n",
       " 114]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(c) for c in data[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63191281-6333-4986-97dc-19ed433da6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BytePairEncoder(vocab_size=256 n_merges=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tokens(tokens: list[int], token_pair: tuple[int,int], new_token: int) -> list[int]:\n",
    "    \"\"\"Takes a list of tokens and replaces every occurence of token_pair with new_token\"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    # Iterate in a while loop because we want to jump ahead two steps sometimes\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        # Edge case: final individual token\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(token)\n",
    "            break\n",
    "        # Look ahead one token to find a token pair\n",
    "        next_token = tokens[i+1]\n",
    "        # On match we should jump ahead two tokens to skip the original pair\n",
    "        if token_pair == (token, next_token):\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            i += 1\n",
    "    return new_tokens    \n",
    "\n",
    "class BytePairEncoder:\n",
    "    def __init__(self, merges: dict[int, tuple[int, int]]=None):\n",
    "        if merges is None:\n",
    "            self.merges = dict()\n",
    "        else:\n",
    "            self.merges = merges\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        vocab_size = self.vocab_size\n",
    "        n_merges = len(self.merges)\n",
    "        return f'BytePairEncoder({vocab_size=} {n_merges=})'\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.get_vocab())\n",
    "\n",
    "    def get_vocab(self) -> dict[int, str]:\n",
    "        base_vocab = {chr(token): token for token in range(256)}        \n",
    "        merge_vocab = {self.decode([token]): token for token in self.merges}\n",
    "        return base_vocab | merge_vocab\n",
    "\n",
    "    def _token_to_bytes(self, token: int) -> Generator[int, None, None]:\n",
    "        if token not in self.merges:\n",
    "            yield token\n",
    "            return\n",
    "        for pair_token in self.merges[token]:\n",
    "            if pair_token >= 256:\n",
    "                yield from self._token_to_bytes(pair_token)\n",
    "            else:\n",
    "                yield pair_token\n",
    "\n",
    "    def train(self, input: str, vocab_size: int = 512) -> None:\n",
    "        assert vocab_size > 256, f'Invalid vocab_size: {vocab_size}, must be larger than 256'\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in trange(num_merges):\n",
    "            pair_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "            merge_pair = pair_counts.most_common(1)[0][0]\n",
    "            new_token = 256 + i\n",
    "            self.merges[new_token] = merge_pair\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        for new_token, merge_pair in self.merges.items():\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        decoded_tokens = chain.from_iterable(map(self._token_to_bytes, tokens))\n",
    "        return bytes(decoded_tokens).decode('utf-8', errors='replace')\n",
    "\n",
    "    def save(self, prefix: str) -> None:\n",
    "        with open(f'{prefix}.vocab', 'w') as fh:\n",
    "            json.dump(self.vocab, fh)\n",
    "        with open(f'{prefix}.model', 'w') as fh:\n",
    "            json.dump(self.merges, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_filename: str) -> 'BytePairEncoder':\n",
    "        with open(model_filename, 'r') as fh:\n",
    "            merges = json.load(fh)\n",
    "        sanitized_merges = {int(k):tuple(v) for k,v in merges.items()}\n",
    "        return cls(sanitized_merges)\n",
    "\n",
    "bpe = BytePairEncoder()\n",
    "bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6e6ccb87-882e-4d66-a871-fff8582ceab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b68184ad3f42e0af7bfe7aeada806e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bpe.train(data, vocab_size=512)\n",
    "bpe.save('shakespeare_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b285943-0404-4c10-91ee-1437d9f485c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BytePairEncoder(vocab_size=260 n_merges=4)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BytePairEncoder.load('./shakespeare_260.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1e4b620-79aa-43fb-9821-8abed25a1ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tokenizers.Tokenizer at 0x105d2e4e0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(models.BPE(byte_fallback=True))\n",
    "trainer = trainers.BpeTrainer(\n",
    "    initial_alphabet=[chr(i) for i in range(256)],\n",
    "    vocab_size=512\n",
    ")\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train([\"input.txt\"], trainer=trainer)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "5f2f5af9-6c02-41d4-ae0b-79eeba89e23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'i', ' h', 'ow', ' ', 'are ', 'you', ' ', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([i]) for i in tokenizer.encode(\"Hi how are you 1234\").ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "45790785-3076-44fd-81f1-e54f78567450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi how are you 1234'"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hi how are you 1234\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "e1b4b16c-c308-4e29-9d1e-4c353572e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'i', ' h', 'ow', ' ', 'are ', 'you', ' ', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bpe.decode([i]) for i in bpe.encode('Hi how are you 1234')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "908fc3c5-c492-4358-a87d-249ed2a149b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi how are you 1234'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(bpe.encode('Hi how are you 1234'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "7437723e-eb94-47b0-9c8f-5a92b272a24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 105, 289, 284, 32, 420, 280, 32, 49, 50, 51, 52]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hi how are you 1234').ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "cf765698-4680-48ec-a4d2-edf71ab31d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 105, 290, 285, 32, 420, 281, 32, 49, 50, 51, 52]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode('Hi how are you 1234')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d65277c9-5b91-48eb-a6cc-bcac1f4aba68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 120,\n",
       " 'ow': 284,\n",
       " 'W': 87,\n",
       " '[': 91,\n",
       " 's\\n': 469,\n",
       " '÷': 247,\n",
       " '¿': 191,\n",
       " 'Â': 194,\n",
       " 'sir': 497,\n",
       " 'å': 229,\n",
       " 'st': 295,\n",
       " '\\x8b': 139,\n",
       " 'lea': 508,\n",
       " 'O:\\n': 337,\n",
       " 'res': 448,\n",
       " '-': 45,\n",
       " 'À': 192,\n",
       " 'ø': 248,\n",
       " '\\x91': 145,\n",
       " 'Ë': 203,\n",
       " 'ck': 380,\n",
       " 's, ': 376,\n",
       " '©': 169,\n",
       " 'lo': 379,\n",
       " 'oun': 442,\n",
       " '·': 183,\n",
       " 'ê': 234,\n",
       " 'om': 291,\n",
       " 'im': 314,\n",
       " 'up': 421,\n",
       " 'us': 440,\n",
       " 'z': 122,\n",
       " 'as ': 352,\n",
       " '3': 51,\n",
       " '¥': 165,\n",
       " 'ee': 326,\n",
       " '4': 52,\n",
       " 'T': 84,\n",
       " ' and ': 434,\n",
       " '\\x88': 136,\n",
       " 'Ã': 195,\n",
       " 't': 116,\n",
       " '#': 35,\n",
       " '\\x18': 24,\n",
       " 're': 308,\n",
       " 'ut ': 335,\n",
       " 'an': 266,\n",
       " 'is ': 278,\n",
       " 'him': 423,\n",
       " '\\x87': 135,\n",
       " 'qu': 458,\n",
       " 'L': 76,\n",
       " '\\x83': 131,\n",
       " 'for': 299,\n",
       " ' s': 281,\n",
       " ':': 58,\n",
       " 'ù': 249,\n",
       " 'Á': 193,\n",
       " '\\n': 10,\n",
       " 'v': 118,\n",
       " 'mor': 460,\n",
       " ' w': 287,\n",
       " 'er': 263,\n",
       " 'ill': 386,\n",
       " '\\x99': 153,\n",
       " '; ': 374,\n",
       " ' the': 465,\n",
       " 'y ': 265,\n",
       " '\\x9a': 154,\n",
       " 'et ': 447,\n",
       " 'ou': 262,\n",
       " 'ING': 433,\n",
       " 'un': 346,\n",
       " '\\x8d': 141,\n",
       " '\\x80': 128,\n",
       " '\\xa0': 160,\n",
       " 'µ': 181,\n",
       " 'Ö': 214,\n",
       " 'Th': 306,\n",
       " 'hat ': 333,\n",
       " '^': 94,\n",
       " 'Ç': 199,\n",
       " 'at ': 310,\n",
       " 'Þ': 222,\n",
       " 'ï': 239,\n",
       " 'is': 319,\n",
       " 'rea': 389,\n",
       " 'Ø': 216,\n",
       " 'ì': 236,\n",
       " 'ing ': 372,\n",
       " 'ake ': 505,\n",
       " 'k': 107,\n",
       " 'man': 443,\n",
       " 'O': 79,\n",
       " '%': 37,\n",
       " '\\x15': 21,\n",
       " 'are ': 420,\n",
       " 'M': 77,\n",
       " ' h': 289,\n",
       " 'OL': 492,\n",
       " 'th': 257,\n",
       " 'her': 473,\n",
       " 'am': 463,\n",
       " '¼': 188,\n",
       " 'H': 72,\n",
       " '\\x85': 133,\n",
       " '8': 56,\n",
       " 'il': 382,\n",
       " 'gh': 317,\n",
       " 'in': 264,\n",
       " ',\\n': 277,\n",
       " 'ro': 342,\n",
       " ' c': 358,\n",
       " 'be': 368,\n",
       " '¡': 161,\n",
       " 'ò': 242,\n",
       " ' b': 296,\n",
       " 'a': 97,\n",
       " 'ü': 252,\n",
       " 'OR': 446,\n",
       " '\\x89': 137,\n",
       " 'n': 110,\n",
       " 'IN': 350,\n",
       " 'ul': 461,\n",
       " ' with': 487,\n",
       " 'own': 431,\n",
       " 's': 115,\n",
       " 'with': 354,\n",
       " 'if': 415,\n",
       " ' wh': 510,\n",
       " 'Ý': 221,\n",
       " '\\x1f': 31,\n",
       " 'en': 270,\n",
       " 'sha': 454,\n",
       " 'ä': 228,\n",
       " '¯': 175,\n",
       " 'the': 362,\n",
       " 'AR': 370,\n",
       " '2': 50,\n",
       " 'by ': 468,\n",
       " 'g': 103,\n",
       " 'e\\n': 393,\n",
       " 'el': 401,\n",
       " 'ô': 244,\n",
       " '{': 123,\n",
       " '0': 48,\n",
       " \"'\": 39,\n",
       " '¹': 185,\n",
       " 'That ': 456,\n",
       " ' my ': 394,\n",
       " 'o ': 269,\n",
       " ' his ': 450,\n",
       " '³': 179,\n",
       " '\\x8e': 142,\n",
       " 'e ': 256,\n",
       " 'G': 71,\n",
       " ' th': 273,\n",
       " 'thy ': 459,\n",
       " 'Ñ': 209,\n",
       " '®': 174,\n",
       " 'The ': 410,\n",
       " 'thou': 375,\n",
       " 'ch': 293,\n",
       " '\\x06': 6,\n",
       " 'su': 485,\n",
       " 'Ü': 220,\n",
       " 'ain': 409,\n",
       " '?\\n': 322,\n",
       " ' a': 292,\n",
       " 'sp': 471,\n",
       " 'le ': 411,\n",
       " '\\x9b': 155,\n",
       " 'Ò': 210,\n",
       " 'ne': 437,\n",
       " 'î': 238,\n",
       " 'An': 311,\n",
       " 'ma': 365,\n",
       " 'do ': 494,\n",
       " 'll ': 307,\n",
       " 'Ï': 207,\n",
       " 'no': 297,\n",
       " 'ç': 231,\n",
       " 'er ': 321,\n",
       " 'us ': 500,\n",
       " ']': 93,\n",
       " 'm': 109,\n",
       " 'I ': 312,\n",
       " ' m': 286,\n",
       " 'ir': 298,\n",
       " 'j': 106,\n",
       " '°': 176,\n",
       " 'Û': 219,\n",
       " 'æ': 230,\n",
       " 'ut': 486,\n",
       " '²': 178,\n",
       " '§': 167,\n",
       " 'not ': 327,\n",
       " 've ': 300,\n",
       " 'd, ': 345,\n",
       " '\\x96': 150,\n",
       " 'e.\\n': 397,\n",
       " 't ': 258,\n",
       " 'X': 88,\n",
       " 'U': 85,\n",
       " 'est ': 480,\n",
       " 'Î': 206,\n",
       " '\\x14': 20,\n",
       " 'õ': 245,\n",
       " 'fa': 353,\n",
       " 'ð': 240,\n",
       " 'D': 68,\n",
       " 'ith': 302,\n",
       " 'Í': 205,\n",
       " '_': 95,\n",
       " '}': 125,\n",
       " 'the ': 294,\n",
       " '5': 53,\n",
       " '\\x01': 1,\n",
       " 'V': 86,\n",
       " '¶': 182,\n",
       " 'our': 323,\n",
       " 'ra': 330,\n",
       " 'sh': 364,\n",
       " 'ñ': 241,\n",
       " 'ct': 498,\n",
       " 'Wh': 359,\n",
       " 'Ó': 211,\n",
       " 'N': 78,\n",
       " 'la': 340,\n",
       " 'B': 66,\n",
       " 'of': 288,\n",
       " '~': 126,\n",
       " 'have ': 355,\n",
       " '\\x0b': 11,\n",
       " ';': 59,\n",
       " 'y': 121,\n",
       " '\\x9e': 158,\n",
       " \"'s \": 325,\n",
       " '6': 54,\n",
       " 'Ù': 217,\n",
       " 'ý': 253,\n",
       " '×': 215,\n",
       " 'll': 275,\n",
       " 'it': 315,\n",
       " 'Z': 90,\n",
       " 'ther': 367,\n",
       " '.': 46,\n",
       " 'se': 304,\n",
       " 'P': 80,\n",
       " 'ag': 444,\n",
       " 'ard': 509,\n",
       " 'me ': 377,\n",
       " 's ': 259,\n",
       " 'he ': 398,\n",
       " '\\x08': 8,\n",
       " '±': 177,\n",
       " ' you': 408,\n",
       " 'y, ': 349,\n",
       " 'that ': 360,\n",
       " 'ri': 334,\n",
       " 'di': 404,\n",
       " '\\x8c': 140,\n",
       " 'ion': 412,\n",
       " '\\x81': 129,\n",
       " 'º': 186,\n",
       " '\\x1a': 26,\n",
       " 'se ': 378,\n",
       " 'e,\\n': 422,\n",
       " 'Õ': 213,\n",
       " 'ay': 436,\n",
       " 'ã': 227,\n",
       " ' in': 453,\n",
       " 'le': 320,\n",
       " 'my ': 328,\n",
       " 'et': 451,\n",
       " 'Ô': 212,\n",
       " 'KING': 470,\n",
       " 'king': 506,\n",
       " 'igh': 419,\n",
       " 'his ': 432,\n",
       " '\\x8f': 143,\n",
       " 'ª': 170,\n",
       " 'o': 111,\n",
       " 'id': 439,\n",
       " '\\x94': 148,\n",
       " '\\x1e': 30,\n",
       " ' this ': 475,\n",
       " 'ing': 290,\n",
       " 'ed': 424,\n",
       " 's.\\n': 507,\n",
       " '\\x1c': 28,\n",
       " '\\x84': 132,\n",
       " ', and ': 499,\n",
       " 'í': 237,\n",
       " ';\\n': 329,\n",
       " 's,\\n': 464,\n",
       " 'û': 251,\n",
       " '$': 36,\n",
       " 'ha': 276,\n",
       " 'ar': 271,\n",
       " 'ood ': 429,\n",
       " '\\x10': 16,\n",
       " '\\x13': 19,\n",
       " 'al': 373,\n",
       " ' the ': 303,\n",
       " 'But ': 479,\n",
       " 'and ': 283,\n",
       " 'ld ': 336,\n",
       " '\\x03': 3,\n",
       " 'Y': 89,\n",
       " 'h': 104,\n",
       " '\\r': 13,\n",
       " 'u': 117,\n",
       " '*': 42,\n",
       " 'self': 503,\n",
       " 'EL': 504,\n",
       " 'it ': 341,\n",
       " 'Ú': 218,\n",
       " '¤': 164,\n",
       " '. ': 369,\n",
       " 'this ': 474,\n",
       " 'Æ': 198,\n",
       " 'st ': 309,\n",
       " '/': 47,\n",
       " 'ce': 363,\n",
       " '\\x90': 144,\n",
       " '\\x98': 152,\n",
       " 'Å': 197,\n",
       " 'ter': 407,\n",
       " 'ent': 361,\n",
       " 'i': 105,\n",
       " 'ich': 427,\n",
       " '.\\n': 272,\n",
       " 'com': 396,\n",
       " '\\x12': 18,\n",
       " 'rom': 438,\n",
       " 'And ': 324,\n",
       " 'ur': 332,\n",
       " 'o, ': 481,\n",
       " '+': 43,\n",
       " '\\x9f': 159,\n",
       " '\\x04': 4,\n",
       " 'C': 67,\n",
       " '\\x1d': 29,\n",
       " '\\\\': 92,\n",
       " '\\t': 9,\n",
       " 'no ': 462,\n",
       " ')': 41,\n",
       " 'ca': 452,\n",
       " '¸': 184,\n",
       " '½': 189,\n",
       " '(': 40,\n",
       " 'r': 114,\n",
       " '\\x9d': 157,\n",
       " '\"': 34,\n",
       " 'è': 232,\n",
       " 'È': 200,\n",
       " 'ea': 285,\n",
       " 'con': 435,\n",
       " ' him': 472,\n",
       " 'á': 225,\n",
       " 'ru': 414,\n",
       " '\\x0c': 12,\n",
       " 'I w': 493,\n",
       " '\\x95': 149,\n",
       " 'ess ': 501,\n",
       " '&': 38,\n",
       " 'or': 268,\n",
       " '9': 57,\n",
       " 'ear': 313,\n",
       " 'þ': 254,\n",
       " ' that ': 476,\n",
       " '\\x7f': 127,\n",
       " 'IO:\\n': 384,\n",
       " 'I': 73,\n",
       " 'w': 119,\n",
       " 'É': 201,\n",
       " 'es ': 344,\n",
       " 'é': 233,\n",
       " 'you': 280,\n",
       " 'end': 491,\n",
       " 'Ê': 202,\n",
       " 'but ': 482,\n",
       " 'q': 113,\n",
       " '\\x07': 7,\n",
       " \"'d \": 383,\n",
       " 'ere ': 343,\n",
       " ', ': 261,\n",
       " '\\x9c': 156,\n",
       " '\\x17': 23,\n",
       " 'ould ': 392,\n",
       " 'A:\\n': 466,\n",
       " 'ú': 250,\n",
       " 'oo': 316,\n",
       " 'IC': 490,\n",
       " 'p': 112,\n",
       " 'ven': 467,\n",
       " '=': 61,\n",
       " 'K': 75,\n",
       " 'Ð': 208,\n",
       " 'ell': 388,\n",
       " 'ß': 223,\n",
       " 'â': 226,\n",
       " 'ó': 243,\n",
       " 'oth': 477,\n",
       " 'Ä': 196,\n",
       " '¨': 168,\n",
       " '\\x0f': 15,\n",
       " '\\x8a': 138,\n",
       " '¾': 190,\n",
       " ' to ': 399,\n",
       " '\\x92': 146,\n",
       " ':\\n': 267,\n",
       " 'EN': 347,\n",
       " 'now': 385,\n",
       " ' g': 405,\n",
       " '»': 187,\n",
       " 'ly ': 428,\n",
       " 'one ': 488,\n",
       " 'es': 279,\n",
       " 'For': 484,\n",
       " ' ': 32,\n",
       " '´': 180,\n",
       " '`': 96,\n",
       " 'e': 101,\n",
       " 'ce ': 371,\n",
       " ' I': 400,\n",
       " 'ill ': 425,\n",
       " 'ET': 455,\n",
       " '¢': 162,\n",
       " 'd': 100,\n",
       " 'know': 496,\n",
       " 'd ': 260,\n",
       " 'c': 99,\n",
       " 'at': 318,\n",
       " 'ë': 235,\n",
       " 'To ': 418,\n",
       " 'US:\\n': 402,\n",
       " 'l': 108,\n",
       " 'UC': 430,\n",
       " 'of ': 338,\n",
       " '?': 63,\n",
       " 'po': 457,\n",
       " 'S:\\n': 356,\n",
       " 'ver': 395,\n",
       " '\\x19': 25,\n",
       " 'em': 416,\n",
       " 'Q': 81,\n",
       " 'Ì': 204,\n",
       " '\\x97': 151,\n",
       " ' d': 351,\n",
       " ' t': 390,\n",
       " '!\\n': 357,\n",
       " '<': 60,\n",
       " ' of': 483,\n",
       " '\\x05': 5,\n",
       " 'S': 83,\n",
       " '>': 62,\n",
       " 'on': 274,\n",
       " 'your': 331,\n",
       " '\\x93': 147,\n",
       " 't, ': 391,\n",
       " 'wh': 381,\n",
       " '\\x86': 134,\n",
       " 'A': 65,\n",
       " 'ong': 478,\n",
       " '¦': 166,\n",
       " '£': 163,\n",
       " 'AN': 441,\n",
       " ' p': 366,\n",
       " 'be ': 387,\n",
       " 'ol': 403,\n",
       " '!': 33,\n",
       " 'ö': 246,\n",
       " '\\x0e': 14,\n",
       " 'sel': 449,\n",
       " 'all': 489,\n",
       " 'li': 305,\n",
       " '\\x1b': 27,\n",
       " 'E': 69,\n",
       " 'ke ': 348,\n",
       " '|': 124,\n",
       " 'to ': 282,\n",
       " '\\x00': 0,\n",
       " ',': 44,\n",
       " ' st': 502,\n",
       " 'ome ': 495,\n",
       " 'op': 511,\n",
       " '\\x82': 130,\n",
       " 'f': 102,\n",
       " 'and': 417,\n",
       " 'ÿ': 255,\n",
       " '1': 49,\n",
       " 'e, ': 301,\n",
       " 'J': 74,\n",
       " 'ER': 445,\n",
       " 'à': 224,\n",
       " 'b': 98,\n",
       " '\\x02': 2,\n",
       " '\\x11': 17,\n",
       " '@': 64,\n",
       " 'R': 82,\n",
       " '\\xad': 173,\n",
       " '7': 55,\n",
       " '«': 171,\n",
       " 'ay ': 406,\n",
       " 'ed ': 339,\n",
       " 'ord': 426,\n",
       " 'F': 70,\n",
       " ' f': 413,\n",
       " '¬': 172,\n",
       " '\\x16': 22}"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
