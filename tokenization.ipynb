{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a67439a-7953-41aa-b8ac-d010d1c4867a",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holmrenser/deep_learning/blob/main/tokenization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d7c526b1-34fb-4dd9-89f5-b188911ca92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citi\n",
      "[70, 105, 114, 115, 116, 32, 67, 105, 116, 105]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm.auto import trange\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Generator\n",
    "\n",
    "with open('input.txt', 'r') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "text_bytes = data.encode('utf-8')\n",
    "tokens = list(text_bytes)\n",
    "print(data[:10])\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "63191281-6333-4986-97dc-19ed433da6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\x01',\n",
       " 2: '\\x02',\n",
       " 3: '\\x03',\n",
       " 4: '\\x04',\n",
       " 5: '\\x05',\n",
       " 6: '\\x06',\n",
       " 7: '\\x07',\n",
       " 8: '\\x08',\n",
       " 9: '\\t',\n",
       " 10: '\\n',\n",
       " 11: '\\x0b',\n",
       " 12: '\\x0c',\n",
       " 13: '\\r',\n",
       " 14: '\\x0e',\n",
       " 15: '\\x0f',\n",
       " 16: '\\x10',\n",
       " 17: '\\x11',\n",
       " 18: '\\x12',\n",
       " 19: '\\x13',\n",
       " 20: '\\x14',\n",
       " 21: '\\x15',\n",
       " 22: '\\x16',\n",
       " 23: '\\x17',\n",
       " 24: '\\x18',\n",
       " 25: '\\x19',\n",
       " 26: '\\x1a',\n",
       " 27: '\\x1b',\n",
       " 28: '\\x1c',\n",
       " 29: '\\x1d',\n",
       " 30: '\\x1e',\n",
       " 31: '\\x1f',\n",
       " 32: ' ',\n",
       " 33: '!',\n",
       " 34: '\"',\n",
       " 35: '#',\n",
       " 36: '$',\n",
       " 37: '%',\n",
       " 38: '&',\n",
       " 39: \"'\",\n",
       " 40: '(',\n",
       " 41: ')',\n",
       " 42: '*',\n",
       " 43: '+',\n",
       " 44: ',',\n",
       " 45: '-',\n",
       " 46: '.',\n",
       " 47: '/',\n",
       " 48: '0',\n",
       " 49: '1',\n",
       " 50: '2',\n",
       " 51: '3',\n",
       " 52: '4',\n",
       " 53: '5',\n",
       " 54: '6',\n",
       " 55: '7',\n",
       " 56: '8',\n",
       " 57: '9',\n",
       " 58: ':',\n",
       " 59: ';',\n",
       " 60: '<',\n",
       " 61: '=',\n",
       " 62: '>',\n",
       " 63: '?',\n",
       " 64: '@',\n",
       " 65: 'A',\n",
       " 66: 'B',\n",
       " 67: 'C',\n",
       " 68: 'D',\n",
       " 69: 'E',\n",
       " 70: 'F',\n",
       " 71: 'G',\n",
       " 72: 'H',\n",
       " 73: 'I',\n",
       " 74: 'J',\n",
       " 75: 'K',\n",
       " 76: 'L',\n",
       " 77: 'M',\n",
       " 78: 'N',\n",
       " 79: 'O',\n",
       " 80: 'P',\n",
       " 81: 'Q',\n",
       " 82: 'R',\n",
       " 83: 'S',\n",
       " 84: 'T',\n",
       " 85: 'U',\n",
       " 86: 'V',\n",
       " 87: 'W',\n",
       " 88: 'X',\n",
       " 89: 'Y',\n",
       " 90: 'Z',\n",
       " 91: '[',\n",
       " 92: '\\\\',\n",
       " 93: ']',\n",
       " 94: '^',\n",
       " 95: '_',\n",
       " 96: '`',\n",
       " 97: 'a',\n",
       " 98: 'b',\n",
       " 99: 'c',\n",
       " 100: 'd',\n",
       " 101: 'e',\n",
       " 102: 'f',\n",
       " 103: 'g',\n",
       " 104: 'h',\n",
       " 105: 'i',\n",
       " 106: 'j',\n",
       " 107: 'k',\n",
       " 108: 'l',\n",
       " 109: 'm',\n",
       " 110: 'n',\n",
       " 111: 'o',\n",
       " 112: 'p',\n",
       " 113: 'q',\n",
       " 114: 'r',\n",
       " 115: 's',\n",
       " 116: 't',\n",
       " 117: 'u',\n",
       " 118: 'v',\n",
       " 119: 'w',\n",
       " 120: 'x',\n",
       " 121: 'y',\n",
       " 122: 'z',\n",
       " 123: '{',\n",
       " 124: '|',\n",
       " 125: '}',\n",
       " 126: '~',\n",
       " 127: '\\x7f',\n",
       " 128: '\\x80',\n",
       " 129: '\\x81',\n",
       " 130: '\\x82',\n",
       " 131: '\\x83',\n",
       " 132: '\\x84',\n",
       " 133: '\\x85',\n",
       " 134: '\\x86',\n",
       " 135: '\\x87',\n",
       " 136: '\\x88',\n",
       " 137: '\\x89',\n",
       " 138: '\\x8a',\n",
       " 139: '\\x8b',\n",
       " 140: '\\x8c',\n",
       " 141: '\\x8d',\n",
       " 142: '\\x8e',\n",
       " 143: '\\x8f',\n",
       " 144: '\\x90',\n",
       " 145: '\\x91',\n",
       " 146: '\\x92',\n",
       " 147: '\\x93',\n",
       " 148: '\\x94',\n",
       " 149: '\\x95',\n",
       " 150: '\\x96',\n",
       " 151: '\\x97',\n",
       " 152: '\\x98',\n",
       " 153: '\\x99',\n",
       " 154: '\\x9a',\n",
       " 155: '\\x9b',\n",
       " 156: '\\x9c',\n",
       " 157: '\\x9d',\n",
       " 158: '\\x9e',\n",
       " 159: '\\x9f',\n",
       " 160: '\\xa0',\n",
       " 161: '¡',\n",
       " 162: '¢',\n",
       " 163: '£',\n",
       " 164: '¤',\n",
       " 165: '¥',\n",
       " 166: '¦',\n",
       " 167: '§',\n",
       " 168: '¨',\n",
       " 169: '©',\n",
       " 170: 'ª',\n",
       " 171: '«',\n",
       " 172: '¬',\n",
       " 173: '\\xad',\n",
       " 174: '®',\n",
       " 175: '¯',\n",
       " 176: '°',\n",
       " 177: '±',\n",
       " 178: '²',\n",
       " 179: '³',\n",
       " 180: '´',\n",
       " 181: 'µ',\n",
       " 182: '¶',\n",
       " 183: '·',\n",
       " 184: '¸',\n",
       " 185: '¹',\n",
       " 186: 'º',\n",
       " 187: '»',\n",
       " 188: '¼',\n",
       " 189: '½',\n",
       " 190: '¾',\n",
       " 191: '¿',\n",
       " 192: 'À',\n",
       " 193: 'Á',\n",
       " 194: 'Â',\n",
       " 195: 'Ã',\n",
       " 196: 'Ä',\n",
       " 197: 'Å',\n",
       " 198: 'Æ',\n",
       " 199: 'Ç',\n",
       " 200: 'È',\n",
       " 201: 'É',\n",
       " 202: 'Ê',\n",
       " 203: 'Ë',\n",
       " 204: 'Ì',\n",
       " 205: 'Í',\n",
       " 206: 'Î',\n",
       " 207: 'Ï',\n",
       " 208: 'Ð',\n",
       " 209: 'Ñ',\n",
       " 210: 'Ò',\n",
       " 211: 'Ó',\n",
       " 212: 'Ô',\n",
       " 213: 'Õ',\n",
       " 214: 'Ö',\n",
       " 215: '×',\n",
       " 216: 'Ø',\n",
       " 217: 'Ù',\n",
       " 218: 'Ú',\n",
       " 219: 'Û',\n",
       " 220: 'Ü',\n",
       " 221: 'Ý',\n",
       " 222: 'Þ',\n",
       " 223: 'ß',\n",
       " 224: 'à',\n",
       " 225: 'á',\n",
       " 226: 'â',\n",
       " 227: 'ã',\n",
       " 228: 'ä',\n",
       " 229: 'å',\n",
       " 230: 'æ',\n",
       " 231: 'ç',\n",
       " 232: 'è',\n",
       " 233: 'é',\n",
       " 234: 'ê',\n",
       " 235: 'ë',\n",
       " 236: 'ì',\n",
       " 237: 'í',\n",
       " 238: 'î',\n",
       " 239: 'ï',\n",
       " 240: 'ð',\n",
       " 241: 'ñ',\n",
       " 242: 'ò',\n",
       " 243: 'ó',\n",
       " 244: 'ô',\n",
       " 245: 'õ',\n",
       " 246: 'ö',\n",
       " 247: '÷',\n",
       " 248: 'ø',\n",
       " 249: 'ù',\n",
       " 250: 'ú',\n",
       " 251: 'û',\n",
       " 252: 'ü',\n",
       " 253: 'ý',\n",
       " 254: 'þ',\n",
       " 255: 'ÿ'}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tokens(tokens: list[int], token_pair: tuple[int,int], new_token: int) -> list[int]:\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(token)\n",
    "            break\n",
    "        next_token = tokens[i+1]\n",
    "        if token_pair == (token, next_token):\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            i += 1\n",
    "    return new_tokens    \n",
    "\n",
    "@dataclass\n",
    "class BytePairEncoder:\n",
    "    merges: dict[int, tuple[int, int]] = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def vocab(self) -> dict[int, str]:\n",
    "        base_vocab = {token: chr(token) for token in range(256)}        \n",
    "        merge_vocab = {token: self.decode([token]) for token in self.merges}\n",
    "        return base_vocab | merge_vocab\n",
    "\n",
    "    def _token_to_bytes(self, token: int) -> Generator[int, None, None]:\n",
    "        if token not in self.merges:\n",
    "            yield token\n",
    "            return\n",
    "        for pair_token in self.merges[token]:\n",
    "            if pair_token >= 256:\n",
    "                yield from self._token_to_bytes(pair_token)\n",
    "            else:\n",
    "                yield pair_token\n",
    "\n",
    "    def train(self, input: str, vocab_size: int = 512) -> None:\n",
    "        assert vocab_size > 256, f'Invalid vocab_size: {vocab_size}, must be larger than 256'\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        num_merges = vocab_size - 256\n",
    "        for i in trange(num_merges):\n",
    "            pair_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "            merge_pair = pair_counts.most_common(1)[0][0]\n",
    "            new_token = 256 + i\n",
    "            self.merges[new_token] = merge_pair\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        for new_token, merge_pair in self.merges.items():\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        decoded_tokens = chain.from_iterable(map(self._token_to_bytes, tokens))\n",
    "        return bytes(decoded_tokens).decode('utf-8', errors='replace')\n",
    "\n",
    "    def save(self, prefix: str) -> None:\n",
    "        with open(f'{prefix}.vocab', 'w') as fh:\n",
    "            json.dump(self.vocab, fh)\n",
    "        with open(f'{prefix}.model', 'w') as fh:\n",
    "            json.dump(self.merges, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_filename: str) -> 'BytePairEncoder':\n",
    "        with open(model_filename, 'r') as fh:\n",
    "            merges = json.load(fh)\n",
    "        sanitized_merges = {int(k):tuple(v) for k,v in merges.items()}\n",
    "        return cls(sanitized_merges)\n",
    "\n",
    "bpe = BytePairEncoder()\n",
    "bpe.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6e6ccb87-882e-4d66-a871-fff8582ceab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0395c4e89443f5bc07f18a69fae999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bpe.train(data, vocab_size=260)\n",
    "bpe.save('shakespeare_260')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cbd5511b-627c-4067-ac87-eec4d3cb1ed2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3b285943-0404-4c10-91ee-1437d9f485c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BytePairEncoder(merges={256: (101, 32), 257: (116, 104), 258: (116, 32), 259: (115, 32)})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BytePairEncoder.load('./shakespeare_260.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2753f75b-592b-4b23-a96f-1b83f013bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sentencepiece\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor, SentencePieceTrainer\n",
    "\n",
    "SentencePieceTrainer.train('--input=input.txt --model_prefix=shakespeare_200 --vocab_size=200 --model_type=bpe')\n",
    "\n",
    "sp = SentencePieceProcessor()\n",
    "sp.load('shakespeare_200.model')\n",
    "\n",
    "sp.decode(sp.encode('hello how are you'))\n",
    "\n",
    "sp.vocab_size()\n",
    "\n",
    "sp.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
