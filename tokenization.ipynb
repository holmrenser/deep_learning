{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a67439a-7953-41aa-b8ac-d010d1c4867a",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holmrenser/deep_learning/blob/main/tokenization.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Tokenization\n",
    "Machine learning approaches for natural language processing face the problem of representing long stretches of natural language (i.e. words, sentences, paragraphs, chapters, etc.) in a meaningful and computationally efficient way. A trivial approach is to split text on interpunction and whitespace, effectively selecting individual words. The downside of this approach is that semantically similar words are encoded differently. For example, 'small', 'smaller', and 'smallest', would all be encoded as different entities, forcing a model to learn any semantic similarity from data alone. An alternative approach would encode 'small' as one entity, and 'er', and 'est' as separate entities. The benefit of this 'subword' approach is that it is more straightforward to model semantic similarity, the downside is that it is not straightforward to identify an optimal subword selection scheme. \n",
    "\n",
    "In this notebook we will explore the [byte pair encoding (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) algorithm for creating subword representations, a.k.a. tokens. Put simply, byte pair encoding iteratively merges the most frequent token pair into a new token, starting from the most simple tokens (e.g. letters), and continuing until the desired number of tokens is reached.\n",
    "\n",
    "For various computational reasons, letters are often represented as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2713507d-28cf-49a9-9847-7cd1f5407a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the whole notebook\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from tqdm.auto import trange\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Generator\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "079b9986-aa40-4a63-a6f3-e9c5efea262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘input.txt’ already there; not retrieving.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Download the tiny shakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5a8796b7-393e-4c41-9af8-a9468a689f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proce'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the tiny shakespeare data and show the first 100 characters\n",
    "with open('input.txt', 'r') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "data[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74ffeb-4785-44e8-99d2-62eb7f2eb5cf",
   "metadata": {},
   "source": [
    "## Naive 'tokens'\n",
    "Probably the simplest tokenization strategy is to assign integers to individual characters in a given dataset. We'll implement this strategy below with a few lines of code. We create two dictionaries that function as lookup table: one encoding characters to integers, and one decoding integers back to characters. To apply our lookup tables we use a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e9439091-ae2b-467b-b807-5c64cd0ade56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some_text = 'First Citizen:\\nBefore we proce'\n",
      "tokens = [18, 47, 56, 57, 58, 1, 15, 47, 58, 47, 64, 43, 52, 10, 0, 14, 43, 44, 53, 56, 43, 1, 61, 43, 1, 54, 56, 53, 41, 43]\n",
      "chars = ['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r', 'e', ' ', 'w', 'e', ' ', 'p', 'r', 'o', 'c', 'e']\n"
     ]
    }
   ],
   "source": [
    "# Calling 'set' on our data returns all individual characters, which are then lexicographically sorted\n",
    "chars = sorted(set(data))\n",
    "# Calling 'enumerate' returns the original iterator and increasing integers, which we'll use as tokens\n",
    "char_to_token = {char:token for token,char in enumerate(chars)}\n",
    "# Reverse the mapping to be able to decode\n",
    "token_to_char = {token:char for char,token in char_to_token.items()}\n",
    "\n",
    "some_text = data[:30]\n",
    "print(f'{some_text = }')\n",
    "\n",
    "# Encode the first 30 characters of the tiny shakespeare dataset\n",
    "tokens = [char_to_token[c] for c in some_text]\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Check that we retrieve our original text when decoding\n",
    "chars = [token_to_char[t] for t in tokens]\n",
    "print(f'{chars = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0830d5d-9c19-452a-81a6-4f98568bbd29",
   "metadata": {},
   "source": [
    "Below we add a bit more functionality and structure to the idea presented above. This allows us to more efficiently use our mappings as tokenizer, pass the tokenizer around more easily, and aligns with code conventions used in many of-the-shelf tokenizer libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "73292fcd-e9af-47a2-91d8-09b69748f246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained tokenizer: NaiveTokenizer(vocab_size=0)\n",
      "Trained tokenizer: NaiveTokenizer(vocab_size=65)\n",
      "tokens = [20, 47, 1, 46, 53, 61, 1, 39, 56, 43, 1, 63, 53, 59]\n",
      "Hi how are you\n"
     ]
    }
   ],
   "source": [
    "class NaiveTokenizer:\n",
    "    \"\"\"Character level tokenizer that enumerates unique characters in a training text\"\"\"\n",
    "    def __init__(self, encoding_dict: dict[str, int]=None):\n",
    "        if encoding_dict is None:\n",
    "            self.encoding_dict = dict()\n",
    "        else:\n",
    "            self.encoding_dict = encoding_dict\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NaiveTokenizer(vocab_size={self.vocab_size})'\n",
    "\n",
    "    @property\n",
    "    def decoding_dict(self) -> dict[int, str]:\n",
    "        \"\"\"Decoding dict is implemented as property to automatically sync with changed encoding dict\"\"\"\n",
    "        return {token:char for char,token in self.encoding_dict.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.encoding_dict)\n",
    "\n",
    "    def train(self, data: str) -> None:\n",
    "        \"\"\"Train on a piece of text by enumerating unique characters\"\"\"\n",
    "        chars = sorted(set(data))\n",
    "        self.encoding_dict = {char:token for token,char in enumerate(chars)}\n",
    "\n",
    "    def encode(self, data: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        return [self.encoding_dict.get(char, -1) for char in data]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        return ''.join(self.decoding_dict.get(token, '<unk>') for token in tokens)\n",
    "\n",
    "# Initialize an empty NaiveTokenizer\n",
    "tokenizer = NaiveTokenizer()\n",
    "print(f'Untrained tokenizer: {tokenizer}')\n",
    "\n",
    "# 'Train' on tiny shakespeare\n",
    "tokenizer.train(data)\n",
    "print(f'Trained tokenizer: {tokenizer}')\n",
    "\n",
    "# Encode a string that is not in the training data\n",
    "tokens = tokenizer.encode('Hi how are you')\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Decode the encoding\n",
    "print(tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172bf54-0ea4-4820-88f1-ca27550abaee",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Investigate the vocabulary of the trained tokenizer by printing `tokenizer.encoding_dict`. Can you come up with a string that cannot be effectively encoded by our naive tokenizer? What happens to this string? How would you circumvent this issue?\n",
    "\n",
    "## Byte Pair Encoding (BPE)\n",
    "\n",
    "#### Converting to- and from bytes\n",
    "Apart from not being able to process unseen characters, tokenizing by enumerating characters in a training text has another issue: different training datasets can assign different tokens to the same character. The most common solution to these problems is to encode characters using [Unicode](https://en.wikipedia.org/wiki/Unicode) codepoints, specifically [UTF-8](https://en.wikipedia.org/wiki/UTF-8). Without going into too much detail, UTF-8 uses up to 4 bytes to encode individual characters, where every codepoint (i.e. byte or byte sequence) can be interpreted as an integer. The byte pair encoding algorithm iteratively merges these unicode bytes.\n",
    "\n",
    "In python, converting individual characters to unicode codepoints and back can be done with the built in `ord` and `chr` functions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "531b0b2b-8ad1-459f-9438-530d62e0310c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single character to unicode\n",
    "ord('H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "012763b9-3f7c-4ace-9fb9-7f3889375f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single unicode codepoint to character\n",
    "chr(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0cb18ada-42e4-4124-bfd0-af20914cf1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unicode_codepoints = [68, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 105, 115, 32, 97, 119, 101, 115, 111, 109, 101]\n",
      "characters = ['D', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ', 'i', 's', ' ', 'a', 'w', 'e', 's', 'o', 'm', 'e']\n"
     ]
    }
   ],
   "source": [
    "# Converting a string to unicode and back\n",
    "some_text = 'Deep learning is awesome'\n",
    "\n",
    "unicode_codepoints = [ord(letter) for letter in some_text]\n",
    "print(f'{unicode_codepoints = }')\n",
    "\n",
    "characters = [chr(codepoint) for codepoint in unicode_codepoints]\n",
    "print(f'{characters = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69399373-d6ef-40ae-8103-ab02b8d53f93",
   "metadata": {},
   "source": [
    "The same principles outlined above can be applied to multi-character strings with a slightly different syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "47498365-3d17-4771-abd1-3141bc0b8a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_bytes = b'Deep learning is awesome'\n",
      "tokens = [68, 101, 101, 112, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 105, 115, 32, 97, 119, 101, 115, 111, 109, 101]\n",
      "reconstructed_text = 'Deep learning is awesome'\n"
     ]
    }
   ],
   "source": [
    "some_text = 'Deep learning is awesome'\n",
    "\n",
    "# Using the 'encode' method on a string converts to bytes, note the leading 'b' when printing\n",
    "text_bytes = some_text.encode('utf-8')\n",
    "print(f'{text_bytes = }')\n",
    "\n",
    "# The list constructor iterates over the bytes, automatically converting to integers\n",
    "tokens = list(text_bytes)\n",
    "print(f'{tokens = }')\n",
    "\n",
    "# Turning a list of integers into bytes and subsequently 'decoding' into text\n",
    "reconstructed_text = bytes(tokens).decode('utf-8')\n",
    "print(f'{reconstructed_text = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fef04-93e7-4d7d-987f-8e82f365f85b",
   "metadata": {},
   "source": [
    "#### Counting pairs\n",
    "Byte pair encoding iteratively merges the most frequent pairs of bytes. We use some built in python functionality to count pairs in an iterator (the example uses characters, BPE  uses bytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f758176e-ec02-4e3d-a50a-ad3d8f42f76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most_common_pair = [(('r', 'e'), 5)]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = 'A text with some repetition somesome reprepreprepetition'\n",
    "\n",
    "# zip together original and shifted sequence\n",
    "pairs = zip(example_sequence[:-1], example_sequence[1:])\n",
    "\n",
    "# count using inbuilt counter from collections module (part of python standard lib)\n",
    "pair_counts = Counter(pairs)\n",
    "\n",
    "# select most common pair\n",
    "most_common_pair = pair_counts.most_common(1)\n",
    "print(f'{most_common_pair = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdb4e7-fa75-4326-b3a4-7074018f041a",
   "metadata": {},
   "source": [
    "#### BPE implementation\n",
    "Below we implement a function to merge token pairs and some functionality to train the tokenizer, encode strings, decode tokens, and save and load trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "63191281-6333-4986-97dc-19ed433da6d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " '\\x05': 5,\n",
       " '\\x06': 6,\n",
       " '\\x07': 7,\n",
       " '\\x08': 8,\n",
       " '\\t': 9,\n",
       " '\\n': 10,\n",
       " '\\x0b': 11,\n",
       " '\\x0c': 12,\n",
       " '\\r': 13,\n",
       " '\\x0e': 14,\n",
       " '\\x0f': 15,\n",
       " '\\x10': 16,\n",
       " '\\x11': 17,\n",
       " '\\x12': 18,\n",
       " '\\x13': 19,\n",
       " '\\x14': 20,\n",
       " '\\x15': 21,\n",
       " '\\x16': 22,\n",
       " '\\x17': 23,\n",
       " '\\x18': 24,\n",
       " '\\x19': 25,\n",
       " '\\x1a': 26,\n",
       " '\\x1b': 27,\n",
       " '\\x1c': 28,\n",
       " '\\x1d': 29,\n",
       " '\\x1e': 30,\n",
       " '\\x1f': 31,\n",
       " ' ': 32,\n",
       " '!': 33,\n",
       " '\"': 34,\n",
       " '#': 35,\n",
       " '$': 36,\n",
       " '%': 37,\n",
       " '&': 38,\n",
       " \"'\": 39,\n",
       " '(': 40,\n",
       " ')': 41,\n",
       " '*': 42,\n",
       " '+': 43,\n",
       " ',': 44,\n",
       " '-': 45,\n",
       " '.': 46,\n",
       " '/': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " '2': 50,\n",
       " '3': 51,\n",
       " '4': 52,\n",
       " '5': 53,\n",
       " '6': 54,\n",
       " '7': 55,\n",
       " '8': 56,\n",
       " '9': 57,\n",
       " ':': 58,\n",
       " ';': 59,\n",
       " '<': 60,\n",
       " '=': 61,\n",
       " '>': 62,\n",
       " '?': 63,\n",
       " '@': 64,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90,\n",
       " '[': 91,\n",
       " '\\\\': 92,\n",
       " ']': 93,\n",
       " '^': 94,\n",
       " '_': 95,\n",
       " '`': 96,\n",
       " 'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " '{': 123,\n",
       " '|': 124,\n",
       " '}': 125,\n",
       " '~': 126,\n",
       " '\\x7f': 127}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_tokens(tokens: list[int], token_pair: tuple[int,int], new_token: int) -> list[int]:\n",
    "    \"\"\"Takes a list of tokens and replaces every occurence of token_pair with new_token\"\"\"\n",
    "    new_tokens = []\n",
    "    i = 0\n",
    "    # Iterate in a while loop because we want to jump ahead two steps sometimes\n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        # Edge case: final individual token\n",
    "        if i == len(tokens) - 1:\n",
    "            new_tokens.append(token)\n",
    "            break\n",
    "        # Look ahead one token to find a token pair\n",
    "        next_token = tokens[i+1]\n",
    "        # On match we should jump ahead two tokens to skip the original pair\n",
    "        if token_pair == (token, next_token):\n",
    "            new_tokens.append(new_token)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            i += 1\n",
    "    return new_tokens    \n",
    "\n",
    "class BytePairEncoder:\n",
    "    \"\"\"Bytepair encoder with a base vocabulary of the first 256 utf-8 codepoints (this captures all 'normal' alphanumeric characters)\"\"\"\n",
    "    def __init__(self, merges: dict[int, tuple[int, int]]=None):\n",
    "        if merges is None:\n",
    "            self.merges = dict()\n",
    "        else:\n",
    "            self.merges = merges\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        vocab_size = self.vocab_size\n",
    "        n_merges = len(self.merges)\n",
    "        return f'BytePairEncoder({vocab_size=} {n_merges=})'\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self.get_vocab())\n",
    "\n",
    "    def get_vocab(self) -> dict[int, str]:\n",
    "        # Base vocabulary of first 256 utf-8 characters\n",
    "        base_vocab = {chr(token): token for token in range(128)}\n",
    "        # Additional vocabulary is determined by trained merges\n",
    "        merge_vocab = {self.decode([token]): token for token in self.merges}\n",
    "        # Total vocabulary is the union of base and merge vocabs\n",
    "        vocab = base_vocab | merge_vocab\n",
    "        return vocab\n",
    "\n",
    "    def _get_parent_tokens(self, token: int) -> Generator[int, None, None]:\n",
    "        \"\"\"Recursively identify whether a token is made up of parent tokens\"\"\"\n",
    "        if token not in self.merges:\n",
    "            yield token\n",
    "            return\n",
    "        for pair_token in self.merges[token]:\n",
    "            yield from self._get_parent_tokens(pair_token)\n",
    "\n",
    "    def train(self, input: str, vocab_size: int = 512) -> None:\n",
    "        \"\"\"Training proceeds by iteratively merging the most frequent token pair until the desired number of tokens is reached\"\"\"\n",
    "        assert vocab_size > 128, f'Invalid vocab_size: {vocab_size}, must be larger than 256'\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        num_merges = vocab_size - 128\n",
    "        for i in trange(num_merges):\n",
    "            pair_counts = Counter(zip(tokens[:-1], tokens[1:]))\n",
    "            merge_pair = pair_counts.most_common(1)[0][0]\n",
    "            new_token = 128 + i\n",
    "            self.merges[new_token] = merge_pair\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        \"\"\"Convert text to tokens\"\"\"\n",
    "        tokens = list(input.encode('utf-8'))\n",
    "        for new_token, merge_pair in self.merges.items():\n",
    "            tokens = merge_tokens(tokens, merge_pair, new_token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Convert tokens to text\"\"\"\n",
    "        decoded_tokens = chain.from_iterable(map(self._get_parent_tokens, tokens))\n",
    "        return bytes(decoded_tokens).decode('utf-8', errors='replace')\n",
    "\n",
    "    def save(self, prefix: str) -> None:\n",
    "        \"\"\"Save a trained model\"\"\"\n",
    "        with open(f'{prefix}.vocab', 'w') as fh:\n",
    "            json.dump(self.get_vocab(), fh)\n",
    "        with open(f'{prefix}.model', 'w') as fh:\n",
    "            json.dump(self.merges, fh)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, model_filename: str) -> 'BytePairEncoder':\n",
    "        \"\"\"Load a pretrained model from a .model file\"\"\"\n",
    "        assert model_filename.endswith('.model'), f'{model_filename} is not a valid model file, must end with .model'\n",
    "        with open(model_filename, 'r') as fh:\n",
    "            merges = json.load(fh)\n",
    "        # The json fileformat does not accept integers as dict keys, and does not have tuples\n",
    "        sanitized_merges = {int(k):tuple(v) for k,v in merges.items()}\n",
    "        return cls(sanitized_merges)\n",
    "\n",
    "bpe = BytePairEncoder()\n",
    "bpe.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6e6ccb87-882e-4d66-a871-fff8582ceab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3a7e7afb404bef8fb8842b88881eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bpe.train(data, vocab_size=256)\n",
    "bpe.save('shakespeare_256')\n",
    "bpe = BytePairEncoder.load('./shakespeare_256.model')\n",
    "bpe.get_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263114c-4829-4725-ad0e-d7d5fe6529a6",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Train a byte pair encoder on the tiny shakespeare dataset with a vocab_size of 256 and inspect the vocabulary. Can you identify tokens that encode some semantically meaningful identity?\n",
    "\n",
    "## Optimized BPE using the transformers library\n",
    "As you can imagine, our python implementation is not optimized to be fast. Several optimized tokenizers are commonly used, most of which have python bindings for ease of use. Below we will reproduce the configuration of our python BPE tokenizer using the transformers tokenizers library. This allows us to train larger vocabularies is a shorter amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d1e4b620-79aa-43fb-9821-8abed25a1ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE(byte_fallback=True))\n",
    "trainer = trainers.BpeTrainer(\n",
    "    initial_alphabet=[chr(i) for i in range(128)],\n",
    "    vocab_size=1024\n",
    ")\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.train([\"input.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5f2f5af9-6c02-41d4-ae0b-79eeba89e23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'i', ' h', 'ow', ' ', 'ar', 'e ', 'you', ' ', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode([i]) for i in tokenizer.encode(\"Hi how are you 1234\").ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "45790785-3076-44fd-81f1-e54f78567450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi how are you 1234'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(\"Hi how are you 1234\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e1b4b16c-c308-4e29-9d1e-4c353572e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'i', ' h', 'ow', ' ', 'ar', 'e ', 'you', ' ', '1', '2', '3', '4']"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[bpe.decode([i]) for i in bpe.encode('Hi how are you 1234')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "908fc3c5-c492-4358-a87d-249ed2a149b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi how are you 1234'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.decode(bpe.encode('Hi how are you 1234'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7437723e-eb94-47b0-9c8f-5a92b272a24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 105, 161, 156, 32, 143, 128, 152, 32, 49, 50, 51, 52]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('Hi how are you 1234').ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "cf765698-4680-48ec-a4d2-edf71ab31d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 105, 162, 157, 32, 144, 128, 153, 32, 49, 50, 51, 52]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.encode('Hi how are you 1234')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d65277c9-5b91-48eb-a6cc-bcac1f4aba68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\x00', 0),\n",
       " ('\\x01', 1),\n",
       " ('\\x02', 2),\n",
       " ('\\x03', 3),\n",
       " ('\\x04', 4),\n",
       " ('\\x05', 5),\n",
       " ('\\x06', 6),\n",
       " ('\\x07', 7),\n",
       " ('\\x08', 8),\n",
       " ('\\t', 9),\n",
       " ('\\n', 10),\n",
       " ('\\x0b', 11),\n",
       " ('\\x0c', 12),\n",
       " ('\\r', 13),\n",
       " ('\\x0e', 14),\n",
       " ('\\x0f', 15),\n",
       " ('\\x10', 16),\n",
       " ('\\x11', 17),\n",
       " ('\\x12', 18),\n",
       " ('\\x13', 19),\n",
       " ('\\x14', 20),\n",
       " ('\\x15', 21),\n",
       " ('\\x16', 22),\n",
       " ('\\x17', 23),\n",
       " ('\\x18', 24),\n",
       " ('\\x19', 25),\n",
       " ('\\x1a', 26),\n",
       " ('\\x1b', 27),\n",
       " ('\\x1c', 28),\n",
       " ('\\x1d', 29),\n",
       " ('\\x1e', 30),\n",
       " ('\\x1f', 31),\n",
       " (' ', 32),\n",
       " ('!', 33),\n",
       " ('\"', 34),\n",
       " ('#', 35),\n",
       " ('$', 36),\n",
       " ('%', 37),\n",
       " ('&', 38),\n",
       " (\"'\", 39),\n",
       " ('(', 40),\n",
       " (')', 41),\n",
       " ('*', 42),\n",
       " ('+', 43),\n",
       " (',', 44),\n",
       " ('-', 45),\n",
       " ('.', 46),\n",
       " ('/', 47),\n",
       " ('0', 48),\n",
       " ('1', 49),\n",
       " ('2', 50),\n",
       " ('3', 51),\n",
       " ('4', 52),\n",
       " ('5', 53),\n",
       " ('6', 54),\n",
       " ('7', 55),\n",
       " ('8', 56),\n",
       " ('9', 57),\n",
       " (':', 58),\n",
       " (';', 59),\n",
       " ('<', 60),\n",
       " ('=', 61),\n",
       " ('>', 62),\n",
       " ('?', 63),\n",
       " ('@', 64),\n",
       " ('A', 65),\n",
       " ('B', 66),\n",
       " ('C', 67),\n",
       " ('D', 68),\n",
       " ('E', 69),\n",
       " ('F', 70),\n",
       " ('G', 71),\n",
       " ('H', 72),\n",
       " ('I', 73),\n",
       " ('J', 74),\n",
       " ('K', 75),\n",
       " ('L', 76),\n",
       " ('M', 77),\n",
       " ('N', 78),\n",
       " ('O', 79),\n",
       " ('P', 80),\n",
       " ('Q', 81),\n",
       " ('R', 82),\n",
       " ('S', 83),\n",
       " ('T', 84),\n",
       " ('U', 85),\n",
       " ('V', 86),\n",
       " ('W', 87),\n",
       " ('X', 88),\n",
       " ('Y', 89),\n",
       " ('Z', 90),\n",
       " ('[', 91),\n",
       " ('\\\\', 92),\n",
       " (']', 93),\n",
       " ('^', 94),\n",
       " ('_', 95),\n",
       " ('`', 96),\n",
       " ('a', 97),\n",
       " ('b', 98),\n",
       " ('c', 99),\n",
       " ('d', 100),\n",
       " ('e', 101),\n",
       " ('f', 102),\n",
       " ('g', 103),\n",
       " ('h', 104),\n",
       " ('i', 105),\n",
       " ('j', 106),\n",
       " ('k', 107),\n",
       " ('l', 108),\n",
       " ('m', 109),\n",
       " ('n', 110),\n",
       " ('o', 111),\n",
       " ('p', 112),\n",
       " ('q', 113),\n",
       " ('r', 114),\n",
       " ('s', 115),\n",
       " ('t', 116),\n",
       " ('u', 117),\n",
       " ('v', 118),\n",
       " ('w', 119),\n",
       " ('x', 120),\n",
       " ('y', 121),\n",
       " ('z', 122),\n",
       " ('{', 123),\n",
       " ('|', 124),\n",
       " ('}', 125),\n",
       " ('~', 126),\n",
       " ('\\x7f', 127),\n",
       " ('e ', 128),\n",
       " ('th', 129),\n",
       " ('t ', 130),\n",
       " ('s ', 131),\n",
       " ('d ', 132),\n",
       " (', ', 133),\n",
       " ('ou', 134),\n",
       " ('er', 135),\n",
       " ('in', 136),\n",
       " ('y ', 137),\n",
       " ('an', 138),\n",
       " (':\\n', 139),\n",
       " ('or', 140),\n",
       " ('o ', 141),\n",
       " ('en', 142),\n",
       " ('ar', 143),\n",
       " ('.\\n', 144),\n",
       " (' th', 145),\n",
       " ('on', 146),\n",
       " ('ll', 147),\n",
       " ('ha', 148),\n",
       " (',\\n', 149),\n",
       " ('is ', 150),\n",
       " ('es', 151),\n",
       " ('you', 152),\n",
       " (' s', 153),\n",
       " ('to ', 154),\n",
       " ('and ', 155),\n",
       " ('ow', 156),\n",
       " ('ea', 157),\n",
       " (' m', 158),\n",
       " (' w', 159),\n",
       " ('of', 160),\n",
       " (' h', 161),\n",
       " ('ing', 162),\n",
       " ('om', 163),\n",
       " (' a', 164),\n",
       " ('ch', 165),\n",
       " ('the ', 166),\n",
       " ('st', 167),\n",
       " (' b', 168),\n",
       " ('no', 169),\n",
       " ('ir', 170),\n",
       " ('for', 171),\n",
       " ('ve ', 172),\n",
       " ('e, ', 173),\n",
       " ('ith', 174),\n",
       " (' the ', 175),\n",
       " ('se', 176),\n",
       " ('li', 177),\n",
       " ('Th', 178),\n",
       " ('ll ', 179),\n",
       " ('re', 180),\n",
       " ('st ', 181),\n",
       " ('at ', 182),\n",
       " ('An', 183),\n",
       " ('I ', 184),\n",
       " ('ear', 185),\n",
       " ('im', 186),\n",
       " ('it', 187),\n",
       " ('oo', 188),\n",
       " ('gh', 189),\n",
       " ('at', 190),\n",
       " ('is', 191),\n",
       " ('le', 192),\n",
       " ('er ', 193),\n",
       " ('?\\n', 194),\n",
       " ('our', 195),\n",
       " ('And ', 196),\n",
       " (\"'s \", 197),\n",
       " ('ee', 198),\n",
       " ('not ', 199),\n",
       " ('my ', 200),\n",
       " (';\\n', 201),\n",
       " ('ra', 202),\n",
       " ('your', 203),\n",
       " ('ur', 204),\n",
       " ('hat ', 205),\n",
       " ('ri', 206),\n",
       " ('ut ', 207),\n",
       " ('ld ', 208),\n",
       " ('O:\\n', 209),\n",
       " ('of ', 210),\n",
       " ('ed ', 211),\n",
       " ('la', 212),\n",
       " ('it ', 213),\n",
       " ('ro', 214),\n",
       " ('ere ', 215),\n",
       " ('es ', 216),\n",
       " ('d, ', 217),\n",
       " ('un', 218),\n",
       " ('EN', 219),\n",
       " ('ke ', 220),\n",
       " ('y, ', 221),\n",
       " ('IN', 222),\n",
       " (' d', 223),\n",
       " ('as ', 224),\n",
       " ('fa', 225),\n",
       " ('with', 226),\n",
       " ('have ', 227),\n",
       " ('S:\\n', 228),\n",
       " ('!\\n', 229),\n",
       " (' c', 230),\n",
       " ('Wh', 231),\n",
       " ('that ', 232),\n",
       " ('ent', 233),\n",
       " ('the', 234),\n",
       " ('ce', 235),\n",
       " ('sh', 236),\n",
       " ('ma', 237),\n",
       " (' p', 238),\n",
       " ('ther', 239),\n",
       " ('be', 240),\n",
       " ('. ', 241),\n",
       " ('AR', 242),\n",
       " ('ce ', 243),\n",
       " ('ing ', 244),\n",
       " ('al', 245),\n",
       " ('; ', 246),\n",
       " ('thou', 247),\n",
       " ('s, ', 248),\n",
       " ('me ', 249),\n",
       " ('se ', 250),\n",
       " ('lo', 251),\n",
       " ('ck', 252),\n",
       " ('wh', 253),\n",
       " ('il', 254),\n",
       " (\"'d \", 255),\n",
       " ('IO:\\n', 256),\n",
       " ('now', 257),\n",
       " ('ill', 258),\n",
       " ('be ', 259),\n",
       " ('ell', 260),\n",
       " ('rea', 261),\n",
       " (' t', 262),\n",
       " ('t, ', 263),\n",
       " ('ould ', 264),\n",
       " ('e\\n', 265),\n",
       " (' my ', 266),\n",
       " ('ver', 267),\n",
       " ('com', 268),\n",
       " ('e.\\n', 269),\n",
       " ('he ', 270),\n",
       " (' to ', 271),\n",
       " (' I', 272),\n",
       " ('el', 273),\n",
       " ('US:\\n', 274),\n",
       " ('ol', 275),\n",
       " ('di', 276),\n",
       " (' g', 277),\n",
       " ('ay ', 278),\n",
       " ('ter', 279),\n",
       " (' you', 280),\n",
       " ('ain', 281),\n",
       " ('The ', 282),\n",
       " ('le ', 283),\n",
       " ('ion', 284),\n",
       " (' f', 285),\n",
       " ('ru', 286),\n",
       " ('if', 287),\n",
       " ('em', 288),\n",
       " ('and', 289),\n",
       " ('To ', 290),\n",
       " ('igh', 291),\n",
       " ('are ', 292),\n",
       " ('up', 293),\n",
       " ('e,\\n', 294),\n",
       " ('him', 295),\n",
       " ('ed', 296),\n",
       " ('ill ', 297),\n",
       " ('ord', 298),\n",
       " ('ich', 299),\n",
       " ('ly ', 300),\n",
       " ('ood ', 301),\n",
       " ('UC', 302),\n",
       " ('own', 303),\n",
       " ('his ', 304),\n",
       " ('ING', 305),\n",
       " (' and ', 306),\n",
       " ('con', 307),\n",
       " ('ay', 308),\n",
       " ('ne', 309),\n",
       " ('rom', 310),\n",
       " ('id', 311),\n",
       " ('us', 312),\n",
       " ('AN', 313),\n",
       " ('oun', 314),\n",
       " ('man', 315),\n",
       " ('ag', 316),\n",
       " ('ER', 317),\n",
       " ('OR', 318),\n",
       " ('et ', 319),\n",
       " ('res', 320),\n",
       " ('sel', 321),\n",
       " (' his ', 322),\n",
       " ('et', 323),\n",
       " ('ca', 324),\n",
       " (' in', 325),\n",
       " ('sha', 326),\n",
       " ('ET', 327),\n",
       " ('That ', 328),\n",
       " ('po', 329),\n",
       " ('qu', 330),\n",
       " ('thy ', 331),\n",
       " ('mor', 332),\n",
       " ('ul', 333),\n",
       " ('no ', 334),\n",
       " ('am', 335),\n",
       " ('s,\\n', 336),\n",
       " (' the', 337),\n",
       " ('A:\\n', 338),\n",
       " ('ven', 339),\n",
       " ('by ', 340),\n",
       " ('s\\n', 341),\n",
       " ('KING', 342),\n",
       " ('sp', 343),\n",
       " (' him', 344),\n",
       " ('her', 345),\n",
       " ('this ', 346),\n",
       " (' this ', 347),\n",
       " (' that ', 348),\n",
       " ('oth', 349),\n",
       " ('ong', 350),\n",
       " ('But ', 351),\n",
       " ('est ', 352),\n",
       " ('o, ', 353),\n",
       " ('but ', 354),\n",
       " (' of', 355),\n",
       " ('For', 356),\n",
       " ('su', 357),\n",
       " ('ut', 358),\n",
       " (' with', 359),\n",
       " ('one ', 360),\n",
       " ('all', 361),\n",
       " ('IC', 362),\n",
       " ('end', 363),\n",
       " ('OL', 364),\n",
       " ('I w', 365),\n",
       " ('do ', 366),\n",
       " ('ome ', 367),\n",
       " ('know', 368),\n",
       " ('sir', 369),\n",
       " ('ct', 370),\n",
       " (', and ', 371),\n",
       " ('us ', 372),\n",
       " ('ess ', 373),\n",
       " (' st', 374),\n",
       " ('self', 375),\n",
       " ('EL', 376),\n",
       " ('ake ', 377),\n",
       " ('king', 378),\n",
       " ('s.\\n', 379),\n",
       " ('lea', 380),\n",
       " ('ard', 381),\n",
       " (' wh', 382),\n",
       " ('op', 383),\n",
       " ('wor', 384),\n",
       " (' be ', 385),\n",
       " ('ad', 386),\n",
       " ('IUS:\\n', 387),\n",
       " ('ARD', 388),\n",
       " ('KING ', 389),\n",
       " ('so ', 390),\n",
       " (' thou', 391),\n",
       " (' your', 392),\n",
       " ('hear', 393),\n",
       " ('ent ', 394),\n",
       " ('mo', 395),\n",
       " ('est', 396),\n",
       " ('--', 397),\n",
       " ('to', 398),\n",
       " ('ci', 399),\n",
       " ('ST', 400),\n",
       " ('My ', 401),\n",
       " ('you ', 402),\n",
       " ('en ', 403),\n",
       " (\"'t\", 404),\n",
       " ('ous ', 405),\n",
       " ('we ', 406),\n",
       " ('d\\n', 407),\n",
       " ('loo', 408),\n",
       " (': ', 409),\n",
       " ('E:\\n', 410),\n",
       " ('! ', 411),\n",
       " (\"I'\", 412),\n",
       " (' am', 413),\n",
       " (' thy ', 414),\n",
       " ('ON', 415),\n",
       " ('de', 416),\n",
       " (' he ', 417),\n",
       " ('ran', 418),\n",
       " ('in ', 419),\n",
       " ('dea', 420),\n",
       " ('sa', 421),\n",
       " ('ink', 422),\n",
       " ('an ', 423),\n",
       " ('uch', 424),\n",
       " (' sha', 425),\n",
       " ('ish', 426),\n",
       " ('son', 427),\n",
       " ('from', 428),\n",
       " ('ive ', 429),\n",
       " ('good ', 430),\n",
       " ('sw', 431),\n",
       " ('ust ', 432),\n",
       " ('ab', 433),\n",
       " ('lord', 434),\n",
       " ('ere', 435),\n",
       " ('As ', 436),\n",
       " ('hath', 437),\n",
       " ('What ', 438),\n",
       " ('a ', 439),\n",
       " ('eak', 440),\n",
       " ('t\\n', 441),\n",
       " ('row', 442),\n",
       " ('y.\\n', 443),\n",
       " ('ine ', 444),\n",
       " ('oul', 445),\n",
       " ('ig', 446),\n",
       " ('der', 447),\n",
       " ('You', 448),\n",
       " ('per', 449),\n",
       " ('lov', 450),\n",
       " ('pr', 451),\n",
       " ('like ', 452),\n",
       " ('ARD I', 453),\n",
       " ('Ha', 454),\n",
       " ('RO', 455),\n",
       " ('e to ', 456),\n",
       " ('TIO:\\n', 457),\n",
       " ('ic', 458),\n",
       " ('ough', 459),\n",
       " (' sh', 460),\n",
       " ('y\\n', 461),\n",
       " ('upon', 462),\n",
       " ('I:\\n', 463),\n",
       " ('out ', 464),\n",
       " ('or ', 465),\n",
       " ('ET:\\n', 466),\n",
       " ('AU', 467),\n",
       " ('was ', 468),\n",
       " ('can', 469),\n",
       " ('pro', 470),\n",
       " ('what ', 471),\n",
       " ('erv', 472),\n",
       " ('d,\\n', 473),\n",
       " ('ay, ', 474),\n",
       " ('let ', 475),\n",
       " ('Y ', 476),\n",
       " ('E ', 477),\n",
       " ('id ', 478),\n",
       " ('ant', 479),\n",
       " ('I have ', 480),\n",
       " (' but ', 481),\n",
       " ('our ', 482),\n",
       " ('for ', 483),\n",
       " (' be', 484),\n",
       " ('hea', 485),\n",
       " ('lif', 486),\n",
       " ('me', 487),\n",
       " ('t.\\n', 488),\n",
       " ('old ', 489),\n",
       " ('well', 490),\n",
       " ('In', 491),\n",
       " ('um', 492),\n",
       " ('irst ', 493),\n",
       " ('? ', 494),\n",
       " ('as', 495),\n",
       " ('ey', 496),\n",
       " ('ang', 497),\n",
       " ('I am', 498),\n",
       " (' of ', 499),\n",
       " ('ist', 500),\n",
       " ('end ', 501),\n",
       " ('them', 502),\n",
       " ('K:\\n', 503),\n",
       " ('d.\\n', 504),\n",
       " ('If', 505),\n",
       " ('your ', 506),\n",
       " ('CA', 507),\n",
       " ('hat', 508),\n",
       " ('ICH', 509),\n",
       " ('RICH', 510),\n",
       " ('war', 511),\n",
       " ('DU', 512),\n",
       " ('eep', 513),\n",
       " ('thou ', 514),\n",
       " ('DUK', 515),\n",
       " ('ful', 516),\n",
       " ('DUKE ', 517),\n",
       " ('so', 518),\n",
       " ('tt', 519),\n",
       " (' as ', 520),\n",
       " ('fore ', 521),\n",
       " ('ick', 522),\n",
       " ('tru', 523),\n",
       " ('ess', 524),\n",
       " ('had ', 525),\n",
       " ('With', 526),\n",
       " ('ble ', 527),\n",
       " ('pp', 528),\n",
       " ('od', 529),\n",
       " ('ENTIO:\\n', 530),\n",
       " ('tim', 531),\n",
       " ('ER:\\n', 532),\n",
       " (' her', 533),\n",
       " ('Which', 534),\n",
       " ('I s', 535),\n",
       " ('more ', 536),\n",
       " ('es, ', 537),\n",
       " ('they ', 538),\n",
       " ('shall', 539),\n",
       " ('ese ', 540),\n",
       " ('ex', 541),\n",
       " ('will ', 542),\n",
       " ('father', 543),\n",
       " ('Y:\\n', 544),\n",
       " ('do', 545),\n",
       " ('; and ', 546),\n",
       " ('then', 547),\n",
       " ('were ', 548),\n",
       " (' all', 549),\n",
       " ('O, ', 550),\n",
       " ('INC', 551),\n",
       " ('ow ', 552),\n",
       " ('He ', 553),\n",
       " ('bl', 554),\n",
       " (\"e's \", 555),\n",
       " ('lor', 556),\n",
       " (' me ', 557),\n",
       " ('bro', 558),\n",
       " ('ose ', 559),\n",
       " ('ign', 560),\n",
       " ('ry ', 561),\n",
       " ('to the ', 562),\n",
       " ('e?\\n', 563),\n",
       " ('ugh', 564),\n",
       " ('Of', 565),\n",
       " ('ep', 566),\n",
       " ('very ', 567),\n",
       " ('y,\\n', 568),\n",
       " ('in the ', 569),\n",
       " ('ence', 570),\n",
       " ('AB', 571),\n",
       " ('will', 572),\n",
       " ('t,\\n', 573),\n",
       " ('fri', 574),\n",
       " ('death', 575),\n",
       " ('ence ', 576),\n",
       " ('ind', 577),\n",
       " (' a ', 578),\n",
       " ('par', 579),\n",
       " ('ight ', 580),\n",
       " ('--\\n', 581),\n",
       " ('any ', 582),\n",
       " ('look', 583),\n",
       " ('LO', 584),\n",
       " ('aw', 585),\n",
       " ('make ', 586),\n",
       " (\"I'll \", 587),\n",
       " ('KING RICH', 588),\n",
       " ('KING RICHARD I', 589),\n",
       " ('EST', 590),\n",
       " ('First ', 591),\n",
       " ('ther ', 592),\n",
       " ('new', 593),\n",
       " (' what ', 594),\n",
       " ('their', 595),\n",
       " ('UCH', 596),\n",
       " ('iv', 597),\n",
       " ('ome', 598),\n",
       " ('ce, ', 599),\n",
       " ('would ', 600),\n",
       " ('gra', 601),\n",
       " ('s and ', 602),\n",
       " ('GLO', 603),\n",
       " ('arm', 604),\n",
       " ('UCEST', 605),\n",
       " ('GLOUCEST', 606),\n",
       " ('GLOUCESTER:\\n', 607),\n",
       " ('vo', 608),\n",
       " ('yet ', 609),\n",
       " ('fair', 610),\n",
       " ('EEN', 611),\n",
       " ('rown', 612),\n",
       " ('This ', 613),\n",
       " ('thee ', 614),\n",
       " (' their', 615),\n",
       " (' so ', 616),\n",
       " ('Thou', 617),\n",
       " ('which', 618),\n",
       " ('LI', 619),\n",
       " ('ure ', 620),\n",
       " ('mad', 621),\n",
       " ('yself', 622),\n",
       " (' we ', 623),\n",
       " ('she ', 624),\n",
       " ('ef', 625),\n",
       " ('onour', 626),\n",
       " ('ster', 627),\n",
       " ('ev', 628),\n",
       " (', my ', 629),\n",
       " ('liv', 630),\n",
       " ('ell ', 631),\n",
       " ('eet ', 632),\n",
       " ('love ', 633),\n",
       " ('ect', 634),\n",
       " ('ind ', 635),\n",
       " (' them', 636),\n",
       " ('of the ', 637),\n",
       " ('QU', 638),\n",
       " ('VINC', 639),\n",
       " ('than', 640),\n",
       " ('did ', 641),\n",
       " ('QUEEN', 642),\n",
       " ('VINCENTIO:\\n', 643),\n",
       " ('Mar', 644),\n",
       " ('shall ', 645),\n",
       " ('Com', 646),\n",
       " ('F ', 647),\n",
       " ('OF ', 648),\n",
       " ('off', 649),\n",
       " ('urn', 650),\n",
       " (' sp', 651),\n",
       " ('ge', 652),\n",
       " ('thing', 653),\n",
       " ('with ', 654),\n",
       " ('Go', 655),\n",
       " (' for', 656),\n",
       " (' me', 657),\n",
       " ('HEN', 658),\n",
       " ('een', 659),\n",
       " ('ts ', 660),\n",
       " ('vi', 661),\n",
       " ('pla', 662),\n",
       " ('ak', 663),\n",
       " ('go', 664),\n",
       " (\"o'\", 665),\n",
       " ('use ', 666),\n",
       " ('again', 667),\n",
       " ('sir, ', 668),\n",
       " ('give ', 669),\n",
       " ('RY ', 670),\n",
       " ('So ', 671),\n",
       " ('all ', 672),\n",
       " ('es\\n', 673),\n",
       " ('come ', 674),\n",
       " ('HENRY ', 675),\n",
       " ('ES', 676),\n",
       " ('ec', 677),\n",
       " ('mar', 678),\n",
       " (\"'tis \", 679),\n",
       " ('sed ', 680),\n",
       " ('ght ', 681),\n",
       " ('ge ', 682),\n",
       " ('take ', 683),\n",
       " ('ore ', 684),\n",
       " (' hear', 685),\n",
       " ('Se', 686),\n",
       " ('entle', 687),\n",
       " ('dis', 688),\n",
       " ('ng', 689),\n",
       " ('ORI', 690),\n",
       " ('es,\\n', 691),\n",
       " ('ber', 692),\n",
       " ('du', 693),\n",
       " ('her ', 694),\n",
       " ('other', 695),\n",
       " ('DUKE VINCENTIO:\\n', 696),\n",
       " (' is ', 697),\n",
       " ('QUEEN ', 698),\n",
       " ('ES:\\n', 699),\n",
       " ('jo', 700),\n",
       " ('Is ', 701),\n",
       " ('ower', 702),\n",
       " ('YOR', 703),\n",
       " ('ds ', 704),\n",
       " (\"e'\", 705),\n",
       " ('men', 706),\n",
       " ('YORK:\\n', 707),\n",
       " ('ap', 708),\n",
       " ('coun', 709),\n",
       " ('art ', 710),\n",
       " ('sin', 711),\n",
       " ('ust', 712),\n",
       " ('ate ', 713),\n",
       " ('Or', 714),\n",
       " ('We ', 715),\n",
       " ('ation', 716),\n",
       " (' it ', 717),\n",
       " ('des', 718),\n",
       " ('fi', 719),\n",
       " ('INIUS:\\n', 720),\n",
       " (' com', 721),\n",
       " ('speak', 722),\n",
       " (' not ', 723),\n",
       " ('pra', 724),\n",
       " ('tw', 725),\n",
       " ('ted ', 726),\n",
       " ('PET', 727),\n",
       " ('iz', 728),\n",
       " ('orn', 729),\n",
       " (' than', 730),\n",
       " ('may ', 731),\n",
       " ('you, ', 732),\n",
       " ('ight', 733),\n",
       " (\"'T\", 734),\n",
       " ('DW', 735),\n",
       " ('EDW', 736),\n",
       " ('It ', 737),\n",
       " ('ac', 738),\n",
       " ('much', 739),\n",
       " ('GR', 740),\n",
       " ('augh', 741),\n",
       " ('land', 742),\n",
       " ('TH', 743),\n",
       " ('ost ', 744),\n",
       " ('Why, ', 745),\n",
       " ('should ', 746),\n",
       " ('reat ', 747),\n",
       " ('e;\\n', 748),\n",
       " ('ook', 749),\n",
       " ('MI', 750),\n",
       " ('UM', 751),\n",
       " ('e:\\n', 752),\n",
       " ('gain', 753),\n",
       " ('when', 754),\n",
       " ('LE', 755),\n",
       " ('LUC', 756),\n",
       " ('MEN', 757),\n",
       " ('bo', 758),\n",
       " ('ter ', 759),\n",
       " ('I m', 760),\n",
       " (' I ', 761),\n",
       " ('IS', 762),\n",
       " ('fear', 763),\n",
       " ('long', 764),\n",
       " (' hea', 765),\n",
       " ('most ', 766),\n",
       " ('some ', 767),\n",
       " ('er:\\n', 768),\n",
       " ('ELL', 769),\n",
       " ('Now', 770),\n",
       " ('Ed', 771),\n",
       " ('ant ', 772),\n",
       " ('cond ', 773),\n",
       " (\"'st \", 774),\n",
       " ('think', 775),\n",
       " ('ord ', 776),\n",
       " ('noble ', 777),\n",
       " ('ME', 778),\n",
       " ('ack', 779),\n",
       " ('less ', 780),\n",
       " ('hat, ', 781),\n",
       " ('go ', 782),\n",
       " ('how', 783),\n",
       " ('ROME', 784),\n",
       " ('ROMEO:\\n', 785),\n",
       " ('s:\\n', 786),\n",
       " ('ENIUS:\\n', 787),\n",
       " ('MENENIUS:\\n', 788),\n",
       " ('By ', 789),\n",
       " ('dy ', 790),\n",
       " ('here ', 791),\n",
       " (' man', 792),\n",
       " ('e, and ', 793),\n",
       " ('must ', 794),\n",
       " ('pres', 795),\n",
       " ('RUCH', 796),\n",
       " ('af', 797),\n",
       " ('ither', 798),\n",
       " ('rem', 799),\n",
       " ('PETRUCH', 800),\n",
       " ('PETRUCHIO:\\n', 801),\n",
       " ('OM', 802),\n",
       " ('nee', 803),\n",
       " ('tell', 804),\n",
       " (' by ', 805),\n",
       " ('rep', 806),\n",
       " ('How', 807),\n",
       " ('ip', 808),\n",
       " ('ot ', 809),\n",
       " ('mon', 810),\n",
       " ('ment', 811),\n",
       " ('ard ', 812),\n",
       " ('ed\\n', 813),\n",
       " ('God ', 814),\n",
       " ('gu', 815),\n",
       " ('lord, ', 816),\n",
       " ('Ay, ', 817),\n",
       " ('Cl', 818),\n",
       " ('HA', 819),\n",
       " ('MAR', 820),\n",
       " ('ONT', 821),\n",
       " ('Come, ', 822),\n",
       " (' on', 823),\n",
       " ('oll', 824),\n",
       " ('and, ', 825),\n",
       " ('ret', 826),\n",
       " (' from', 827),\n",
       " ('CORI', 828),\n",
       " ('s of', 829),\n",
       " ('Where ', 830),\n",
       " ('OLAN', 831),\n",
       " ('friend', 832),\n",
       " ('CORIOLAN', 833),\n",
       " (' at ', 834),\n",
       " ('not', 835),\n",
       " (' shall ', 836),\n",
       " ('cannot ', 837),\n",
       " ('Second ', 838),\n",
       " ('CORIOLANUS:\\n', 839),\n",
       " ('day', 840),\n",
       " ('rong', 841),\n",
       " ('sc', 842),\n",
       " ('Be', 843),\n",
       " ('Rome', 844),\n",
       " ('fl', 845),\n",
       " ('s;\\n', 846),\n",
       " ('that', 847),\n",
       " ('s to ', 848),\n",
       " ('BA', 849),\n",
       " ('Rich', 850),\n",
       " ('na', 851),\n",
       " ('wick', 852),\n",
       " ('are', 853),\n",
       " ('eal', 854),\n",
       " ('queen', 855),\n",
       " ('pe', 856),\n",
       " ('say ', 857),\n",
       " ('ten', 858),\n",
       " ('ness ', 859),\n",
       " ('ren', 860),\n",
       " ('atch', 861),\n",
       " ('urse', 862),\n",
       " ('though', 863),\n",
       " (' ha', 864),\n",
       " ('is the ', 865),\n",
       " ('itiz', 866),\n",
       " ('ound ', 867),\n",
       " ('too ', 868),\n",
       " ('itizen', 869),\n",
       " ('it.\\n', 870),\n",
       " ('INA:\\n', 871),\n",
       " ('rough', 872),\n",
       " (' she ', 873),\n",
       " (' will ', 874),\n",
       " (' her ', 875),\n",
       " ('D:\\n', 876),\n",
       " ('Lord ', 877),\n",
       " ('bear', 878),\n",
       " (' con', 879),\n",
       " ('II:\\n', 880),\n",
       " ('ast', 881),\n",
       " ('io', 882),\n",
       " ('thee', 883),\n",
       " ('KING RICHARD III:\\n', 884),\n",
       " ('Sir', 885),\n",
       " ('see ', 886),\n",
       " ('ise ', 887),\n",
       " (' they ', 888),\n",
       " ('ble', 889),\n",
       " ('don', 890),\n",
       " (' gra', 891),\n",
       " ('es.\\n', 892),\n",
       " (' such', 893),\n",
       " ('to be ', 894),\n",
       " ('cont', 895),\n",
       " (\"I'll\", 896),\n",
       " ('War', 897),\n",
       " ('Your', 898),\n",
       " ('You ', 899),\n",
       " ('plea', 900),\n",
       " ('ty ', 901),\n",
       " ('I do ', 902),\n",
       " (' you ', 903),\n",
       " ('Warwick', 904),\n",
       " ('Un', 905),\n",
       " ('pre', 906),\n",
       " ('man:\\n', 907),\n",
       " ('On', 908),\n",
       " ('doth', 909),\n",
       " ('over', 910),\n",
       " ('lie', 911),\n",
       " ('s, and ', 912),\n",
       " ('AD', 913),\n",
       " ('His ', 914),\n",
       " ('re ', 915),\n",
       " ('ure', 916),\n",
       " ('s of ', 917),\n",
       " ('ison', 918),\n",
       " ('When', 919),\n",
       " (' to', 920),\n",
       " ('ongu', 921),\n",
       " ('OF YORK:\\n', 922),\n",
       " ('brea', 923),\n",
       " ('tun', 924),\n",
       " ('tain', 925),\n",
       " ('uck', 926),\n",
       " ('oman', 927),\n",
       " ('I will ', 928),\n",
       " ('ABELL', 929),\n",
       " ('ISABELL', 930),\n",
       " ('ISABELLA:\\n', 931),\n",
       " ('ce.\\n', 932),\n",
       " (' thee ', 933),\n",
       " ('RAN', 934),\n",
       " ('Who ', 935),\n",
       " ('St', 936),\n",
       " ('d:\\n', 937),\n",
       " ('e!\\n', 938),\n",
       " ('ss', 939),\n",
       " ('there ', 940),\n",
       " ('ark', 941),\n",
       " (' will', 942),\n",
       " ('chil', 943),\n",
       " ('I know', 944),\n",
       " ('ity ', 945),\n",
       " (\"'Tis \", 946),\n",
       " ('JU', 947),\n",
       " ('Let ', 948),\n",
       " ('Sh', 949),\n",
       " ('ew', 950),\n",
       " ('ing, ', 951),\n",
       " ('not, ', 952),\n",
       " ('Than', 953),\n",
       " ('And, ', 954),\n",
       " ('call', 955),\n",
       " (' should ', 956),\n",
       " ('LIET:\\n', 957),\n",
       " ('LEONT', 958),\n",
       " ('JULIET:\\n', 959),\n",
       " ('LEONTES:\\n', 960),\n",
       " ('a p', 961),\n",
       " ('Then', 962),\n",
       " ('nor', 963),\n",
       " ('and\\n', 964),\n",
       " ('LAD', 965),\n",
       " ('ath', 966),\n",
       " ('a b', 967),\n",
       " ('cl', 968),\n",
       " ('wif', 969),\n",
       " ('hav', 970),\n",
       " ('urder', 971),\n",
       " ('it is ', 972),\n",
       " ('LADY ', 973),\n",
       " ('CE:\\n', 974),\n",
       " ('Nor', 975),\n",
       " ('VOL', 976),\n",
       " ('hou', 977),\n",
       " ('ry', 978),\n",
       " ('ut, ', 979),\n",
       " (' these ', 980),\n",
       " ('ENCE:\\n', 981),\n",
       " ('ce,\\n', 982),\n",
       " (' shall', 983),\n",
       " ('ward ', 984),\n",
       " ('brother', 985),\n",
       " ('BR', 986),\n",
       " ('ed, ', 987),\n",
       " ('ever', 988),\n",
       " ('s?\\n', 989),\n",
       " ('tal', 990),\n",
       " ('such', 991),\n",
       " ('Citizen', 992),\n",
       " ('Serv', 993),\n",
       " ('down', 994),\n",
       " (' can', 995),\n",
       " (' your ', 996),\n",
       " ('made ', 997),\n",
       " ('Pro', 998),\n",
       " ('ork', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(tokenizer.get_vocab().items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "a132be57-9709-40ce-a686-aa0689560036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\x00': 0,\n",
       " '\\x01': 1,\n",
       " '\\x02': 2,\n",
       " '\\x03': 3,\n",
       " '\\x04': 4,\n",
       " '\\x05': 5,\n",
       " '\\x06': 6,\n",
       " '\\x07': 7,\n",
       " '\\x08': 8,\n",
       " '\\t': 9,\n",
       " '\\n': 10,\n",
       " '\\x0b': 11,\n",
       " '\\x0c': 12,\n",
       " '\\r': 13,\n",
       " '\\x0e': 14,\n",
       " '\\x0f': 15,\n",
       " '\\x10': 16,\n",
       " '\\x11': 17,\n",
       " '\\x12': 18,\n",
       " '\\x13': 19,\n",
       " '\\x14': 20,\n",
       " '\\x15': 21,\n",
       " '\\x16': 22,\n",
       " '\\x17': 23,\n",
       " '\\x18': 24,\n",
       " '\\x19': 25,\n",
       " '\\x1a': 26,\n",
       " '\\x1b': 27,\n",
       " '\\x1c': 28,\n",
       " '\\x1d': 29,\n",
       " '\\x1e': 30,\n",
       " '\\x1f': 31,\n",
       " ' ': 32,\n",
       " '!': 33,\n",
       " '\"': 34,\n",
       " '#': 35,\n",
       " '$': 36,\n",
       " '%': 37,\n",
       " '&': 38,\n",
       " \"'\": 39,\n",
       " '(': 40,\n",
       " ')': 41,\n",
       " '*': 42,\n",
       " '+': 43,\n",
       " ',': 44,\n",
       " '-': 45,\n",
       " '.': 46,\n",
       " '/': 47,\n",
       " '0': 48,\n",
       " '1': 49,\n",
       " '2': 50,\n",
       " '3': 51,\n",
       " '4': 52,\n",
       " '5': 53,\n",
       " '6': 54,\n",
       " '7': 55,\n",
       " '8': 56,\n",
       " '9': 57,\n",
       " ':': 58,\n",
       " ';': 59,\n",
       " '<': 60,\n",
       " '=': 61,\n",
       " '>': 62,\n",
       " '?': 63,\n",
       " '@': 64,\n",
       " 'A': 65,\n",
       " 'B': 66,\n",
       " 'C': 67,\n",
       " 'D': 68,\n",
       " 'E': 69,\n",
       " 'F': 70,\n",
       " 'G': 71,\n",
       " 'H': 72,\n",
       " 'I': 73,\n",
       " 'J': 74,\n",
       " 'K': 75,\n",
       " 'L': 76,\n",
       " 'M': 77,\n",
       " 'N': 78,\n",
       " 'O': 79,\n",
       " 'P': 80,\n",
       " 'Q': 81,\n",
       " 'R': 82,\n",
       " 'S': 83,\n",
       " 'T': 84,\n",
       " 'U': 85,\n",
       " 'V': 86,\n",
       " 'W': 87,\n",
       " 'X': 88,\n",
       " 'Y': 89,\n",
       " 'Z': 90,\n",
       " '[': 91,\n",
       " '\\\\': 92,\n",
       " ']': 93,\n",
       " '^': 94,\n",
       " '_': 95,\n",
       " '`': 96,\n",
       " 'a': 97,\n",
       " 'b': 98,\n",
       " 'c': 99,\n",
       " 'd': 100,\n",
       " 'e': 101,\n",
       " 'f': 102,\n",
       " 'g': 103,\n",
       " 'h': 104,\n",
       " 'i': 105,\n",
       " 'j': 106,\n",
       " 'k': 107,\n",
       " 'l': 108,\n",
       " 'm': 109,\n",
       " 'n': 110,\n",
       " 'o': 111,\n",
       " 'p': 112,\n",
       " 'q': 113,\n",
       " 'r': 114,\n",
       " 's': 115,\n",
       " 't': 116,\n",
       " 'u': 117,\n",
       " 'v': 118,\n",
       " 'w': 119,\n",
       " 'x': 120,\n",
       " 'y': 121,\n",
       " 'z': 122,\n",
       " '{': 123,\n",
       " '|': 124,\n",
       " '}': 125,\n",
       " '~': 126,\n",
       " '\\x7f': 127,\n",
       " 'e ': 128,\n",
       " 'th': 129,\n",
       " 't ': 130,\n",
       " 's ': 131,\n",
       " 'd ': 132,\n",
       " ', ': 133,\n",
       " 'ou': 134,\n",
       " 'er': 135,\n",
       " 'in': 136,\n",
       " 'y ': 137,\n",
       " 'an': 138,\n",
       " ':\\n': 139,\n",
       " 'or': 140,\n",
       " 'o ': 141,\n",
       " 'en': 142,\n",
       " '\\n\\n': 143,\n",
       " 'ar': 144,\n",
       " ' th': 145,\n",
       " 'on': 146,\n",
       " 'll': 147,\n",
       " 'ha': 148,\n",
       " ',\\n': 149,\n",
       " '.\\n\\n': 150,\n",
       " 'is ': 151,\n",
       " 'es': 152,\n",
       " 'you': 153,\n",
       " ' s': 154,\n",
       " 'to ': 155,\n",
       " 'and ': 156,\n",
       " 'ow': 157,\n",
       " 'ea': 158,\n",
       " ' m': 159,\n",
       " ' w': 160,\n",
       " 'of': 161,\n",
       " ' h': 162,\n",
       " 'ing': 163,\n",
       " 'om': 164,\n",
       " ' a': 165,\n",
       " 'ch': 166,\n",
       " 'the ': 167,\n",
       " 'st': 168,\n",
       " ' b': 169,\n",
       " 'no': 170,\n",
       " 'ir': 171,\n",
       " 'for': 172,\n",
       " 've ': 173,\n",
       " 'e, ': 174,\n",
       " 'ith': 175,\n",
       " ' the ': 176,\n",
       " 'se': 177,\n",
       " 'li': 178,\n",
       " 'Th': 179,\n",
       " 'll ': 180,\n",
       " 're': 181,\n",
       " 'st ': 182,\n",
       " 'at ': 183,\n",
       " 'An': 184,\n",
       " 'I ': 185,\n",
       " 'ear': 186,\n",
       " 'im': 187,\n",
       " 'it': 188,\n",
       " 'oo': 189,\n",
       " 'gh': 190,\n",
       " 'at': 191,\n",
       " 'is': 192,\n",
       " 'le': 193,\n",
       " 'er ': 194,\n",
       " 'our': 195,\n",
       " 'And ': 196,\n",
       " \"'s \": 197,\n",
       " 'ee': 198,\n",
       " 'not ': 199,\n",
       " 'my ': 200,\n",
       " ';\\n': 201,\n",
       " 'ra': 202,\n",
       " '.\\n': 203,\n",
       " 'your': 204,\n",
       " 'ur': 205,\n",
       " 'hat ': 206,\n",
       " 'ri': 207,\n",
       " 'ut ': 208,\n",
       " 'ld ': 209,\n",
       " 'of ': 210,\n",
       " 'O:\\n': 211,\n",
       " 'ed ': 212,\n",
       " 'la': 213,\n",
       " 'it ': 214,\n",
       " 'ro': 215,\n",
       " 'ere ': 216,\n",
       " 'es ': 217,\n",
       " 'd, ': 218,\n",
       " 'un': 219,\n",
       " 'EN': 220,\n",
       " 'ke ': 221,\n",
       " 'y, ': 222,\n",
       " 'IN': 223,\n",
       " ' d': 224,\n",
       " '?\\n\\n': 225,\n",
       " 'as ': 226,\n",
       " 'fa': 227,\n",
       " 'with': 228,\n",
       " 'have ': 229,\n",
       " 'S:\\n': 230,\n",
       " ' c': 231,\n",
       " 'Wh': 232,\n",
       " 'that ': 233,\n",
       " 'ent': 234,\n",
       " 'the': 235,\n",
       " 'ce': 236,\n",
       " 'sh': 237,\n",
       " 'ma': 238,\n",
       " ' p': 239,\n",
       " 'ther': 240,\n",
       " 'be': 241,\n",
       " '. ': 242,\n",
       " 'AR': 243,\n",
       " 'ce ': 244,\n",
       " 'ing ': 245,\n",
       " 'al': 246,\n",
       " '; ': 247,\n",
       " 'thou': 248,\n",
       " 's, ': 249,\n",
       " 'me ': 250,\n",
       " 'se ': 251,\n",
       " 'lo': 252,\n",
       " 'ck': 253,\n",
       " 'wh': 254,\n",
       " 'il': 255}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpe.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0ffca083-d761-4409-ac35-244269921d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
