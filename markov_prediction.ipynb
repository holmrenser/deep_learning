{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c54284-4f94-45a2-a4c2-5efffa394aed",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/holmrenser/deep_learning/blob/main/markov_prediction.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "# Markov prediction\n",
    "In this notebook we will implement (arguably) the most simple form of sequence generation: each subsequent token (character) is generated by sampling from the conditional probability distribution $P(X_t \\mid X_{t-1}) \\sim Multinomial_{X_{t-1}}(N_{tokens})$, i.e. every character (token) has an associated probility table for what the next character (token) will be. 'Training' proceeds through simple counting of observed character pairs.\n",
    "\n",
    "We start with a very minimal implementation and then proceed to introduce some boilerplate for dataloading, model forwarding, and training. Whereas this is not strictly necesarry for this model, it makes comparisons with later more complex models more straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19228e5-abe9-4040-884b-2d852997e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import math\n",
    "from typing import Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eef47-cf85-4121-91f6-90310247f5ae",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We use the tiny shakespeare dataset to train a character level markov model to predict text that looks very little like shakespeare. All data is in one text file, which we download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a233403-af6a-425b-9e19-ebfd6db3a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-15 16:13:28--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1,1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1,06M  --.-KB/s    in 0,1s    \n",
      "\n",
      "2024-02-15 16:13:29 (8,29 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de2039-f92e-4b59-b01d-a1264fc4b8a8",
   "metadata": {},
   "source": [
    "# Minimal implementation\n",
    "Below is a very minimal implementation of generating a text sample using 'markov prediction': every next token is sampled according to a probability based only on the previous token. 'Training' consists of counting observed character pairs. We don't calculate a loss and model evaluation is based on vibes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "0a14bae4-4ecf-4f7f-b044-e3f873b55237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b599d350a0ba40298ba829ba59c0cc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample:\n",
      "---\n",
      "\n",
      "FScomen cat dwofr tee3F\n",
      "NCcaURXugriu CP\n",
      "Au, ongs'Voullobewo'lVGDRen t?kefoLXs usm\n",
      "POLkey be Trynghea thet wA;\n",
      "MS: Ves!fmathobbze, ;Fiy yo ithod athy was ous. l r BZzlonsppof a tade othe iruhkn bPged Ins:B-Fint,\n",
      "E.vYXWikngOIman:'PrldEN'toMs, hssWhifende ug\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x28d35cf50>"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('input.txt') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "# create a sorted list of unique characters\n",
    "chars = sorted(list(set(data)))\n",
    "# count how many 'tokens'\n",
    "vocab_size = len(chars)\n",
    "# map characters to integers\n",
    "encode = { ch:i for i,ch in enumerate(chars) }\n",
    "# map integers to characters\n",
    "decode = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# initialize counts with ones for automatic laplace/additive smoothing: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "counts = torch.ones(vocab_size, vocab_size)\n",
    "\n",
    "# 'train' (i.e. count) for n steps (i.e. character pairs)\n",
    "n_train_chars = 10000\n",
    "for i in trange(n_train_chars):\n",
    "    token_i = encode[data[i]]\n",
    "    token_j = encode[data[i+1]]\n",
    "    counts[token_i][token_j] += 1\n",
    "\n",
    "# divide by row sums to get transition probabilities\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "# start our sample with token 0 (i.e. the newline character \\n)\n",
    "sample = [0]\n",
    "sample_n_tokens = 256\n",
    "\n",
    "# for n steps, sample a token from the distribution belonging the last token in the current sample and add it to the sample\n",
    "for _ in range(sample_n_tokens):\n",
    "    next_token = torch.multinomial(probs[sample[-1]], num_samples=1)\n",
    "    sample.append(next_token.item())\n",
    "\n",
    "# decode sample tokens into characters\n",
    "decoded_sample = ''.join(decode[token] for token in sample)\n",
    "print(f'Sample:\\n---{decoded_sample}\\n---')\n",
    "\n",
    "# plot observed transition probabilities\n",
    "plt.imshow(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d9317129-ccbc-4511-ab69-3657259b7c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:100],data[100]\n",
    "idx = 99\n",
    "order = 4\n",
    "counts = torch.ones([vocab_size] * (order + 1))\n",
    "x = data[idx-order:idx]\n",
    "y = data[idx]\n",
    "x_tokens = [encode[c] for c in x]\n",
    "y_tokens = [encode[c] for c in y]\n",
    "x_tokens,y_tokens\n",
    "counts[*x_tokens, *y_tokens] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "990b227c-483b-4363-8fe2-f6cc4db1ad3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[*x_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "5953eebd-8a9a-4808-b320-f996fafebdcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 100]), torch.Size([65, 65, 65, 65]), range(3, 100))"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.shape,counts.shape, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "15c823ad-3c62-418a-a355-902a05636e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 29 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[459], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(order, tokens\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      8\u001b[0m x_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([torch\u001b[38;5;241m.\u001b[39marange(idx \u001b[38;5;241m-\u001b[39m order, idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])\n\u001b[0;32m----> 9\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_indices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m y \u001b[38;5;241m=\u001b[39m tokens[indices]\n\u001b[1;32m     11\u001b[0m x_indices\n",
      "\u001b[0;31mIndexError\u001b[0m: index 29 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "tokens = torch.tensor([\n",
    "    [encode[c] for c in data[:100]],\n",
    "    [encode[c] for c in data[200:300]]\n",
    "])\n",
    "order = 3\n",
    "counts = torch.ones([vocab_size] * (order + 1))\n",
    "indices = range(order, tokens.shape[1])\n",
    "x_indices = torch.vstack([torch.arange(idx - order, idx) for idx in indices])\n",
    "x = tokens[x_indices]\n",
    "y = tokens[indices]\n",
    "x_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "ceb78672-f9dc-49e1-806a-99a44c2f7b51",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "shape mismatch: indexing tensors could not be broadcast together with shapes [2], [97, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[467], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_indices\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: shape mismatch: indexing tensors could not be broadcast together with shapes [2], [97, 3]"
     ]
    }
   ],
   "source": [
    "tokens[torch.arange(tokens.shape[0]), x_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "1a820dd2-462a-44f7-bcd4-3494fbf78552",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([t[x_indices][None] for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "248c2beb-f042-4c51-a5c0-e30c9cd311d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[485], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 4"
     ]
    }
   ],
   "source": [
    "counts[*x[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "6cc6eb6e-3966-4050-b9c4-49ac79b01478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([97])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[*x[0].T,y].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "8a69f88b-3c2f-4eaa-b819-0faea2aa0137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1., 11.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1., 11.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., 11.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [11.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., 11.,  1.,  1.,\n",
       "          1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[*x.T, y] += 10\n",
    "probs = counts / counts.sum(dim=-1)\n",
    "counts[*x.T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "86663489-c654-4b74-b945-db7eb6e950de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([75., 75., 75., 75., 75.])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts[*x.T].sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "ea4ff9e4-2da5-4ca0-8176-104332eb3214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65, 65, 65])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "2e86a674-026d-401c-a714-3e56ead82ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = counts / counts.sum(dim=-1, keepdim=True)\n",
    "probs[*x.T].sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbced26-0fda-40b2-b931-cf6e265dec0b",
   "metadata": {},
   "source": [
    "# Advanced implementation\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "1ab1af0c-d783-4ff5-8ad1-5cbb2b547732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892059, 222823)"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CharacterTokenizer:\n",
    "    def __init__(self, chars: list[str]):\n",
    "        self.decode_dict: dict[int, str] = dict(enumerate(chars))\n",
    "        self.encode_dict: dict[str, int] = {v:k for k,v in self.decode_dict.items()}\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        return [self.encode_dict[char] for char in input]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        return ''.join(self.decode_dict[token] for token in tokens)\n",
    "\n",
    "class CharacterDataset:\n",
    "    def __init__(self, data: str, tokenizer: CharacterTokenizer, block_size: int=256):\n",
    "        self.chars = sorted(list(set(data)))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.block_size = block_size\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.chars)\n",
    "\n",
    "    @classmethod\n",
    "    def from_textfile(cls, filename: str, block_size: int=256) -> 'CharacterDataset':\n",
    "        with open(filename, 'r') as fh:\n",
    "            data = fh.read()\n",
    "            chars = sorted(set(data))\n",
    "            tokenizer = CharacterTokenizer(chars)\n",
    "            return cls(data, tokenizer, block_size=block_size)\n",
    "\n",
    "    def train_test_split(self, train_percentage: float=0.8) -> Tuple['CharacterDataset','CharacterDataset']:\n",
    "        n_train_chars = int(train_percentage * len(self.data))\n",
    "\n",
    "        train_data = self.data[:n_train_chars]\n",
    "        train_dataset = CharacterDataset(train_data, self.tokenizer, self.block_size)\n",
    "\n",
    "        test_data = self.data[n_train_chars:]\n",
    "        test_dataset = CharacterDataset(test_data, self.tokenizer, self.block_size)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.tensor:\n",
    "        # grab a chunk of block_size characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size]\n",
    "        # encode every character to an integer\n",
    "        tokens = self.tokenizer.encode(chunk)\n",
    "        # return as tensors\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "dataset = CharacterDataset.from_textfile('./input.txt')\n",
    "train_dataset,test_dataset = dataset.train_test_split()\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "0c198432-03c6-4382-bac4-4fed9847d601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
       "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
       "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
       "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
       "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
       "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
       "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
       "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
       "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
       "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
       "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
       "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
       "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
       "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
       "        50, 50, 10,  0])"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdf3cf-e33f-4456-b714-be5f149a4868",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "44183014-3ccf-4d69-93df-1dc823475704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovPredictor(nn.Module):\n",
    "    \"\"\"Very simple model for next character prediction by counting observed character pairs\"\"\"\n",
    "    def __init__(self, vocab_size, order=1):\n",
    "        super().__init__()\n",
    "        # counting character pairs in a vocab_size ** (order + 1) table\n",
    "        self.counts = torch.ones([vocab_size] * (order + 1))\n",
    "\n",
    "    @property\n",
    "    def probs(self):\n",
    "        \"\"\"Normalize counts to probabilities by dividing over row sums\"\"\"\n",
    "        return self.counts / self.counts.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None):\n",
    "        probs = self.probs[idx]\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(probs.view(-1, probs.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return probs,loss\n",
    "\n",
    "    def generate(self, n_samples: int=1, sample_length: int=256, do_sample: bool=True):\n",
    "        \"\"\"Generate samples\"\"\"\n",
    "        idx = torch.zeros((n_samples, 1), dtype=torch.long)\n",
    "\n",
    "        for _ in trange(sample_length):\n",
    "            probs,_ = self(idx[:,-1])\n",
    "\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        samples = []\n",
    "        for sample in idx:\n",
    "            samples.append(dataset.tokenizer.decode(sample.tolist()))\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "993facb9-ff87-4271-ada9-caa7399d8196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8120faa0910f4267a47b33ca8b5d4889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "JOA&uH\n",
      "YCeFJkvWNSisKTg\n",
      "KXOEo;yCtcvO-BlJb.WevSN3gST?$gNXf-:EnW3F'mKKSr,3a3J.eDsIi,v$bHo,HAfo3nrRSzTqnbU U M-q-wvN!yW:YHiLKJ:zpAOo;DA-rvoA\n",
      "SHLF;hTkGuFTAGfg;yDz;oGZHgEwEfcP?KUJr-mj JBHz-QROLS.-ja,.'kJFSuh&lSde3YxQJcBNxCP-upmxDae&t.,'C:kNdalDVLnVnEk-KpRQei:g&V\n"
     ]
    }
   ],
   "source": [
    "model = MarkovPredictor(dataset.vocab_size)\n",
    "for sample in model.generate():\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "81b3a5c4-c52a-4fea-a088-faccaa3e4b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 255, 65])"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = next(iter(train_dataloader))\n",
    "x = seq[:,:-1]\n",
    "y = seq[:,1:]\n",
    "probs = model.probs[x]\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f101c2d-9d1e-40ea-9d8d-7bb0805a1eb0",
   "metadata": {},
   "source": [
    "## Training\n",
    "Note that whereas we can call this 'training', all the code below does is count character pairs and update the pair count table of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "3257a0d9-cecc-40d3-bc0e-1e6559645c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed58c4de45f4da99a32734128aa195b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[388], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m y \u001b[38;5;241m=\u001b[39m seq[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 25\u001b[0m     _,loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     27\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([model(seq[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],seq[\u001b[38;5;241m1\u001b[39m:])[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m test_dataloader])\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/transformers/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[350], line 18\u001b[0m, in \u001b[0;36mMarkovPredictor.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(probs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, probs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m probs,loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "dataset = CharacterDataset.from_textfile('./input.txt', block_size=256)\n",
    "train_dataset,test_dataset = dataset.train_test_split()\n",
    "model = MarkovPredictor(dataset.vocab_size)\n",
    "\n",
    "batch_size = 50\n",
    "train_steps = 200\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset, replacement=True, num_samples=train_steps * batch_size),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=RandomSampler(test_dataset, replacement=True, num_samples=20),\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "for i,seq in enumerate(tqdm(train_dataloader)):\n",
    "    # Calculate losses every 5 steps\n",
    "    x = seq[:,:-1]\n",
    "    y = seq[:,1:]\n",
    "    if i % 5 == 0:\n",
    "        _,loss = model(x,y)\n",
    "        train_losses.append(loss.item())\n",
    "        test_loss = torch.tensor([model(seq[:-1],seq[1:])[1] for seq in test_dataloader])\n",
    "        test_losses.append(torch.mean(test_loss).item())\n",
    "    # Count letter order in training data\n",
    "    for batch in zip(x,y):\n",
    "        for x_i,y_i in zip(*batch):\n",
    "            model.counts[x_i][y_i] += 1\n",
    "\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(test_losses, label='test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255bbf36-c57a-4e7a-ad6f-82f5694149bd",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The model has learned some structure, e.g. there are probably a bit more newlines and a colon is always followed by a newline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "3e6b1661-f2c4-48bf-b432-6103a2a7e73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cde90bcc345495a8f5b1a0248239431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iRoys dns h medpAWIdonaoe aieN\n",
      "rpo\n",
      "sAu ls ;\n",
      "oto ohmtmtowsuo\n",
      "iieaetaaws' li pt\n",
      "niHu wootsdUo,heHth sdtWT\n",
      "sifu,rw wde.e sdTL\n",
      "lao,k,a,Mn I\n",
      "oyhA ukcw\n",
      " e Rhfeya uhchw dylirr\n",
      "Iteaomter, eouudenoha hi  ctftssra a t ttnnc :W\n",
      "eeoehjde  \n",
      "b uefo  oEbm: lgade nn:Teci\n"
     ]
    }
   ],
   "source": [
    "for sample in model.generate():\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "afa6716d-05b5-4840-8884-f7f0e0115155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2835, 9.4612, 0.1748, 0.0510, 0.0510, 0.3929, 1.1048, 0.1568, 0.4791,\n",
       "        0.0527, 0.5872, 0.2412, 0.1821, 0.4585, 0.1874, 0.2597, 0.1651, 0.4159,\n",
       "        0.1448, 0.1820, 0.2191, 0.6829, 0.0709, 0.1295, 0.2676, 0.2233, 0.3198,\n",
       "        0.3384, 0.1116, 0.0669, 0.3365, 0.3048, 0.4270, 0.2283, 0.0879, 0.2617,\n",
       "        0.0594, 0.1653, 0.0609, 3.0535, 0.6768, 0.9304, 1.8167, 5.3133, 0.9518,\n",
       "        0.8170, 2.9028, 2.5223, 0.0866, 0.4539, 1.9358, 1.2955, 2.7103, 3.6472,\n",
       "        0.6802, 0.0886, 2.7879, 2.8116, 3.7706, 1.5203, 0.4868, 1.0139, 0.0804,\n",
       "        1.1811, 0.0715])"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.probs.sum(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4c87d47b-47f1-40d9-b303-bd6e69f51966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 3., 3., 3., 3., 3., 3., 3., 3., 3.])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(10,3)\n",
    "x.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "f9629854-3e76-4541-85ed-a101f49fabb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
