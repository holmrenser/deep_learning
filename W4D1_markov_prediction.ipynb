{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1c54284-4f94-45a2-a4c2-5efffa394aed",
   "metadata": {},
   "source": [
    "# Markov prediction\n",
    "In this notebook we will implement (arguably) the most simple form of sequence generation: a [bigram](https://en.wikipedia.org/wiki/Bigram) model where each subsequent token (character) is generated by sampling from the conditional probability distribution $P(X_t \\mid X_{t-1}) \\sim Multinomial_{X_{t-1}}(N_{tokens})$, i.e. every character (token) has an associated probility table for what the next character (token) will be. 'Training' proceeds through simple counting of observed character pairs.\n",
    "\n",
    "We start with a very minimal implementation and then proceed to introduce some boilerplate for dataloading, model forwarding, and training. Whereas this is not strictly necesarry for this model, it makes comparisons with later more complex models more straightforward. More importantly, our advanced implementation will be generalized to [n-grams](https://en.wikipedia.org/wiki/N-gram), i.e. higher order markov chains. For example a n-gram model of order two: $P(X_t \\mid X_{t-1}, X_{t-2}) \\sim Multinomial_{X_{t-1}, X_{t-2}}(N_{tokens})$. This will allow us to explore the limits of conditional probability lookup tables (e.g. in terms of performance and number of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19228e5-abe9-4040-884b-2d852997e2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all dependencies for the entire notebook\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import RandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524eef47-cf85-4121-91f6-90310247f5ae",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We use the tiny shakespeare dataset to train a character level markov model to predict text that looks very little like shakespeare. All data is in one text file, which we download below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a233403-af6a-425b-9e19-ebfd6db3a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the tinyshakespeare dataset\n",
    "!wget -nc https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7de2039-f92e-4b59-b01d-a1264fc4b8a8",
   "metadata": {},
   "source": [
    "# Minimal bigram implementation\n",
    "Below is a very minimal implementation of generating a text sample using 'markov prediction': every next token is sampled according to a probability based only on the previous token. 'Training' consists of counting observed character pairs. We don't calculate a loss and model evaluation is based on vibes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a14bae4-4ecf-4f7f-b044-e3f873b55237",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input.txt') as fh:\n",
    "    data = fh.read()\n",
    "\n",
    "# create a sorted list of unique characters\n",
    "chars = sorted(list(set(data)))\n",
    "# count how many 'tokens'\n",
    "vocab_size = len(chars)\n",
    "# map characters to integers\n",
    "encode = { ch:i for i,ch in enumerate(chars) }\n",
    "# map integers to characters\n",
    "decode = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# initialize counts with ones for automatic laplace/additive smoothing: https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "counts = torch.ones(vocab_size, vocab_size)\n",
    "\n",
    "# 'train' (i.e. count) for n steps (i.e. character pairs)\n",
    "n_train_chars = len(data) - 1\n",
    "for i in trange(n_train_chars):\n",
    "    token_i = encode[data[i]]\n",
    "    token_j = encode[data[i+1]]\n",
    "    counts[token_i][token_j] += 1\n",
    "\n",
    "# divide by row sums to get transition probabilities\n",
    "probs = counts / counts.sum(dim=1, keepdim=True)\n",
    "\n",
    "# start our sample with token 0 (i.e. the newline character \\n)\n",
    "sample = [0]\n",
    "sample_n_tokens = 256\n",
    "\n",
    "# for n steps, sample a token from the distribution belonging the last token in the current sample and add it to the sample\n",
    "for _ in range(sample_n_tokens):\n",
    "    last_token = sample[-1]\n",
    "    next_token_probs = probs[last_token]\n",
    "    next_token = torch.multinomial(next_token_probs, num_samples=1)\n",
    "    sample.append(next_token.item())\n",
    "\n",
    "# decode sample tokens into characters\n",
    "decoded_sample = ''.join(decode[token] for token in sample)\n",
    "print(f'Sample:\\n{decoded_sample}\\n')\n",
    "\n",
    "# plot observed transition probabilities\n",
    "plt.imshow(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbced26-0fda-40b2-b931-cf6e265dec0b",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Experiment with different number of training steps (e.g. 100, 1000, and 10,000), when do you start seeing some structure appearing in the generated samples? What about if you 'train' on the entire dataset? (i.e. `n_train_chars = len(data) - 1`)?\n",
    "\n",
    "# Advanced n-gram implementation\n",
    "\n",
    "Clearly bigrams pick up some structure from a large enough text, but are not powerful enough to generate realistic sentences. Below we implement the same counting principle for n-gram models. To do so, we create a character level tokenizer, a dataset class that returns tensor of certain order, and a model class that takes care of our most commonly used procedures (e.g. parameter initialization, prediction, calculating a loss score, and generating novel text). These main building blocks (tokenizer, dataset, model) look very similar to what we will use in the rest of the week when we build more advanced neural network models.\n",
    "\n",
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1af0c-d783-4ff5-8ad1-5cbb2b547732",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CharacterTokenizer:\n",
    "    decode_dict: dict[int, str] = field(default_factory=dict)\n",
    "    encode_dict: dict[str, int] = field(default_factory=dict)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\"Character to int mapping\"\"\"\n",
    "        return self.encode_dict\n",
    "\n",
    "    def train(self, input_str: str) -> None:\n",
    "        \"\"\"Determine what character will be mapped to which int using lexicograpical order\"\"\"\n",
    "        chars = sorted(set(input_str))\n",
    "        self.decode_dict: dict[int, str] = dict(enumerate(chars))\n",
    "        self.encode_dict: dict[str, int] = {v:k for k,v in self.decode_dict.items()}\n",
    "\n",
    "    def encode(self, input: str) -> list[int]:\n",
    "        \"\"\"Turn a string into a list of ints using a pretrained lookup table\"\"\"\n",
    "        return [self.encode_dict[char] for char in input]\n",
    "\n",
    "    def decode(self, tokens: list[int]) -> str:\n",
    "        \"\"\"Turn a list of ints into a string using a reverse lookup table\"\"\"\n",
    "        return ''.join(self.decode_dict[token] for token in tokens)\n",
    "\n",
    "class CharacterDataset:\n",
    "    def __init__(self, data: str, tokenizer: CharacterTokenizer, block_size: int=256, order: int=1):\n",
    "        assert order > 0, 'Order must be > 1'\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.get_vocab())\n",
    "        self.block_size = block_size\n",
    "        self.order = order\n",
    "\n",
    "    def __repr__(self):\n",
    "        n_chars = len(self.data)\n",
    "        vocab_size = self.vocab_size\n",
    "        block_size = self.block_size\n",
    "        order = self.order\n",
    "        return f'CharacterDataset({n_chars=}, {vocab_size=}, {block_size=}, {order=})'\n",
    "\n",
    "    @classmethod\n",
    "    def from_textfile(cls, filename: str, block_size: int=256, order: int=1) -> 'CharacterDataset':\n",
    "        tokenizer = CharacterTokenizer()\n",
    "        with open(filename, 'r') as fh:\n",
    "            data = fh.read()\n",
    "            tokenizer.train(data)\n",
    "            return cls(data, tokenizer, block_size=block_size, order=order)\n",
    "\n",
    "    def train_test_split(self, train_percentage: float=0.8) -> tuple['CharacterDataset','CharacterDataset']:\n",
    "        n_train_chars = int(train_percentage * len(self.data))\n",
    "\n",
    "        train_data = self.data[:n_train_chars]\n",
    "        train_dataset = CharacterDataset(train_data, self.tokenizer, self.block_size, self.order)\n",
    "\n",
    "        test_data = self.data[n_train_chars:]\n",
    "        test_dataset = CharacterDataset(test_data, self.tokenizer, self.block_size, self.order)\n",
    "\n",
    "        return train_dataset, test_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data) - self.block_size - self.order\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.tensor:\n",
    "        # grab a chunk of block_size characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + self.order]\n",
    "        # encode every character to an integer\n",
    "        tokens = self.tokenizer.encode(chunk)\n",
    "        # convert to tensor\n",
    "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "        # determine indices based on order\n",
    "        x_indices = (\n",
    "            torch.arange(self.block_size).unsqueeze(1) # determine starting indices and add empty dim\n",
    "            + torch.arange(self.order).repeat(self.block_size, 1) # add range(0, order) to get ranges of len(order) at every starting index\n",
    "        )\n",
    "        # return relevant tokens based on indices\n",
    "        x = tokens[x_indices]\n",
    "        y = tokens[self.order:]\n",
    "        return x,y\n",
    "\n",
    "dataset = CharacterDataset.from_textfile('./input.txt')\n",
    "train_dataset,test_dataset = dataset.train_test_split()\n",
    "len(train_dataset),len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cdf3cf-e33f-4456-b714-be5f149a4868",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44183014-3ccf-4d69-93df-1dc823475704",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramModel(nn.Module):\n",
    "    \"\"\"Very simple model for next character prediction by counting observed n-grams\"\"\"\n",
    "    def __init__(self, vocab_size, order=1):\n",
    "        assert order > 0, 'Order must be > 1'\n",
    "        super().__init__()\n",
    "        self.order = order\n",
    "        # counting character pairs in a vocab_size ** (order + 1) table\n",
    "        self.counts = torch.ones([vocab_size] * (order + 1))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        order = self.order\n",
    "        return f'NGramModel({order=})'\n",
    "\n",
    "    @property\n",
    "    def probs(self) -> torch.tensor:\n",
    "        \"\"\"Normalize counts to probabilities by dividing over row sums\"\"\"\n",
    "        return self.counts / self.counts.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor=None):\n",
    "        # (B, L, order) -> (B, L, vocab_size)\n",
    "        probs = self.probs[*idx.permute(2,1,0)].permute(1,0,2)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.cross_entropy(probs.view(-1, probs.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return probs,loss\n",
    "\n",
    "    def generate(self, sample_length: int=256):\n",
    "        \"\"\"Generate samples\"\"\"\n",
    "        idx = torch.zeros((1, self.order), dtype=torch.long)\n",
    "        for _ in trange(sample_length, desc='Sampling'):\n",
    "            x_indices = torch.arange(idx.shape[1] - self.order, idx.shape[1])\n",
    "            x = torch.vstack([idx[i, x_indices][None][None] for i in range(1)])\n",
    "            probs,_ = model(x)\n",
    "            new_idx = torch.multinomial(probs.squeeze(1), num_samples=1)\n",
    "            idx = torch.hstack([idx, new_idx])\n",
    "\n",
    "        samples = []\n",
    "        for sample in idx:\n",
    "            samples.append(dataset.tokenizer.decode(sample.tolist()))\n",
    "        return samples\n",
    "\n",
    "dataset = CharacterDataset.from_textfile('./input.txt', block_size=12, order=3)\n",
    "model = NGramModel(vocab_size = dataset.vocab_size, order = dataset.order)\n",
    "\n",
    "print(dataset)\n",
    "print(model)\n",
    "\n",
    "# Show a sample from an untrained model\n",
    "for sample in model.generate():\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f101c2d-9d1e-40ea-9d8d-7bb0805a1eb0",
   "metadata": {},
   "source": [
    "## Training\n",
    "Note that whereas we can call this 'training', all the code below does is count character pairs (triples, etc...) and update the pair count table of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f274d9-0cdc-4d37-a0fc-9c5301e5198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CharacterDataset.from_textfile('./input.txt', block_size=256, order=1)\n",
    "train_dataset, test_dataset = dataset.train_test_split()\n",
    "\n",
    "model = NGramModel(dataset.vocab_size, dataset.order)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    sampler=RandomSampler(train_dataset, num_samples=100000),\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    sampler=RandomSampler(test_dataset, replacement=True),\n",
    "    batch_size=10\n",
    ")\n",
    "test_dataloader = iter(test_dataloader)\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "\n",
    "for i, train_batch in enumerate(tqdm(train_dataloader, desc='Training')):\n",
    "    for x,y in zip(*train_batch):\n",
    "        model.counts[*x.T,y] += 1\n",
    "    if i % 100 == 0:\n",
    "        test_batch = next(test_dataloader)\n",
    "        test_loss.append(model(*test_batch)[1])\n",
    "        train_loss.append(model(*train_batch)[1])\n",
    "\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(test_loss, label='test loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255bbf36-c57a-4e7a-ad6f-82f5694149bd",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The model has learned some structure, e.g. there are probably a bit more newlines and a colon is always followed by a newline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6b1661-f2c4-48bf-b432-6103a2a7e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in model.generate():\n",
    "    print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfffbc4-7722-4612-9ffd-de0cc7918af3",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Fit N-gram models of order 1, 2, and 3, note the training and test loss, and evaluate a few generated samples. What do you observe? Does an n-gram model of order 3 result in reasonable text? \n",
    "\n",
    "### Exercise 3\n",
    "How many parameters do you need for an N-gram model of order 4? What do you expect the training loss will be (extrapolate from the results you got in excercise 2)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffba0007-5256-4c7c-b131-9fd0818dc3e8",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "### Exercise 1\n",
    "100 and 100 are generally not enough, 10000 picks up some structure such as repeated newlines and sometimes word fragments. Training on the whole dataset generally produces giberish, but some interesting structure emerges, such as repeated capital letters, interpunction at the end of sentences, and word fragments.\n",
    "\n",
    "### Exercise 2\n",
    "- Order 1: train and test loss of ~4.07, samples in line with exercise 1\n",
    "- Order 2: train loss 3.94, test loss slightly higher, samples produce actual words similar to shakespeare, such as QUEEN\n",
    "- Order 3: train loss 3.80, test loss 3.85, samples become hit and miss. One approach is to train for much longer, which generated the sample below:\n",
    "> SICINIUS:\n",
    "> Some King was part ind law to-day.\n",
    ">\n",
    "> KINGHAM:\n",
    "> Wher that you, or broung Herefusin, cons, I life\n",
    "> But those compassance\n",
    "> Why, have threen resolding.\n",
    ">\n",
    "> Nurs\n",
    "> Since that heir pers?\n",
    ">\n",
    "> CAMILLO:\n",
    "> And Citizens bothe in preve\n",
    "> Go, love;\n",
    "> For him, anous sun than an\n",
    "\n",
    "Additional note: higher order models converge slower --> they need more data to work.\n",
    "\n",
    "### Exercise 3\n",
    "\n",
    "65 ** 4 = 17.850.625 (~18M) parameters. Assuming our loss drops again by ~0.15 this is does not seem very effifient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
