{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmLp0eqq4NRJ"
      },
      "source": [
        "# Fine-tuning BERT\n",
        "\n",
        "In this notebook, you will download a pre-trained BERT network and fine-tune it on a task of predicting whether two sequences are semantically similar or not (a two-class classification problem). To save time, you will only fine-tune the newly added parameters.\n",
        "\n",
        "Note that the Huggingface transformer interface (that you used in the tokenization notebook) makes these tasks far easier, by providing functions that download trained models; we here spell out the code, to give you an impression of what happens behind the scenes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJDBH1Sm4una"
      },
      "source": [
        "First, we install specific versions of torch, numpy, and the D2L module - **if you get an error, do not restart the session!** Just continue, things should work fine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ARDBTFgbgjZ"
      },
      "outputs": [],
      "source": [
        "!pip install torch==2.5.1 torchvision==0.20.1\n",
        "!pip install numpy<2\n",
        "!pip install --no-deps d2l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Itajs2ruXPMl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from d2l import torch as d2l\n",
        "import json\n",
        "import multiprocessing\n",
        "import tensorflow_datasets as tfds   # DDR: HAD TO INSTALL FIRST\n",
        "import numpy as np\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2LDRZdNXik9"
      },
      "source": [
        "We make use of a dataset distributed with tensorflow. It's part of GLUE, the General Language Understanding Evaluation benchmark (https://gluebenchmark.com/), a collection of resources for training, evaluating, and analyzing natural language understanding systems. In particular, we focus on the Microsoft Research Paraphrase Corpus (MRPC), a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-Eaw4-vJpOm"
      },
      "outputs": [],
      "source": [
        "tfds.disable_progress_bar()\n",
        "glue, info = tfds.load('glue/mrpc', with_info=True,\n",
        "                       # It's small, load the whole dataset\n",
        "                       batch_size=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLommOIaSLH"
      },
      "source": [
        "The MRPC dataset contains training data, test data and validation data. Items in the datasets contain pairs of sentences (sentence1 and sentence2) with the associated label indicating that the two sentences are equivalent (1) or not (0). In the test data, the label is set to -1 (unlabelled)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLZWuCyqJvfz"
      },
      "outputs": [],
      "source": [
        "def extract_text(s):\n",
        "    # Remove information that will not be used by us\n",
        "    s = re.sub('\\\\(', '', s)\n",
        "    s = re.sub('\\\\)', '', s)\n",
        "    # Substitute two or more consecutive whitespace with space\n",
        "    s = re.sub('\\\\s{2,}', ' ', s)\n",
        "    return s.strip()\n",
        "\n",
        "def read_preprocess(data):\n",
        "    \"\"\"Read the dataset into sentence1, sentence2, and labels.\"\"\"\n",
        "    label_set = {'not_equivalent': 0, 'equivalent': 1}\n",
        "    sentences1 = [extract_text(s.numpy().decode()) for s in data['sentence1']]\n",
        "    sentences2 = [extract_text(s.numpy().decode()) for s in data['sentence2']]\n",
        "    labels = [s.numpy() for s in data['label']]\n",
        "    return sentences1, sentences2, labels\n",
        "\n",
        "print('Labels: ', info.features['label'].names)\n",
        "train_data      = read_preprocess(glue['train'])\n",
        "test_data       = read_preprocess(glue['test'])\n",
        "validation_data = read_preprocess(glue['validation'])\n",
        "\n",
        "# For train, test, validation: print number of cases with label -1,0,1\n",
        "for data in [train_data, test_data, validation_data]:\n",
        "    print([[row for row in data[2]].count(i) for i in [-1,0,1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOlVbh3way09"
      },
      "source": [
        "Some examples of sentence pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLeQLlWcVqDl"
      },
      "outputs": [],
      "source": [
        "sents1, sents2, labels = train_data\n",
        "print(sents1[0], '<->', sents2[0], ': ', labels[0])\n",
        "sents1, sents2, labels = validation_data\n",
        "print(sents1[0], '<->', sents2[0], ': ', labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40PrlOlnc922"
      },
      "source": [
        "We now will load a pre-trained BERT, specifically a 12-layer, 768-hidden units, 12-head, 110M parameter base model (there are also small and large versions). We also load the vocabulary which was obtained when pre-training the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCgz4jOvYoqK"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model(pretrained_model, num_hiddens, ffn_num_hiddens,\n",
        "                          num_heads, num_blks, dropout, max_len, devices):\n",
        "    data_dir = d2l.download_extract(pretrained_model)\n",
        "\n",
        "    # Define an empty vocabulary to load the predefined vocabulary\n",
        "    vocab = d2l.Vocab()\n",
        "    vocab.idx_to_token = json.load(open(os.path.join(data_dir, 'vocab.json')))\n",
        "    vocab.token_to_idx = {token: idx for idx, token in enumerate(vocab.idx_to_token)}\n",
        "\n",
        "    # Instantiate the network architecture\n",
        "    bert = d2l.BERTModel(len(vocab), num_hiddens, ffn_num_hiddens=ffn_num_hiddens,\n",
        "                         num_heads=num_heads, num_blks=num_blks, dropout=dropout, max_len=max_len)\n",
        "\n",
        "    # Load pretrained BERT parameters\n",
        "    bert.load_state_dict(torch.load(os.path.join(data_dir, 'pretrained.params')))\n",
        "    return bert, vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBGacMayYrcc"
      },
      "outputs": [],
      "source": [
        "devices = d2l.try_all_gpus()\n",
        "\n",
        "d2l.DATA_HUB['bert.base'] = (d2l.DATA_URL + 'bert.base.torch.zip',\n",
        "                             '225d66f04cae318b841a13d32af3acc165f253ac')\n",
        "\n",
        "bert, vocab = load_pretrained_model('bert.base', num_hiddens=768,\n",
        "                                    ffn_num_hiddens=3072, num_heads=12,\n",
        "                                    num_blks=12, dropout=0.1, max_len=512,\n",
        "                                    devices=devices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U_yli2_bZLm"
      },
      "source": [
        "Let's inspect the pretrained BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lG5aZ0t_bZLm"
      },
      "outputs": [],
      "source": [
        "bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-FeOp6iImst"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "In the `bert` architecture above, check that you recognize the encoder blocks (how many?), each with four submodules (which ones?).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tSB1PzhFPRN"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "You can use the code below to investigate the context dependency of embeddings of the same word - in this case \"bank\", which gets a different 768D embedding depending on the sentence in which it is used. Do the similarities make sense? See if you can come up with some different sentences where the same words are used in different contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHAoIT7l7kqD"
      },
      "outputs": [],
      "source": [
        "def get_bert_encoding(net, tokens):\n",
        "    toks, segments = d2l.get_tokens_and_segments(tokens)\n",
        "    token_ids = torch.tensor(vocab[toks], device=devices[0]).unsqueeze(0)\n",
        "    segments = torch.tensor(segments, device=devices[0]).unsqueeze(0)\n",
        "    valid_len = torch.tensor(len(toks), device=devices[0]).unsqueeze(0)\n",
        "    encoded_X = net(token_ids, segments, valid_len)\n",
        "    return encoded_X\n",
        "\n",
        "bert.to(devices[0])\n",
        "bert.eval()\n",
        "\n",
        "tokens_a = 'i walked along the road to get cash from my bank'.split()\n",
        "tokens_b = 'we managed to open a savings account at the local bank'.split()\n",
        "tokens_c = 'i swam across the river to get to its other bank'.split()\n",
        "\n",
        "# First token is <cls>, so 'bank' is the 11th token.\n",
        "enc_a = get_bert_encoding(bert.encoder, tokens_a)[:,11,:]\n",
        "enc_b = get_bert_encoding(bert.encoder, tokens_b)[:,11,:]\n",
        "enc_c = get_bert_encoding(bert.encoder, tokens_c)[:,11,:]\n",
        "\n",
        "print(F.cosine_similarity(enc_a,enc_b))\n",
        "print(F.cosine_similarity(enc_a,enc_c))\n",
        "print(F.cosine_similarity(enc_b,enc_c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGaDs_AkeNqh"
      },
      "source": [
        "## The dataset for fine-tuning BERT\n",
        "\n",
        "For the task on the GLUE dataset, we define a customized dataset class `GLUEBERTDataset`.\n",
        "In each example, the two sentences form a pair of text sequences packed into one BERT input sequence. Segment IDs are used to distinguish the two text sequences.\n",
        "With the predefined maximum length of a BERT input sequence (`max_len`, here 128), the last token of the longer of the input text pair keeps getting removed until the maximum length is met."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWi41PrIZn9S"
      },
      "outputs": [],
      "source": [
        "class GLUEBERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset, max_len, vocab=None):\n",
        "        # Unpack all tokens\n",
        "        all_tokens = [[s1_tokens, s2_tokens]\n",
        "            for s1_tokens, s2_tokens in zip(*[\n",
        "                d2l.tokenize([s.lower() for s in sentences])\n",
        "                    for sentences in dataset[:2]])]\n",
        "\n",
        "        self.labels = torch.tensor(dataset[2])\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        (self.all_token_ids, self.all_segments,\n",
        "         self.valid_lens) = self._preprocess(all_tokens)\n",
        "        print('Read ' + str(len(self.all_token_ids)) + ' examples')\n",
        "\n",
        "    def _preprocess(self, all_tokens):\n",
        "        pool = multiprocessing.Pool(4)  # Use 4 worker processes\n",
        "        out = pool.map(self._mp_worker, all_tokens)\n",
        "        all_token_ids = [token_ids for token_ids, segments, valid_len in out]\n",
        "        all_segments = [segments for token_ids, segments, valid_len in out]\n",
        "        valid_lens = [valid_len for token_ids, segments, valid_len in out]\n",
        "        return (torch.tensor(all_token_ids, dtype=torch.long),\n",
        "                torch.tensor(all_segments, dtype=torch.long), torch.tensor(valid_lens))\n",
        "\n",
        "    def _mp_worker(self, all_tokens):\n",
        "        s1_tokens, s2_tokens = all_tokens\n",
        "        self._truncate_pair_of_tokens(s1_tokens, s2_tokens)\n",
        "        tokens, segments = d2l.get_tokens_and_segments(s1_tokens, s2_tokens)\n",
        "        token_ids = self.vocab[tokens] + [self.vocab['<pad>']] \\\n",
        "                             * (self.max_len - len(tokens))\n",
        "        segments = segments + [0] * (self.max_len - len(segments))\n",
        "        valid_len = len(tokens)\n",
        "        return token_ids, segments, valid_len\n",
        "\n",
        "    def _truncate_pair_of_tokens(self, s1_tokens, s2_tokens):\n",
        "        # Reserve slots for '<CLS>', '<SEP>', and '<SEP>' tokens for the BERT input\n",
        "        while len(s1_tokens) + len(s2_tokens) > self.max_len - 3:\n",
        "            if len(s1_tokens) > len(s2_tokens):\n",
        "                s1_tokens.pop()\n",
        "            else:\n",
        "                s2_tokens.pop()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx].to(devices[0]),\n",
        "                self.all_segments[idx].to(devices[0]),\n",
        "                self.valid_lens[idx].to(devices[0])), self.labels[idx].to(devices[0])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4b5PSRxZwIt"
      },
      "outputs": [],
      "source": [
        "max_len = 128\n",
        "batch_size = 64\n",
        "\n",
        "train_set       = GLUEBERTDataset(train_data, max_len, vocab)\n",
        "validation_set  = GLUEBERTDataset(validation_data, max_len, vocab)\n",
        "test_set        = GLUEBERTDataset(read_preprocess(glue['test']), max_len, vocab)\n",
        "\n",
        "# We use glue_validate for testing (test set is unlabelled)\n",
        "train_iter      = torch.utils.data.DataLoader(train_set, batch_size, shuffle=True)\n",
        "validation_iter = torch.utils.data.DataLoader(validation_set, batch_size, shuffle=False)\n",
        "test_iter       = torch.utils.data.DataLoader(test_set, batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOkeA11kbZLt"
      },
      "source": [
        "Next, we create a network out of parts of the pretrained BERT model - its encoder and hidden layer - followed by a simple linear output layer with 2 units coding for our two classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWiA2O3gasMW"
      },
      "outputs": [],
      "source": [
        "class BERTClassifier(nn.Module):\n",
        "    def __init__(self, bert, num_hiddens):\n",
        "        super(BERTClassifier, self).__init__()\n",
        "        # Freeze BERT parameters\n",
        "        for param in bert.parameters():\n",
        "          param.requires_grad = False\n",
        "        self.encoder = bert.encoder\n",
        "        self.hidden = bert.hidden\n",
        "        self.output = nn.Sequential(\n",
        "                nn.Linear(num_hiddens, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        tokens_X, segments_X, valid_lens_x = inputs\n",
        "        encoded_X = self.encoder(tokens_X, segments_X, valid_lens_x)\n",
        "        return self.output(self.hidden(encoded_X[:, 0, :]))\n",
        "\n",
        "net = BERTClassifier(bert,768)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKjmkpp4bZLv"
      },
      "outputs": [],
      "source": [
        "# Taken from the D2L book\n",
        "def local_train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs,\n",
        "                     devices=d2l.try_all_gpus()):\n",
        "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
        "    net = nn.DataParallel(net, device_ids=devices).to(devices[0])\n",
        "    for epoch in range(num_epochs):\n",
        "        # Store training_loss, training_accuracy, num_examples, num_features\n",
        "        metric = d2l.Accumulator(4)\n",
        "        for i, (features, labels) in enumerate(train_iter):\n",
        "            timer.start()\n",
        "            l, acc = d2l.train_batch_ch13(\n",
        "                net, features, labels, loss, trainer, devices)\n",
        "            metric.add(l, acc, labels.shape[0], labels.numel())\n",
        "            timer.stop()\n",
        "        test_acc = d2l.evaluate_accuracy_gpu(net, test_iter)\n",
        "        print(f'epoch {epoch}: '\n",
        "              f'loss {metric[0] / metric[2]:.3f}, train acc '\n",
        "              f'{metric[1] / metric[3]:.3f}, val acc {test_acc:.3f}')\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec on '\n",
        "          f'{str(devices)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfaHAhCQbZLw"
      },
      "source": [
        "We can now train the network, using a routine provided by D2L. We only train for 10 epochs, which may not be enough but avoids that we run out of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oycY1l1Za0Gu"
      },
      "outputs": [],
      "source": [
        "lr, num_epochs = 1e-4, 10\n",
        "\n",
        "# For now, we freeze BERT's main parameters.\n",
        "# To train these as well, use trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "# To use different learning rates for various layers, use e.g.:\n",
        "# trainer = torch.optim.Adam([{'params': net.encoder.parameters(), 'lr': 1e-6},\n",
        "#                             {'params': net.hidden.parameters(),  'lr': 1e-6},\n",
        "#                             {'params': net.output.parameters(),  'lr': 1e-4}])\n",
        "trainer = torch.optim.Adam(net.output.parameters(), lr=lr)\n",
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "print('Start:', datetime.now().strftime(\"%H:%M:%S\"))\n",
        "local_train_ch13(net, train_iter, validation_iter, loss, trainer, num_epochs, devices)\n",
        "print('End:', datetime.now().strftime(\"%H:%M:%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ7Zu8eAe1EI"
      },
      "outputs": [],
      "source": [
        "# Test the network on a batch of sentence pairs\n",
        "net.to(devices[0])\n",
        "net.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  X,y = next(iter(train_iter))\n",
        "  yhat = np.argmax(net(X).detach().cpu().numpy(),axis=1)\n",
        "\n",
        "for i in range(len(yhat)):\n",
        "  print('Input: ', ' '.join([vocab.idx_to_token[j] for j in X[0][i]]))\n",
        "  print('Prediction: ', yhat[i], y[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpjEWFWlbZLy"
      },
      "outputs": [],
      "source": [
        "# To test our own sentences, first create a GLUEBERTDataset with them\n",
        "s1 = \"We use this as a test sentence to see whether the network works.\"\n",
        "s2 = \"To test if the network works we use this sentence.\"\n",
        "\n",
        "#s1 = \"Google is a very large company indeed.\"\n",
        "#s2 = \"And now for something completely different.\"\n",
        "\n",
        "data = ([extract_text(s1)], [extract_text(s2)], [0])\n",
        "print(data)\n",
        "\n",
        "my_set = GLUEBERTDataset(data, max_len, vocab)\n",
        "my_iter = torch.utils.data.DataLoader(my_set, 1)\n",
        "\n",
        "with torch.no_grad():\n",
        "  X, y = next(iter(my_iter))\n",
        "  yhat = np.argmax(net(X).detach().cpu().numpy(),axis=1)\n",
        "print('Prediction: ', yhat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4DKqdjuI6kg"
      },
      "source": [
        "Instead of only looking at the actual prediction of the model (as above) we can also look at the probabilities for the two possible labels (\"similar or not similar\"):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-LHlMDoI8Yy"
      },
      "outputs": [],
      "source": [
        "torch.nn.functional.softmax(net(X).detach(),dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yquEAb6L62CN"
      },
      "source": [
        "## Exercise 3\n",
        "Try changing specific key words in the sentence pairs above, to get some idea on how much understanding the model has about language. For example, replace \"network\" by \"model\" or by \"car\" in one of the two sentences of the first pair. Look at the resulting prediction (equivalent or not) and also at the underlying probabilities which the model gives as output.\n",
        "\n",
        "## Exercise 4\n",
        "As you can see, performance on the test set is not very good yet. If you have the time, you can experiment with training more parameters, i.e. including the weights of the BERT network. You can do this by changing code at the places marked by \"freeze BERT\". Perhaps it's necessary to use a lower learning rate for the pretrained weights. What is the effect on training and test accuracy? NB: if you rerun code, do not forget to reload the pretrained BERT parameters and include them in the network.\n",
        "\n",
        "In a similar vein, you could try extending the final classification model added on top of BERT. Does performance improve if you add another layer?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m918nk75JFuq",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "You can clearly identify 12 encoder blocks, as follows:\n",
        "\n",
        "```\n",
        "TransformerEncoderBlock(\n",
        "        (attention): MultiHeadAttention(\n",
        "          (attention): DotProductAttention(\n",
        "            (dropout): Dropout(p=0.1, inplace=False)\n",
        "          )\n",
        "          (W_q): Linear(in_features=0, out_features=768, bias=True)\n",
        "          (W_k): Linear(in_features=0, out_features=768, bias=True)\n",
        "          (W_v): Linear(in_features=0, out_features=768, bias=True)\n",
        "          (W_o): Linear(in_features=0, out_features=768, bias=True)\n",
        "        )\n",
        "        (addnorm1): AddNorm(\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "        (ffn): PositionWiseFFN(\n",
        "          (dense1): Linear(in_features=0, out_features=3072, bias=True)\n",
        "          (relu): ReLU()\n",
        "          (dense2): Linear(in_features=0, out_features=768, bias=True)\n",
        "        )\n",
        "        (addnorm2): AddNorm(\n",
        "          (dropout): Dropout(p=0.1, inplace=False)\n",
        "          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "      )\n",
        "```\n",
        "\n",
        "At the end, you will see an MLM layer (maxed language modelling) and an NSP layer (next sentence prediction); these were both used in pretraining. When fine-tuning BERT, we take the output of the encoder block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9CbefBuZd8Q"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "If all goes well, you should get a similarity of ~0.91 for the two sentences that use \"bank\" to mean \"financial institution\", and ~0.80 resp. ~0.75 for the pairs with one sentence using \"bank\" as \"riverside\".\n",
        "\n",
        "There are many more homonyms that you can play with, see e.g. https://www.yourdictionary.com/articles/examples-homonyms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQIk7h8XaQk3"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "Changing \"network\" in the first sentence to \"model\" or \"car\" indeed changes the classifcation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uahp5aOqaX46"
      },
      "source": [
        "## Exercise 4\n",
        "\n",
        "Left as an exercise for the reader, but full fine-tuning with more complex layers will likely take a bit more time than we have during the course."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}