{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec1f1e7-b61a-4742-a3e5-a99c7e4096f2",
   "metadata": {
    "id": "dec1f1e7-b61a-4742-a3e5-a99c7e4096f2"
   },
   "source": [
    "# Word embeddings\n",
    "In this notebook we work with pretrained word embedding scores from the GloVe project. We use the smallest version, which maps 400,000 words into 50D embedding space, and was trained on 6 billion words.\n",
    "From the project description:\n",
    "> \"The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence\".\n",
    "\n",
    "For more details on model formulation and training procedures visit the [GloVe project website](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeb7dc75-8854-4a31-9ccb-ccd10126d524",
   "metadata": {
    "id": "aeb7dc75-8854-4a31-9ccb-ccd10126d524"
   },
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from warnings import warn\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb07660-cbbb-4445-8e61-059cf8dfa6fe",
   "metadata": {
    "id": "edb07660-cbbb-4445-8e61-059cf8dfa6fe"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01db6910-745b-4fdb-8dd1-9b12f3247564",
   "metadata": {
    "id": "01db6910-745b-4fdb-8dd1-9b12f3247564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "gunzip: can't stat: glove.6B.50d.txt.gz (glove.6B.50d.txt.gz.gz): No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# Download and unzip glove word embeddings\n",
    "!wget -nc https://github.com/holmrenser/deep_learning/raw/main/data/glove.6B.50d.txt.gz\n",
    "!gunzip -f glove.6B.50d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f57a2-49e1-4c7a-8f84-168b8fa8b43c",
   "metadata": {
    "id": "609f57a2-49e1-4c7a-8f84-168b8fa8b43c"
   },
   "source": [
    "## Model\n",
    "We create a small class that wraps functionality for reading in the tab delimited file with pretrained embeddings, let's us select embeddings for specific words, and can calculate closest (in embedding space) words to a given word. The pretrained embeddings are parsed into a vocabulary mapping words to integer indices, and a torch embedding table that is accessed using these indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5226c299-865a-4425-9645-9cc32e24ac10",
   "metadata": {
    "id": "5226c299-865a-4425-9645-9cc32e24ac10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb6530d7ecd4a4092135cb7f8480062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading:   0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    \"\"\"Wrapper class for working with GloVe word embeddings\"\"\"\n",
    "    def __init__(self, vocab: dict[str, int], embeddings: torch.tensor):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, filename: str) -> 'WordEmbedding':\n",
    "        \"\"\"Load pretrained embeddings from a whitespace-separated text file, first column is the word, rest are embeddings\"\"\"\n",
    "        vocab = {'<unk>': 0} # start vocabulary with special character <unk> for unknown words\n",
    "        embeddings = []\n",
    "\n",
    "        with open(filename,'r') as fh:\n",
    "            data = fh.readlines()\n",
    "            for i,line in enumerate(tqdm(data, desc='Loading')):\n",
    "                parts = line.split()\n",
    "\n",
    "                token = parts[0]\n",
    "                vocab[token] = i + 1 # add one to account for predefined <unk> token\n",
    "\n",
    "                embedding = list(map(float, parts[1:]))\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "        unk_emb = embeddings.mean(dim=0) # embedding of unknown characters is average of all embeddings\n",
    "        embeddings = torch.vstack([unk_emb, embeddings])\n",
    "\n",
    "        return cls(vocab, embeddings)\n",
    "\n",
    "    def forward(self, word: str) -> torch.tensor:\n",
    "        \"\"\"Maps word to embedding vector\"\"\"\n",
    "        i = self.vocab.get(word, 0) # 0 is the index of the <unk> character\n",
    "        if i == 0:\n",
    "            warn(f'{word} is not in the vocabulary, returning average embedding')\n",
    "        return self.embeddings(torch.tensor([i]))\n",
    "\n",
    "    def find_closest(self, vec: torch.tensor, k: int=1) -> str:\n",
    "        \"\"\"Find closest k words of an embedding vector using cosine similarity\"\"\"\n",
    "        cos_sim = F.cosine_similarity(emb.embeddings.weight, vec)\n",
    "        closest_idx = torch.argsort(cos_sim, descending=True)[:k]\n",
    "        reverse_vocab = {v:k for k,v in self.vocab.items()}\n",
    "        words = [reverse_vocab[idx] for idx in closest_idx.tolist()]\n",
    "        return words[0] if k == 1 else words\n",
    "\n",
    "emb = WordEmbedding.from_pretrained('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761c6a8-9626-41db-b469-a57cae6ed45b",
   "metadata": {
    "id": "b761c6a8-9626-41db-b469-a57cae6ed45b"
   },
   "source": [
    "## Examples\n",
    "__Example 1:__ Selecting embeddings for arbitrary words can be done by calling a WordEmbedding class instance with a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00514ad6-df44-47a4-a25b-b6341f93182c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.6663e-01,  6.9023e-01,  7.5462e-02,  1.1688e-01, -7.9722e-01,\n",
       "         -1.9606e-01, -7.7409e-01,  1.7351e-01,  2.6248e-01,  5.5295e-01,\n",
       "         -2.9190e-01, -2.4505e-01,  5.9885e-01,  1.2445e+00,  2.6401e-01,\n",
       "          2.0211e-01,  4.2139e-02,  5.1844e-01, -8.1704e-01, -1.0801e+00,\n",
       "          2.2864e-01,  9.1212e-02,  1.5638e+00,  7.5056e-01, -6.1206e-02,\n",
       "         -6.9001e-01, -5.3558e-01,  1.1311e+00,  1.3871e+00,  3.6151e-01,\n",
       "          2.8475e+00,  1.0733e-01, -1.7073e-02,  4.5358e-01, -7.1374e-03,\n",
       "          1.1177e-01, -1.5955e-01,  3.0205e-01,  5.4222e-01, -5.4103e-01,\n",
       "          2.3276e-01,  2.1756e-01, -4.1444e-02,  1.7056e-03,  7.6265e-01,\n",
       "          6.6241e-01, -4.5484e-02, -8.1479e-01,  4.6763e-02,  3.1134e-01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb('hot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10b6ad-3939-4fbc-9e0b-c69f221e7e28",
   "metadata": {},
   "source": [
    "__Example 2:__ Strings that are not in the pretrained vocabulary of 400,000 'words' raise a warning and return the average embedding of all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c547747-afcd-45fd-abea-bdb9632f9f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/ymk92z8x4cb3wb1qbzb0k5v00000gn/T/ipykernel_26402/134606215.py:35: UserWarning: solidgoldmagikarp is not in the vocabulary, returning average embedding\n",
      "  warn(f'{word} is not in the vocabulary, returning average embedding')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1292, -0.2887, -0.0122, -0.0568, -0.2021, -0.0839,  0.3336,  0.1605,\n",
       "          0.0387,  0.1783,  0.0470, -0.0029,  0.2910,  0.0461, -0.2092, -0.0661,\n",
       "         -0.0682,  0.0767,  0.3134,  0.1785, -0.1226, -0.0992, -0.0750,  0.0641,\n",
       "          0.1444,  0.6089,  0.1746,  0.0534, -0.0127,  0.0347, -0.8124, -0.0469,\n",
       "          0.2019,  0.2031, -0.0394,  0.0697, -0.0155, -0.0341, -0.0653,  0.1225,\n",
       "          0.1399, -0.1745, -0.0801,  0.0850, -0.0104, -0.1370,  0.2013,  0.1007,\n",
       "          0.0065,  0.0169]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb('solidgoldmagikarp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a53482-10eb-469b-a7f0-3158348916a6",
   "metadata": {},
   "source": [
    "__Example 3:__ Using cosine similarity, we can identify words that are close in embedding space. The `find_closest` method implements searching with a given embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9dcca9a-1185-45b5-8d7c-4d5c144dd0db",
   "metadata": {
    "id": "c9dcca9a-1185-45b5-8d7c-4d5c144dd0db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8s/ymk92z8x4cb3wb1qbzb0k5v00000gn/T/ipykernel_26402/134606215.py:35: UserWarning: solidgoldmagikarp is not in the vocabulary, returning average embedding\n",
      "  warn(f'{word} is not in the vocabulary, returning average embedding')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " 'tom.fowler@chron.com',\n",
       " 'mangxamba',\n",
       " 'mongkolporn',\n",
       " 'ryryryryryry',\n",
       " 'jenalia.moreno@chron.com',\n",
       " 'purva.patel@chron.com',\n",
       " 'jiwamol',\n",
       " 'afp02',\n",
       " 'thongrung']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the 10 words that are closest in embedding space to the embedding of 'frog'\n",
    "emb.find_closest(emb('solidgoldmagikarp'), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511a0bd3-5abb-4958-bff2-c39f7e382eef",
   "metadata": {},
   "source": [
    "__Example 4:__ We can perform arithmetic on embedding vectors and find the closest word to resulting vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4870fb72-8535-43fa-a761-a89bb08b1b57",
   "metadata": {
    "id": "4870fb72-8535-43fa-a761-a89bb08b1b57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'berlin'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reproducing bishop eq. 12.27 (p. 376)\n",
    "emb.find_closest(emb('paris') - emb('france') + emb('germany'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bb784-9a9a-49bf-aa2d-6c18e50d5dda",
   "metadata": {
    "id": "980bb784-9a9a-49bf-aa2d-6c18e50d5dda"
   },
   "source": [
    "### Exercise 1\n",
    "What is the result of example 4 when you substitute 'italy' with 'germany'? Are there countries where this doesn't work?\n",
    "\n",
    "### Exercise 2\n",
    "Can you find a word in example 3 where the 10 closest words are not all semantically related to the input word? Can you explain why training on co-occurence can result in this observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b490e6f-1f4e-4f6b-abe0-5b43812c90d9",
   "metadata": {
    "id": "3b490e6f-1f4e-4f6b-abe0-5b43812c90d9"
   },
   "source": [
    "## Answers\n",
    "\n",
    "### Exercise 1\n",
    "'rome' becomes 'berlin', exactly as expected. Belgium does not work, returning 'paris' instead of 'brussels'\n",
    "\n",
    "### Exercise 2\n",
    "The 10 closest words to 'water' include 'dry' and 'sand', indicating co-occurrence (which was used for training) does not always capture semantic similarity. A wider context might be necessary to capture the true semantic meaning of an individual word."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
