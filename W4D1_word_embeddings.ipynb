{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec1f1e7-b61a-4742-a3e5-a99c7e4096f2",
   "metadata": {},
   "source": [
    "# Word embeddings\n",
    "In this notebook we work with pretrained word embedding scores from the GloVe project. We use the smallest version, which maps 400,000 words into 50D embedding space, and was trained on 6billion words. \n",
    "From the project description: \n",
    "> \"The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence\".\n",
    "\n",
    "For more details on model formulation and training procedures visit the [GloVe project website](https://nlp.stanford.edu/projects/glove/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb7dc75-8854-4a31-9ccb-ccd10126d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dependencies for the entire notebook\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from warnings import warn\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb07660-cbbb-4445-8e61-059cf8dfa6fe",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01db6910-745b-4fdb-8dd1-9b12f3247564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip glove word embeddings\n",
    "!wget -nc https://github.com/holmrenser/deep_learning/raw/main/data/glove.6B.50d.txt.gz\n",
    "!gunzip -f glove.6B.50d.txt.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609f57a2-49e1-4c7a-8f84-168b8fa8b43c",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5226c299-865a-4425-9645-9cc32e24ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    \"\"\"Wrapper class for working with GloVe word embeddings\"\"\"\n",
    "    def __init__(self, vocab: dict[str, int], embeddings: torch.tensor):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, filename: str) -> 'WordEmbedding':\n",
    "        \"\"\"Load pretrained embeddings from a whitespace-separated text file, first column is the word, rest are embeddings\"\"\"\n",
    "        vocab = {'<unk>': 0} # start vocabulary with special character <unk> for unknown words\n",
    "        embeddings = []\n",
    "        \n",
    "        with open(filename,'r') as fh:\n",
    "            data = fh.readlines()\n",
    "            for i,line in enumerate(tqdm(data, desc='Loading')):\n",
    "                parts = line.split()\n",
    "                \n",
    "                token = parts[0]\n",
    "                vocab[token] = i + 1 # add one to account for predefined <unk> token\n",
    "                \n",
    "                embedding = list(map(float, parts[1:]))\n",
    "                embeddings.append(embedding)\n",
    "\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "        unk_emb = embeddings.mean(dim=0) # embedding of unknown characters is average of all embeddings\n",
    "        embeddings = torch.vstack([unk_emb, embeddings])\n",
    "        \n",
    "        return cls(vocab, embeddings)\n",
    "\n",
    "    def forward(self, word: str) -> torch.tensor:\n",
    "        \"\"\"Maps word to embedding vector\"\"\"\n",
    "        i = self.vocab.get(word, 0) # 0 is the index of the <unk> character\n",
    "        if i == 0:\n",
    "            warn(f'{word} is not in the vocabulary, returning average embedding')\n",
    "        return self.embeddings(torch.tensor([i]))\n",
    "\n",
    "    def find_closest(self, vec: torch.tensor, k: int=1) -> str:\n",
    "        \"\"\"Find closest k words of an embedding vector using cosine similarity\"\"\"\n",
    "        cos_sim = F.cosine_similarity(emb.embeddings.weight, vec)\n",
    "        closest_idx = {*map(int, torch.argsort(cos_sim)[-k:])}\n",
    "        words = [word for word,idx in self.vocab.items() if idx in closest_idx]\n",
    "        return words[0] if k == 1 else words\n",
    "\n",
    "emb = WordEmbedding.from_pretrained('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b761c6a8-9626-41db-b469-a57cae6ed45b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4870fb72-8535-43fa-a761-a89bb08b1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducing bishop eq. 12.27 (p. 376)\n",
    "emb.find_closest(emb('paris') - emb('france') + emb('italy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcca9a-1185-45b5-8d7c-4d5c144dd0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 10 words that are closest in embedding space to the embedding of 'frog'\n",
    "emb.find_closest(emb('frog'), k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980bb784-9a9a-49bf-aa2d-6c18e50d5dda",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "What is the result of example 1 when you substitute 'italy' with 'germany'? Are there countries where this doesn't work?\n",
    "\n",
    "### Exercise 2\n",
    "Can you find a word where the 10 closest words are not all semantically related to the input word? Can you explain why training on co-occurence can result in this observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b490e6f-1f4e-4f6b-abe0-5b43812c90d9",
   "metadata": {},
   "source": [
    "## Answers\n",
    "\n",
    "### Exercise 1\n",
    "'rome' becomes 'berlin', exactly as expected. Belgium does not work, returning 'paris' instead of 'brussels'\n",
    "\n",
    "### Exercise 2\n",
    "The 10 closest words to 'water' include 'dry' and 'sand', indicating co-occurrence (which was used for training) does not always capture semantic similarity --> larger context might be necesarry to capture true semanting meaning of an individual word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
