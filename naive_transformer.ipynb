{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cca113c-86f8-464e-905c-051ecff81327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86b36e3e-5dcf-446c-b3ee-05743d8c082a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerConfig(block_size=64, vocab_size=65, n_layers=4, n_embed=12)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NaiveTransformer(\n",
       "  (transformer): ModuleDict(\n",
       "    (token_embed): Embedding(65, 12)\n",
       "    (pos_embed): Embedding(64, 12)\n",
       "    (attention_blocks): ModuleList(\n",
       "      (0-3): 4 x TransformerBlock(\n",
       "        (attention): Sequential(\n",
       "          (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): DotProductAttention(\n",
       "            (w_k): Linear(in_features=12, out_features=12, bias=False)\n",
       "            (q_k): Linear(in_features=12, out_features=12, bias=False)\n",
       "            (v_k): Linear(in_features=12, out_features=12, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (projection): Sequential(\n",
       "          (0): LayerNorm((12,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): MLP(\n",
       "            (mlp): Sequential(\n",
       "              (0): Linear(in_features=12, out_features=48, bias=False)\n",
       "              (1): ReLU()\n",
       "              (2): Linear(in_features=48, out_features=12, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=12, out_features=65, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"Simple multi-layer perceptron with two linear layers and a relu non-linearity in between\"\"\"\n",
    "    def __init__(self, n_embed: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=n_embed, out_features=4 * n_embed, bias=bias),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=4 * n_embed, out_features=n_embed, bias=bias),\n",
    "            #nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.mlp(x)\n",
    "\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, n_embed: int, bias: bool=False):\n",
    "        super().__init__()\n",
    "        self.w_k = nn.Linear(in_features=n_embed, out_features=n_embed, bias=bias)\n",
    "        self.q_k = nn.Linear(in_features=n_embed, out_features=n_embed, bias=bias)\n",
    "        self.v_k = nn.Linear(in_features=n_embed, out_features=n_embed, bias=bias)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        _,n_embed = x.size()\n",
    "        \n",
    "        k = self.w_k(x)\n",
    "        q = self.w_q(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        attention = (k @ q.T) / math.sqrt(n_embed)\n",
    "\n",
    "        return F.softmax(attention) @ v\n",
    "        \n",
    "        \n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block that combines attention and MLP, both with pre-layernorm and residual connections\"\"\"\n",
    "    def __init__(self, n_embed: int, bias: bool = False):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.LayerNorm(n_embed, bias=bias),\n",
    "            DotProductAttention(n_embed=n_embed, bias=bias)\n",
    "        )\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.LayerNorm(n_embed, bias=bias),\n",
    "            MLP(n_embed=n_embed, bias=bias)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        attn = x + self.attention(x)\n",
    "        proj = attn + self.projection(attn)\n",
    "        return proj\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TransformerConfig:\n",
    "    block_size: int = None\n",
    "    vocab_size: int = None\n",
    "    n_layers: int = 4\n",
    "    n_embed: int = 12\n",
    "    block_size: int = 64\n",
    "\n",
    "class NaiveTransformer(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            token_embed = nn.Embedding(num_embeddings=config.vocab_size, embedding_dim=config.n_embed),\n",
    "            pos_embed = nn.Embedding(num_embeddings=config.block_size, embedding_dim=config.n_embed),\n",
    "            attention_blocks = nn.ModuleList([TransformerBlock(n_embed=config.n_embed) for _ in range(config.n_layers)]),\n",
    "        ))\n",
    "        self.output_projection = nn.Linear(in_features=config.n_embed, out_features=config.vocab_size, bias=False)\n",
    "\n",
    "config = TransformerConfig(block_size=64, vocab_size=65)\n",
    "print(config)\n",
    "NaiveTransformer(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
